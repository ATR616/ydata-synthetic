{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# The credit fraud dataset - Synthesizing the minority class\n",
    "In this notebook it's presented a practical exercise of how to use the avilable library GANs to synthesize tabular data.\n",
    "For the purpose of this exercise it has been used the Credit Fraud dataset from Kaggle, that you can find here:https: //www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.cluster as cluster\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.gan import model\n",
    "importlib.reload(model)\n",
    "\n",
    "from models.gan.model import GAN\n",
    "from preprocessing.credit_fraud import *\n",
    "\n",
    "model = GAN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/fabiana/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'preprocessing.dataset'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-13-5f27d42f34a9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mmodels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgan\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mGAN\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpreprocessing\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'preprocessing.dataset'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n"
     ]
    }
   ],
   "source": [
    "#Read the data\n",
    "data = pd.read_csv('data/data_processed.csv', index_col=[0])\n",
    "data_cols = list(data.columns[ data.columns != 'Class' ])\n",
    "label_cols = ['Class']\n",
    "\n",
    "print('Dataset columns: {}'.format(data_cols))\n",
    "sorted_cols = ['V14', 'V4', 'V10', 'V17', 'Time', 'V12', 'V26', 'Amount', 'V21', 'V8', 'V11', 'V7', 'V28', 'V19', 'V3', 'V22', 'V6', 'V20', 'V27', 'V16', 'V13', 'V25', 'V24', 'V18', 'V2', 'V1', 'V5', 'V15', 'V9', 'V23', 'Class']\n",
    "data = data[ sorted_cols ].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info: Number of records - 492 Number of varibles - 31\n",
      "   count\n",
      "0    357\n",
      "1    135\n"
     ]
    }
   ],
   "source": [
    "#Before training the GAN do not forget to apply the required data transformations\n",
    "#Such as Log transformation to some of the variables and Normalization such as MinMax.\n",
    "\n",
    "\n",
    "#For the purpose of this example we will only synthesize the minority class\n",
    "train_data = data.loc[ data['Class']==1 ].copy()\n",
    "\n",
    "print(\"Dataset info: Number of records - {} Number of varibles - {}\".format(train_data.shape[0], train_data.shape[1]))\n",
    "\n",
    "algorithm = cluster.KMeans\n",
    "args, kwds = (), {'n_clusters':2, 'random_state':0}\n",
    "labels = algorithm(*args, **kwds).fit_predict(train_data[ data_cols ])\n",
    "\n",
    "print( pd.DataFrame( [ [np.sum(labels==i)] for i in np.unique(labels) ], columns=['count'], index=np.unique(labels) ) )\n",
    "\n",
    "fraud_w_classes = train_data.copy()\n",
    "fraud_w_classes['Class'] = labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Define the GAN and training parameters\n",
    "noise_dim = 32\n",
    "dim = 128\n",
    "batch_size = 128\n",
    "\n",
    "log_step = 100\n",
    "epochs = 5000+1\n",
    "learning_rate = 5e-4\n",
    "models_dir = './cache'\n",
    "\n",
    "train_sample = fraud_w_classes.copy().reset_index(drop=True)\n",
    "label_cols = [ i for i in train_sample.columns if 'Class' in i ]\n",
    "data_cols = [ i for i in train_sample.columns if i not in label_cols ]\n",
    "train_sample[ data_cols ] = train_sample[ data_cols ] / 10 # scale to random noise size, one less thing to learn\n",
    "train_no_label = train_sample[ data_cols ]\n",
    "\n",
    "gan_args = [batch_size, learning_rate, noise_dim, train_sample.shape[1], dim]\n",
    "train_args = ['', epochs, log_step]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.708098, acc.: 33.98%] [G loss: 0.679608]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "1 [D loss: 0.673586, acc.: 50.00%] [G loss: 0.667763]\n",
      "2 [D loss: 0.661436, acc.: 50.00%] [G loss: 0.656845]\n",
      "3 [D loss: 0.660129, acc.: 50.00%] [G loss: 0.649775]\n",
      "4 [D loss: 0.685785, acc.: 49.22%] [G loss: 0.632976]\n",
      "5 [D loss: 0.703788, acc.: 49.22%] [G loss: 0.633304]\n",
      "6 [D loss: 0.734014, acc.: 48.05%] [G loss: 0.674047]\n",
      "7 [D loss: 0.720512, acc.: 42.58%] [G loss: 0.758383]\n",
      "8 [D loss: 0.713380, acc.: 34.38%] [G loss: 0.872537]\n",
      "9 [D loss: 0.708723, acc.: 33.20%] [G loss: 0.972510]\n",
      "10 [D loss: 0.698299, acc.: 44.92%] [G loss: 1.079360]\n",
      "11 [D loss: 0.677419, acc.: 55.47%] [G loss: 1.141391]\n",
      "12 [D loss: 0.650518, acc.: 53.91%] [G loss: 1.219948]\n",
      "13 [D loss: 0.639470, acc.: 59.38%] [G loss: 1.197750]\n",
      "14 [D loss: 0.648842, acc.: 63.28%] [G loss: 1.071212]\n",
      "15 [D loss: 0.666268, acc.: 70.31%] [G loss: 0.962600]\n",
      "16 [D loss: 0.666424, acc.: 70.70%] [G loss: 0.877686]\n",
      "17 [D loss: 0.656399, acc.: 70.31%] [G loss: 0.839275]\n",
      "18 [D loss: 0.650442, acc.: 68.36%] [G loss: 0.843819]\n",
      "19 [D loss: 0.636332, acc.: 73.44%] [G loss: 0.825661]\n",
      "20 [D loss: 0.631869, acc.: 67.19%] [G loss: 0.799550]\n",
      "21 [D loss: 0.632563, acc.: 56.25%] [G loss: 0.769062]\n",
      "22 [D loss: 0.619472, acc.: 54.30%] [G loss: 0.750027]\n",
      "23 [D loss: 0.626200, acc.: 53.12%] [G loss: 0.744112]\n",
      "24 [D loss: 0.626607, acc.: 50.78%] [G loss: 0.740511]\n",
      "25 [D loss: 0.615408, acc.: 53.91%] [G loss: 0.745594]\n",
      "26 [D loss: 0.628138, acc.: 51.56%] [G loss: 0.754134]\n",
      "27 [D loss: 0.620178, acc.: 52.73%] [G loss: 0.765757]\n",
      "28 [D loss: 0.612869, acc.: 55.86%] [G loss: 0.815192]\n",
      "29 [D loss: 0.590421, acc.: 66.80%] [G loss: 0.882182]\n",
      "30 [D loss: 0.573770, acc.: 79.69%] [G loss: 0.920032]\n",
      "31 [D loss: 0.551212, acc.: 89.45%] [G loss: 0.958594]\n",
      "32 [D loss: 0.557651, acc.: 85.16%] [G loss: 0.948166]\n",
      "33 [D loss: 0.580102, acc.: 74.22%] [G loss: 0.886033]\n",
      "34 [D loss: 0.622186, acc.: 55.08%] [G loss: 0.782605]\n",
      "35 [D loss: 0.650187, acc.: 46.88%] [G loss: 0.749044]\n",
      "36 [D loss: 0.671213, acc.: 41.41%] [G loss: 0.736077]\n",
      "37 [D loss: 0.684592, acc.: 38.67%] [G loss: 0.753381]\n",
      "38 [D loss: 0.678159, acc.: 40.23%] [G loss: 0.779481]\n",
      "39 [D loss: 0.692544, acc.: 34.77%] [G loss: 0.831161]\n",
      "40 [D loss: 0.669482, acc.: 38.67%] [G loss: 0.865670]\n",
      "41 [D loss: 0.641683, acc.: 52.73%] [G loss: 0.911331]\n",
      "42 [D loss: 0.615178, acc.: 67.19%] [G loss: 0.971943]\n",
      "43 [D loss: 0.597408, acc.: 72.27%] [G loss: 0.969043]\n",
      "44 [D loss: 0.586702, acc.: 75.00%] [G loss: 0.942998]\n",
      "45 [D loss: 0.589830, acc.: 64.45%] [G loss: 0.920727]\n",
      "46 [D loss: 0.587972, acc.: 58.59%] [G loss: 0.878841]\n",
      "47 [D loss: 0.585279, acc.: 55.47%] [G loss: 0.850029]\n",
      "48 [D loss: 0.618350, acc.: 47.66%] [G loss: 0.831427]\n",
      "49 [D loss: 0.627363, acc.: 45.31%] [G loss: 0.845361]\n",
      "50 [D loss: 0.622320, acc.: 51.17%] [G loss: 0.830466]\n",
      "51 [D loss: 0.627119, acc.: 53.52%] [G loss: 0.824224]\n",
      "52 [D loss: 0.631493, acc.: 57.03%] [G loss: 0.846012]\n",
      "53 [D loss: 0.652306, acc.: 57.42%] [G loss: 0.883810]\n",
      "54 [D loss: 0.673209, acc.: 58.20%] [G loss: 0.897036]\n",
      "55 [D loss: 0.632661, acc.: 60.55%] [G loss: 0.970425]\n",
      "56 [D loss: 0.634200, acc.: 63.28%] [G loss: 1.102695]\n",
      "57 [D loss: 0.616405, acc.: 67.19%] [G loss: 1.175821]\n",
      "58 [D loss: 0.562659, acc.: 71.48%] [G loss: 1.266244]\n",
      "59 [D loss: 0.564843, acc.: 71.09%] [G loss: 1.358078]\n",
      "60 [D loss: 0.482192, acc.: 82.81%] [G loss: 1.483691]\n",
      "61 [D loss: 0.469053, acc.: 82.81%] [G loss: 1.406752]\n",
      "62 [D loss: 0.484374, acc.: 81.25%] [G loss: 1.148047]\n",
      "63 [D loss: 0.526784, acc.: 74.22%] [G loss: 0.949712]\n",
      "64 [D loss: 0.610088, acc.: 58.20%] [G loss: 0.772288]\n",
      "65 [D loss: 0.678762, acc.: 53.12%] [G loss: 0.677479]\n",
      "66 [D loss: 0.798032, acc.: 46.09%] [G loss: 0.576762]\n",
      "67 [D loss: 0.825059, acc.: 39.84%] [G loss: 0.609936]\n",
      "68 [D loss: 0.827398, acc.: 38.67%] [G loss: 0.767644]\n",
      "69 [D loss: 0.797611, acc.: 34.77%] [G loss: 0.977211]\n",
      "70 [D loss: 0.854730, acc.: 19.92%] [G loss: 1.035635]\n",
      "71 [D loss: 0.849284, acc.: 32.81%] [G loss: 1.053891]\n",
      "72 [D loss: 0.812283, acc.: 46.48%] [G loss: 1.032400]\n",
      "73 [D loss: 0.790219, acc.: 49.61%] [G loss: 0.984511]\n",
      "74 [D loss: 0.741904, acc.: 53.52%] [G loss: 0.942906]\n",
      "75 [D loss: 0.709203, acc.: 56.64%] [G loss: 0.964101]\n",
      "76 [D loss: 0.673296, acc.: 60.16%] [G loss: 0.962599]\n",
      "77 [D loss: 0.642019, acc.: 62.50%] [G loss: 0.958009]\n",
      "78 [D loss: 0.613265, acc.: 67.19%] [G loss: 0.994937]\n",
      "79 [D loss: 0.589459, acc.: 69.92%] [G loss: 1.011406]\n",
      "80 [D loss: 0.578463, acc.: 73.83%] [G loss: 0.990333]\n",
      "81 [D loss: 0.575545, acc.: 75.78%] [G loss: 0.985632]\n",
      "82 [D loss: 0.575581, acc.: 74.22%] [G loss: 0.955846]\n",
      "83 [D loss: 0.577403, acc.: 75.39%] [G loss: 0.928547]\n",
      "84 [D loss: 0.587194, acc.: 72.27%] [G loss: 0.894531]\n",
      "85 [D loss: 0.606867, acc.: 64.06%] [G loss: 0.869952]\n",
      "86 [D loss: 0.638243, acc.: 52.34%] [G loss: 0.853134]\n",
      "87 [D loss: 0.661662, acc.: 52.34%] [G loss: 0.887598]\n",
      "88 [D loss: 0.655120, acc.: 55.08%] [G loss: 0.987330]\n",
      "89 [D loss: 0.596137, acc.: 76.95%] [G loss: 1.181694]\n",
      "90 [D loss: 0.564694, acc.: 82.03%] [G loss: 1.214171]\n",
      "91 [D loss: 0.571170, acc.: 81.25%] [G loss: 1.136280]\n",
      "92 [D loss: 0.590739, acc.: 73.83%] [G loss: 1.012714]\n",
      "93 [D loss: 0.630111, acc.: 64.45%] [G loss: 0.917292]\n",
      "94 [D loss: 0.661698, acc.: 56.25%] [G loss: 0.953570]\n",
      "95 [D loss: 0.604817, acc.: 70.70%] [G loss: 1.105834]\n",
      "96 [D loss: 0.562516, acc.: 80.86%] [G loss: 1.192489]\n",
      "97 [D loss: 0.530868, acc.: 82.03%] [G loss: 1.160093]\n",
      "98 [D loss: 0.552497, acc.: 76.17%] [G loss: 1.051393]\n",
      "99 [D loss: 0.586223, acc.: 68.75%] [G loss: 0.969016]\n",
      "100 [D loss: 0.638806, acc.: 58.59%] [G loss: 0.928941]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "101 [D loss: 0.638410, acc.: 64.06%] [G loss: 0.946763]\n",
      "102 [D loss: 0.676895, acc.: 55.47%] [G loss: 1.027810]\n",
      "103 [D loss: 0.638763, acc.: 73.44%] [G loss: 1.059871]\n",
      "104 [D loss: 0.616976, acc.: 76.17%] [G loss: 1.106189]\n",
      "105 [D loss: 0.571723, acc.: 76.56%] [G loss: 1.188485]\n",
      "106 [D loss: 0.560254, acc.: 76.56%] [G loss: 1.182233]\n",
      "107 [D loss: 0.574533, acc.: 71.09%] [G loss: 1.039969]\n",
      "108 [D loss: 0.638622, acc.: 53.12%] [G loss: 0.989202]\n",
      "109 [D loss: 0.673954, acc.: 50.78%] [G loss: 1.065441]\n",
      "110 [D loss: 0.609060, acc.: 56.25%] [G loss: 1.210330]\n",
      "111 [D loss: 0.529146, acc.: 74.22%] [G loss: 1.441490]\n",
      "112 [D loss: 0.461043, acc.: 78.12%] [G loss: 1.630737]\n",
      "113 [D loss: 0.430756, acc.: 81.25%] [G loss: 1.577463]\n",
      "114 [D loss: 0.424442, acc.: 82.42%] [G loss: 1.476537]\n",
      "115 [D loss: 0.426632, acc.: 83.20%] [G loss: 1.360669]\n",
      "116 [D loss: 0.431157, acc.: 83.98%] [G loss: 1.194592]\n",
      "117 [D loss: 0.446810, acc.: 83.59%] [G loss: 1.093068]\n",
      "118 [D loss: 0.457070, acc.: 84.38%] [G loss: 1.006130]\n",
      "119 [D loss: 0.503120, acc.: 76.95%] [G loss: 0.926335]\n",
      "120 [D loss: 0.553485, acc.: 71.48%] [G loss: 0.881968]\n",
      "121 [D loss: 0.580689, acc.: 66.41%] [G loss: 0.831281]\n",
      "122 [D loss: 0.662454, acc.: 57.42%] [G loss: 0.920785]\n",
      "123 [D loss: 0.746060, acc.: 53.91%] [G loss: 1.174515]\n",
      "124 [D loss: 0.593462, acc.: 70.31%] [G loss: 1.522954]\n",
      "125 [D loss: 0.726927, acc.: 53.52%] [G loss: 1.478470]\n",
      "126 [D loss: 0.968174, acc.: 32.03%] [G loss: 1.046730]\n",
      "127 [D loss: 1.267323, acc.: 14.45%] [G loss: 0.899200]\n",
      "128 [D loss: 1.111613, acc.: 13.67%] [G loss: 1.137806]\n",
      "129 [D loss: 0.874802, acc.: 48.05%] [G loss: 1.504598]\n",
      "130 [D loss: 0.779296, acc.: 51.56%] [G loss: 1.481183]\n",
      "131 [D loss: 0.721550, acc.: 52.34%] [G loss: 1.378943]\n",
      "132 [D loss: 0.677627, acc.: 55.08%] [G loss: 1.268865]\n",
      "133 [D loss: 0.645911, acc.: 59.77%] [G loss: 1.181238]\n",
      "134 [D loss: 0.639374, acc.: 61.72%] [G loss: 1.143484]\n",
      "135 [D loss: 0.629609, acc.: 62.50%] [G loss: 1.066081]\n",
      "136 [D loss: 0.626009, acc.: 63.67%] [G loss: 1.019416]\n",
      "137 [D loss: 0.626227, acc.: 63.28%] [G loss: 0.991077]\n",
      "138 [D loss: 0.620627, acc.: 66.02%] [G loss: 0.946663]\n",
      "139 [D loss: 0.635774, acc.: 64.45%] [G loss: 0.923848]\n",
      "140 [D loss: 0.653764, acc.: 60.55%] [G loss: 0.916137]\n",
      "141 [D loss: 0.660331, acc.: 61.33%] [G loss: 0.893967]\n",
      "142 [D loss: 0.690649, acc.: 55.86%] [G loss: 0.905166]\n",
      "143 [D loss: 0.703687, acc.: 51.95%] [G loss: 0.886061]\n",
      "144 [D loss: 0.716122, acc.: 50.78%] [G loss: 0.938209]\n",
      "145 [D loss: 0.679577, acc.: 64.06%] [G loss: 1.009317]\n",
      "146 [D loss: 0.666150, acc.: 64.45%] [G loss: 1.070749]\n",
      "147 [D loss: 0.633266, acc.: 67.19%] [G loss: 1.106465]\n",
      "148 [D loss: 0.612756, acc.: 68.75%] [G loss: 1.106980]\n",
      "149 [D loss: 0.610344, acc.: 71.09%] [G loss: 1.098545]\n",
      "150 [D loss: 0.600985, acc.: 74.61%] [G loss: 1.057262]\n",
      "151 [D loss: 0.587598, acc.: 80.08%] [G loss: 0.997843]\n",
      "152 [D loss: 0.593997, acc.: 82.42%] [G loss: 0.958126]\n",
      "153 [D loss: 0.594817, acc.: 78.12%] [G loss: 0.920713]\n",
      "154 [D loss: 0.611902, acc.: 75.00%] [G loss: 0.874936]\n",
      "155 [D loss: 0.615600, acc.: 69.53%] [G loss: 0.813350]\n",
      "156 [D loss: 0.629723, acc.: 63.28%] [G loss: 0.789770]\n",
      "157 [D loss: 0.635776, acc.: 59.77%] [G loss: 0.786637]\n",
      "158 [D loss: 0.650779, acc.: 53.12%] [G loss: 0.799978]\n",
      "159 [D loss: 0.677598, acc.: 51.17%] [G loss: 0.816717]\n",
      "160 [D loss: 0.684020, acc.: 47.27%] [G loss: 0.788029]\n",
      "161 [D loss: 0.705020, acc.: 51.56%] [G loss: 0.797327]\n",
      "162 [D loss: 0.709256, acc.: 54.69%] [G loss: 0.847937]\n",
      "163 [D loss: 0.660548, acc.: 54.30%] [G loss: 0.950379]\n",
      "164 [D loss: 0.587194, acc.: 69.53%] [G loss: 1.069551]\n",
      "165 [D loss: 0.564496, acc.: 69.92%] [G loss: 1.108985]\n",
      "166 [D loss: 0.565747, acc.: 68.36%] [G loss: 1.127926]\n",
      "167 [D loss: 0.590481, acc.: 66.02%] [G loss: 0.985391]\n",
      "168 [D loss: 0.674500, acc.: 59.77%] [G loss: 0.983051]\n",
      "169 [D loss: 0.688014, acc.: 56.25%] [G loss: 1.007149]\n",
      "170 [D loss: 0.645952, acc.: 61.33%] [G loss: 1.020718]\n",
      "171 [D loss: 0.719653, acc.: 50.78%] [G loss: 1.039610]\n",
      "172 [D loss: 0.777491, acc.: 41.02%] [G loss: 0.987724]\n",
      "173 [D loss: 0.829420, acc.: 35.16%] [G loss: 1.006696]\n",
      "174 [D loss: 0.848024, acc.: 31.25%] [G loss: 1.034547]\n",
      "175 [D loss: 0.837367, acc.: 32.42%] [G loss: 1.195438]\n",
      "176 [D loss: 0.760889, acc.: 48.44%] [G loss: 1.335876]\n",
      "177 [D loss: 0.734385, acc.: 50.00%] [G loss: 1.276115]\n",
      "178 [D loss: 0.705222, acc.: 51.17%] [G loss: 1.218693]\n",
      "179 [D loss: 0.690608, acc.: 50.78%] [G loss: 1.124284]\n",
      "180 [D loss: 0.680420, acc.: 54.69%] [G loss: 1.037856]\n",
      "181 [D loss: 0.669770, acc.: 56.25%] [G loss: 1.020877]\n",
      "182 [D loss: 0.655407, acc.: 60.55%] [G loss: 0.966983]\n",
      "183 [D loss: 0.659493, acc.: 65.23%] [G loss: 0.914354]\n",
      "184 [D loss: 0.655270, acc.: 69.53%] [G loss: 0.889192]\n",
      "185 [D loss: 0.645518, acc.: 72.66%] [G loss: 0.848403]\n",
      "186 [D loss: 0.648545, acc.: 70.70%] [G loss: 0.819903]\n",
      "187 [D loss: 0.648919, acc.: 69.14%] [G loss: 0.809753]\n",
      "188 [D loss: 0.649497, acc.: 71.09%] [G loss: 0.798980]\n",
      "189 [D loss: 0.645677, acc.: 70.31%] [G loss: 0.781624]\n",
      "190 [D loss: 0.662819, acc.: 64.84%] [G loss: 0.765137]\n",
      "191 [D loss: 0.674892, acc.: 59.38%] [G loss: 0.796642]\n",
      "192 [D loss: 0.656928, acc.: 70.70%] [G loss: 0.811911]\n",
      "193 [D loss: 0.660793, acc.: 66.41%] [G loss: 0.826224]\n",
      "194 [D loss: 0.653820, acc.: 69.14%] [G loss: 0.823067]\n",
      "195 [D loss: 0.652611, acc.: 69.53%] [G loss: 0.816272]\n",
      "196 [D loss: 0.653179, acc.: 68.75%] [G loss: 0.811017]\n",
      "197 [D loss: 0.655760, acc.: 66.02%] [G loss: 0.794570]\n",
      "198 [D loss: 0.664185, acc.: 58.98%] [G loss: 0.791883]\n",
      "199 [D loss: 0.661792, acc.: 61.33%] [G loss: 0.774418]\n",
      "200 [D loss: 0.670264, acc.: 57.81%] [G loss: 0.766367]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "201 [D loss: 0.664860, acc.: 58.98%] [G loss: 0.776692]\n",
      "202 [D loss: 0.659037, acc.: 56.25%] [G loss: 0.808168]\n",
      "203 [D loss: 0.661492, acc.: 55.08%] [G loss: 0.794216]\n",
      "204 [D loss: 0.662491, acc.: 52.73%] [G loss: 0.826883]\n",
      "205 [D loss: 0.645280, acc.: 60.16%] [G loss: 0.854587]\n",
      "206 [D loss: 0.637909, acc.: 64.45%] [G loss: 0.901335]\n",
      "207 [D loss: 0.639324, acc.: 63.67%] [G loss: 0.905799]\n",
      "208 [D loss: 0.642265, acc.: 61.72%] [G loss: 0.922126]\n",
      "209 [D loss: 0.641099, acc.: 63.28%] [G loss: 0.935456]\n",
      "210 [D loss: 0.646995, acc.: 60.55%] [G loss: 0.909349]\n",
      "211 [D loss: 0.657355, acc.: 61.72%] [G loss: 0.899015]\n",
      "212 [D loss: 0.676424, acc.: 55.86%] [G loss: 0.876382]\n",
      "213 [D loss: 0.682850, acc.: 56.64%] [G loss: 0.910037]\n",
      "214 [D loss: 0.688240, acc.: 55.86%] [G loss: 0.902737]\n",
      "215 [D loss: 0.714369, acc.: 51.17%] [G loss: 0.902992]\n",
      "216 [D loss: 0.697680, acc.: 54.30%] [G loss: 0.883952]\n",
      "217 [D loss: 0.733372, acc.: 44.53%] [G loss: 0.850054]\n",
      "218 [D loss: 0.726085, acc.: 47.66%] [G loss: 0.849358]\n",
      "219 [D loss: 0.735910, acc.: 45.70%] [G loss: 0.853206]\n",
      "220 [D loss: 0.736817, acc.: 45.31%] [G loss: 0.897193]\n",
      "221 [D loss: 0.717653, acc.: 51.95%] [G loss: 0.909067]\n",
      "222 [D loss: 0.704522, acc.: 53.91%] [G loss: 0.920823]\n",
      "223 [D loss: 0.691073, acc.: 54.30%] [G loss: 0.949770]\n",
      "224 [D loss: 0.683236, acc.: 57.42%] [G loss: 0.957757]\n",
      "225 [D loss: 0.672440, acc.: 57.03%] [G loss: 0.923112]\n",
      "226 [D loss: 0.666104, acc.: 58.98%] [G loss: 0.907075]\n",
      "227 [D loss: 0.663461, acc.: 60.16%] [G loss: 0.885802]\n",
      "228 [D loss: 0.664217, acc.: 61.72%] [G loss: 0.887474]\n",
      "229 [D loss: 0.663365, acc.: 63.28%] [G loss: 0.857254]\n",
      "230 [D loss: 0.663603, acc.: 64.45%] [G loss: 0.840853]\n",
      "231 [D loss: 0.666610, acc.: 63.67%] [G loss: 0.826021]\n",
      "232 [D loss: 0.666920, acc.: 60.16%] [G loss: 0.814893]\n",
      "233 [D loss: 0.664601, acc.: 66.80%] [G loss: 0.798923]\n",
      "234 [D loss: 0.663427, acc.: 63.28%] [G loss: 0.797543]\n",
      "235 [D loss: 0.657387, acc.: 65.23%] [G loss: 0.792857]\n",
      "236 [D loss: 0.653891, acc.: 67.19%] [G loss: 0.823560]\n",
      "237 [D loss: 0.648525, acc.: 67.97%] [G loss: 0.830635]\n",
      "238 [D loss: 0.640663, acc.: 68.75%] [G loss: 0.833313]\n",
      "239 [D loss: 0.637294, acc.: 66.80%] [G loss: 0.828914]\n",
      "240 [D loss: 0.643796, acc.: 62.11%] [G loss: 0.812566]\n",
      "241 [D loss: 0.663021, acc.: 51.56%] [G loss: 0.771197]\n",
      "242 [D loss: 0.667585, acc.: 48.83%] [G loss: 0.746931]\n",
      "243 [D loss: 0.682446, acc.: 41.41%] [G loss: 0.734716]\n",
      "244 [D loss: 0.688172, acc.: 33.59%] [G loss: 0.732007]\n",
      "245 [D loss: 0.687729, acc.: 47.66%] [G loss: 0.750645]\n",
      "246 [D loss: 0.667928, acc.: 64.06%] [G loss: 0.778733]\n",
      "247 [D loss: 0.661312, acc.: 61.72%] [G loss: 0.815964]\n",
      "248 [D loss: 0.639720, acc.: 60.94%] [G loss: 0.856403]\n",
      "249 [D loss: 0.647118, acc.: 62.11%] [G loss: 0.871552]\n",
      "250 [D loss: 0.647671, acc.: 60.94%] [G loss: 0.874086]\n",
      "251 [D loss: 0.665967, acc.: 56.25%] [G loss: 0.882345]\n",
      "252 [D loss: 0.654169, acc.: 60.16%] [G loss: 0.886929]\n",
      "253 [D loss: 0.650847, acc.: 58.98%] [G loss: 0.886229]\n",
      "254 [D loss: 0.640889, acc.: 69.14%] [G loss: 0.878647]\n",
      "255 [D loss: 0.639486, acc.: 71.88%] [G loss: 0.865965]\n",
      "256 [D loss: 0.636906, acc.: 74.22%] [G loss: 0.846047]\n",
      "257 [D loss: 0.646135, acc.: 69.14%] [G loss: 0.820749]\n",
      "258 [D loss: 0.634530, acc.: 71.88%] [G loss: 0.822449]\n",
      "259 [D loss: 0.649856, acc.: 63.67%] [G loss: 0.804937]\n",
      "260 [D loss: 0.647860, acc.: 66.41%] [G loss: 0.806597]\n",
      "261 [D loss: 0.654684, acc.: 66.02%] [G loss: 0.790205]\n",
      "262 [D loss: 0.652886, acc.: 67.19%] [G loss: 0.808146]\n",
      "263 [D loss: 0.645137, acc.: 67.19%] [G loss: 0.783854]\n",
      "264 [D loss: 0.654386, acc.: 62.50%] [G loss: 0.784800]\n",
      "265 [D loss: 0.655858, acc.: 63.28%] [G loss: 0.786070]\n",
      "266 [D loss: 0.651901, acc.: 65.62%] [G loss: 0.790384]\n",
      "267 [D loss: 0.640216, acc.: 69.14%] [G loss: 0.808311]\n",
      "268 [D loss: 0.637046, acc.: 66.80%] [G loss: 0.830331]\n",
      "269 [D loss: 0.627054, acc.: 69.53%] [G loss: 0.834825]\n",
      "270 [D loss: 0.623365, acc.: 71.48%] [G loss: 0.866094]\n",
      "271 [D loss: 0.628669, acc.: 67.19%] [G loss: 0.884569]\n",
      "272 [D loss: 0.631778, acc.: 67.19%] [G loss: 0.873781]\n",
      "273 [D loss: 0.639279, acc.: 64.45%] [G loss: 0.888932]\n",
      "274 [D loss: 0.640824, acc.: 60.55%] [G loss: 0.918837]\n",
      "275 [D loss: 0.629168, acc.: 65.23%] [G loss: 0.928480]\n",
      "276 [D loss: 0.637751, acc.: 63.67%] [G loss: 0.959298]\n",
      "277 [D loss: 0.630672, acc.: 64.45%] [G loss: 1.000465]\n",
      "278 [D loss: 0.624121, acc.: 62.11%] [G loss: 0.997114]\n",
      "279 [D loss: 0.656891, acc.: 53.91%] [G loss: 1.016343]\n",
      "280 [D loss: 0.671384, acc.: 51.17%] [G loss: 0.937176]\n",
      "281 [D loss: 0.716970, acc.: 46.09%] [G loss: 0.900656]\n",
      "282 [D loss: 0.725022, acc.: 45.70%] [G loss: 0.889729]\n",
      "283 [D loss: 0.729846, acc.: 48.44%] [G loss: 0.931147]\n",
      "284 [D loss: 0.724754, acc.: 47.66%] [G loss: 0.994330]\n",
      "285 [D loss: 0.690169, acc.: 58.20%] [G loss: 1.076486]\n",
      "286 [D loss: 0.675039, acc.: 55.86%] [G loss: 1.092622]\n",
      "287 [D loss: 0.686474, acc.: 55.08%] [G loss: 1.079626]\n",
      "288 [D loss: 0.691503, acc.: 55.08%] [G loss: 0.954273]\n",
      "289 [D loss: 0.726816, acc.: 50.78%] [G loss: 0.904366]\n",
      "290 [D loss: 0.714037, acc.: 52.34%] [G loss: 0.850466]\n",
      "291 [D loss: 0.719105, acc.: 46.09%] [G loss: 0.792086]\n",
      "292 [D loss: 0.725309, acc.: 46.48%] [G loss: 0.790134]\n",
      "293 [D loss: 0.747209, acc.: 43.36%] [G loss: 0.836848]\n",
      "294 [D loss: 0.695940, acc.: 53.52%] [G loss: 0.882366]\n",
      "295 [D loss: 0.680928, acc.: 53.12%] [G loss: 0.903432]\n",
      "296 [D loss: 0.653942, acc.: 53.52%] [G loss: 0.926407]\n",
      "297 [D loss: 0.639690, acc.: 53.91%] [G loss: 0.952131]\n",
      "298 [D loss: 0.630932, acc.: 58.98%] [G loss: 0.946790]\n",
      "299 [D loss: 0.623060, acc.: 67.97%] [G loss: 0.922218]\n",
      "300 [D loss: 0.614407, acc.: 73.83%] [G loss: 0.907281]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "301 [D loss: 0.615124, acc.: 77.34%] [G loss: 0.894350]\n",
      "302 [D loss: 0.616441, acc.: 79.69%] [G loss: 0.888627]\n",
      "303 [D loss: 0.607381, acc.: 80.47%] [G loss: 0.872420]\n",
      "304 [D loss: 0.609743, acc.: 81.25%] [G loss: 0.860286]\n",
      "305 [D loss: 0.622558, acc.: 78.52%] [G loss: 0.820467]\n",
      "306 [D loss: 0.639764, acc.: 65.62%] [G loss: 0.770081]\n",
      "307 [D loss: 0.666432, acc.: 55.08%] [G loss: 0.708033]\n",
      "308 [D loss: 0.687119, acc.: 42.58%] [G loss: 0.683707]\n",
      "309 [D loss: 0.708177, acc.: 40.62%] [G loss: 0.700178]\n",
      "310 [D loss: 0.686375, acc.: 42.19%] [G loss: 0.756829]\n",
      "311 [D loss: 0.658688, acc.: 55.86%] [G loss: 0.863469]\n",
      "312 [D loss: 0.628668, acc.: 71.48%] [G loss: 0.981132]\n",
      "313 [D loss: 0.623524, acc.: 64.45%] [G loss: 0.991468]\n",
      "314 [D loss: 0.637032, acc.: 66.41%] [G loss: 0.968466]\n",
      "315 [D loss: 0.656476, acc.: 60.16%] [G loss: 0.916956]\n",
      "316 [D loss: 0.666378, acc.: 60.94%] [G loss: 0.887049]\n",
      "317 [D loss: 0.679264, acc.: 56.64%] [G loss: 0.881385]\n",
      "318 [D loss: 0.655096, acc.: 62.89%] [G loss: 0.899548]\n",
      "319 [D loss: 0.642392, acc.: 63.67%] [G loss: 0.960152]\n",
      "320 [D loss: 0.638483, acc.: 64.45%] [G loss: 0.967411]\n",
      "321 [D loss: 0.622475, acc.: 65.23%] [G loss: 0.963532]\n",
      "322 [D loss: 0.629530, acc.: 66.02%] [G loss: 0.937020]\n",
      "323 [D loss: 0.636612, acc.: 64.45%] [G loss: 0.937157]\n",
      "324 [D loss: 0.628811, acc.: 65.62%] [G loss: 0.910951]\n",
      "325 [D loss: 0.639242, acc.: 63.28%] [G loss: 0.923548]\n",
      "326 [D loss: 0.632557, acc.: 65.23%] [G loss: 0.884248]\n",
      "327 [D loss: 0.633310, acc.: 67.97%] [G loss: 0.855084]\n",
      "328 [D loss: 0.631081, acc.: 68.36%] [G loss: 0.833278]\n",
      "329 [D loss: 0.621292, acc.: 69.92%] [G loss: 0.847488]\n",
      "330 [D loss: 0.618695, acc.: 71.48%] [G loss: 0.854095]\n",
      "331 [D loss: 0.612872, acc.: 75.78%] [G loss: 0.870860]\n",
      "332 [D loss: 0.621693, acc.: 73.44%] [G loss: 0.887407]\n",
      "333 [D loss: 0.615820, acc.: 72.66%] [G loss: 0.896789]\n",
      "334 [D loss: 0.623471, acc.: 71.48%] [G loss: 0.913460]\n",
      "335 [D loss: 0.614189, acc.: 73.44%] [G loss: 0.955830]\n",
      "336 [D loss: 0.616962, acc.: 73.83%] [G loss: 0.947266]\n",
      "337 [D loss: 0.613728, acc.: 73.83%] [G loss: 0.959125]\n",
      "338 [D loss: 0.601188, acc.: 76.56%] [G loss: 0.967052]\n",
      "339 [D loss: 0.622063, acc.: 71.88%] [G loss: 0.951129]\n",
      "340 [D loss: 0.612014, acc.: 74.22%] [G loss: 0.944851]\n",
      "341 [D loss: 0.605338, acc.: 73.83%] [G loss: 0.946341]\n",
      "342 [D loss: 0.617048, acc.: 70.31%] [G loss: 0.948714]\n",
      "343 [D loss: 0.631806, acc.: 64.06%] [G loss: 0.954928]\n",
      "344 [D loss: 0.615039, acc.: 68.75%] [G loss: 0.987493]\n",
      "345 [D loss: 0.621030, acc.: 62.89%] [G loss: 0.955446]\n",
      "346 [D loss: 0.622333, acc.: 64.84%] [G loss: 0.959292]\n",
      "347 [D loss: 0.629828, acc.: 63.67%] [G loss: 0.928287]\n",
      "348 [D loss: 0.636688, acc.: 62.50%] [G loss: 0.975957]\n",
      "349 [D loss: 0.651679, acc.: 55.08%] [G loss: 0.987239]\n",
      "350 [D loss: 0.647198, acc.: 57.42%] [G loss: 1.008624]\n",
      "351 [D loss: 0.656101, acc.: 54.30%] [G loss: 1.017988]\n",
      "352 [D loss: 0.649777, acc.: 61.72%] [G loss: 1.003464]\n",
      "353 [D loss: 0.656033, acc.: 63.28%] [G loss: 0.986453]\n",
      "354 [D loss: 0.674308, acc.: 62.89%] [G loss: 0.977804]\n",
      "355 [D loss: 0.663991, acc.: 62.89%] [G loss: 1.007100]\n",
      "356 [D loss: 0.692836, acc.: 58.98%] [G loss: 1.033439]\n",
      "357 [D loss: 0.690640, acc.: 54.69%] [G loss: 1.031957]\n",
      "358 [D loss: 0.697697, acc.: 56.25%] [G loss: 1.006060]\n",
      "359 [D loss: 0.699732, acc.: 52.34%] [G loss: 0.996606]\n",
      "360 [D loss: 0.684930, acc.: 57.81%] [G loss: 1.019601]\n",
      "361 [D loss: 0.642326, acc.: 62.89%] [G loss: 1.019111]\n",
      "362 [D loss: 0.647057, acc.: 65.23%] [G loss: 0.969695]\n",
      "363 [D loss: 0.623644, acc.: 66.80%] [G loss: 0.970828]\n",
      "364 [D loss: 0.642570, acc.: 64.84%] [G loss: 0.931456]\n",
      "365 [D loss: 0.643977, acc.: 57.42%] [G loss: 0.930921]\n",
      "366 [D loss: 0.667428, acc.: 54.69%] [G loss: 0.966278]\n",
      "367 [D loss: 0.656763, acc.: 57.81%] [G loss: 0.927401]\n",
      "368 [D loss: 0.669166, acc.: 54.30%] [G loss: 0.912337]\n",
      "369 [D loss: 0.677600, acc.: 50.00%] [G loss: 0.936343]\n",
      "370 [D loss: 0.680203, acc.: 49.61%] [G loss: 0.926221]\n",
      "371 [D loss: 0.662240, acc.: 53.12%] [G loss: 0.988974]\n",
      "372 [D loss: 0.644091, acc.: 57.81%] [G loss: 1.068822]\n",
      "373 [D loss: 0.646029, acc.: 55.08%] [G loss: 1.042483]\n",
      "374 [D loss: 0.655605, acc.: 55.47%] [G loss: 1.058021]\n",
      "375 [D loss: 0.650763, acc.: 60.16%] [G loss: 1.016717]\n",
      "376 [D loss: 0.633610, acc.: 64.45%] [G loss: 1.082583]\n",
      "377 [D loss: 0.576817, acc.: 79.30%] [G loss: 1.136458]\n",
      "378 [D loss: 0.548106, acc.: 79.30%] [G loss: 1.176273]\n",
      "379 [D loss: 0.541424, acc.: 82.42%] [G loss: 1.073404]\n",
      "380 [D loss: 0.544954, acc.: 82.81%] [G loss: 1.026145]\n",
      "381 [D loss: 0.557646, acc.: 82.42%] [G loss: 0.971652]\n",
      "382 [D loss: 0.566415, acc.: 81.25%] [G loss: 0.906444]\n",
      "383 [D loss: 0.587818, acc.: 76.95%] [G loss: 0.853195]\n",
      "384 [D loss: 0.624556, acc.: 61.33%] [G loss: 0.815472]\n",
      "385 [D loss: 0.628595, acc.: 59.77%] [G loss: 0.762595]\n",
      "386 [D loss: 0.654297, acc.: 58.20%] [G loss: 0.688144]\n",
      "387 [D loss: 0.674710, acc.: 53.91%] [G loss: 0.699952]\n",
      "388 [D loss: 0.706331, acc.: 48.83%] [G loss: 0.697060]\n",
      "389 [D loss: 0.715065, acc.: 43.36%] [G loss: 0.679420]\n",
      "390 [D loss: 0.718713, acc.: 43.36%] [G loss: 0.680340]\n",
      "391 [D loss: 0.710054, acc.: 41.41%] [G loss: 0.738800]\n",
      "392 [D loss: 0.708060, acc.: 39.84%] [G loss: 0.760048]\n",
      "393 [D loss: 0.685546, acc.: 41.41%] [G loss: 0.869304]\n",
      "394 [D loss: 0.657191, acc.: 62.11%] [G loss: 0.976905]\n",
      "395 [D loss: 0.633872, acc.: 65.23%] [G loss: 1.097275]\n",
      "396 [D loss: 0.630522, acc.: 64.45%] [G loss: 1.121874]\n",
      "397 [D loss: 0.648122, acc.: 62.11%] [G loss: 1.083317]\n",
      "398 [D loss: 0.652837, acc.: 62.50%] [G loss: 1.012614]\n",
      "399 [D loss: 0.677137, acc.: 58.59%] [G loss: 0.927442]\n",
      "400 [D loss: 0.666850, acc.: 59.38%] [G loss: 0.893424]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "401 [D loss: 0.688443, acc.: 57.81%] [G loss: 0.922689]\n",
      "402 [D loss: 0.660013, acc.: 61.72%] [G loss: 0.929366]\n",
      "403 [D loss: 0.632909, acc.: 63.67%] [G loss: 0.936450]\n",
      "404 [D loss: 0.627959, acc.: 61.72%] [G loss: 0.940377]\n",
      "405 [D loss: 0.622634, acc.: 65.62%] [G loss: 0.946911]\n",
      "406 [D loss: 0.617028, acc.: 66.02%] [G loss: 0.930171]\n",
      "407 [D loss: 0.614140, acc.: 68.75%] [G loss: 0.904900]\n",
      "408 [D loss: 0.618836, acc.: 66.02%] [G loss: 0.882100]\n",
      "409 [D loss: 0.619091, acc.: 68.36%] [G loss: 0.866243]\n",
      "410 [D loss: 0.621199, acc.: 64.84%] [G loss: 0.824586]\n",
      "411 [D loss: 0.634434, acc.: 56.64%] [G loss: 0.792122]\n",
      "412 [D loss: 0.653422, acc.: 51.95%] [G loss: 0.749312]\n",
      "413 [D loss: 0.689302, acc.: 45.31%] [G loss: 0.713564]\n",
      "414 [D loss: 0.708037, acc.: 39.45%] [G loss: 0.703092]\n",
      "415 [D loss: 0.703415, acc.: 37.50%] [G loss: 0.712710]\n",
      "416 [D loss: 0.710903, acc.: 35.94%] [G loss: 0.715157]\n",
      "417 [D loss: 0.724355, acc.: 41.80%] [G loss: 0.761582]\n",
      "418 [D loss: 0.706257, acc.: 62.50%] [G loss: 0.830368]\n",
      "419 [D loss: 0.652944, acc.: 66.80%] [G loss: 0.978888]\n",
      "420 [D loss: 0.628822, acc.: 64.84%] [G loss: 1.089760]\n",
      "421 [D loss: 0.605164, acc.: 67.19%] [G loss: 1.163788]\n",
      "422 [D loss: 0.596303, acc.: 69.53%] [G loss: 1.238420]\n",
      "423 [D loss: 0.611685, acc.: 67.97%] [G loss: 1.123161]\n",
      "424 [D loss: 0.652937, acc.: 65.62%] [G loss: 0.998398]\n",
      "425 [D loss: 0.654428, acc.: 63.67%] [G loss: 0.918888]\n",
      "426 [D loss: 0.708404, acc.: 59.38%] [G loss: 0.876604]\n",
      "427 [D loss: 0.688246, acc.: 58.59%] [G loss: 0.829863]\n",
      "428 [D loss: 0.701134, acc.: 53.91%] [G loss: 0.837685]\n",
      "429 [D loss: 0.700306, acc.: 56.64%] [G loss: 0.863944]\n",
      "430 [D loss: 0.675768, acc.: 60.94%] [G loss: 0.901896]\n",
      "431 [D loss: 0.706899, acc.: 52.34%] [G loss: 0.898120]\n",
      "432 [D loss: 0.706423, acc.: 50.39%] [G loss: 0.849039]\n",
      "433 [D loss: 0.740430, acc.: 35.94%] [G loss: 0.798360]\n",
      "434 [D loss: 0.746568, acc.: 31.64%] [G loss: 0.801120]\n",
      "435 [D loss: 0.711714, acc.: 38.67%] [G loss: 0.885722]\n",
      "436 [D loss: 0.684869, acc.: 56.64%] [G loss: 0.935526]\n",
      "437 [D loss: 0.647118, acc.: 64.45%] [G loss: 1.022357]\n",
      "438 [D loss: 0.630091, acc.: 66.41%] [G loss: 1.010624]\n",
      "439 [D loss: 0.619020, acc.: 68.75%] [G loss: 0.976121]\n",
      "440 [D loss: 0.621254, acc.: 68.75%] [G loss: 0.944936]\n",
      "441 [D loss: 0.629789, acc.: 66.41%] [G loss: 0.902559]\n",
      "442 [D loss: 0.622475, acc.: 71.48%] [G loss: 0.887423]\n",
      "443 [D loss: 0.637637, acc.: 66.80%] [G loss: 0.827073]\n",
      "444 [D loss: 0.638053, acc.: 69.14%] [G loss: 0.828933]\n",
      "445 [D loss: 0.659162, acc.: 63.28%] [G loss: 0.789181]\n",
      "446 [D loss: 0.672907, acc.: 56.25%] [G loss: 0.786876]\n",
      "447 [D loss: 0.647174, acc.: 63.28%] [G loss: 0.765040]\n",
      "448 [D loss: 0.657466, acc.: 58.59%] [G loss: 0.792359]\n",
      "449 [D loss: 0.658361, acc.: 58.59%] [G loss: 0.781464]\n",
      "450 [D loss: 0.650946, acc.: 59.77%] [G loss: 0.779422]\n",
      "451 [D loss: 0.659779, acc.: 59.38%] [G loss: 0.808678]\n",
      "452 [D loss: 0.648826, acc.: 63.67%] [G loss: 0.815891]\n",
      "453 [D loss: 0.642134, acc.: 67.97%] [G loss: 0.852004]\n",
      "454 [D loss: 0.640944, acc.: 65.23%] [G loss: 0.876204]\n",
      "455 [D loss: 0.637975, acc.: 64.06%] [G loss: 0.885709]\n",
      "456 [D loss: 0.638293, acc.: 63.67%] [G loss: 0.864391]\n",
      "457 [D loss: 0.636469, acc.: 65.23%] [G loss: 0.830936]\n",
      "458 [D loss: 0.661105, acc.: 56.64%] [G loss: 0.838967]\n",
      "459 [D loss: 0.651029, acc.: 57.42%] [G loss: 0.823499]\n",
      "460 [D loss: 0.657832, acc.: 58.20%] [G loss: 0.861027]\n",
      "461 [D loss: 0.646086, acc.: 67.97%] [G loss: 0.862516]\n",
      "462 [D loss: 0.635978, acc.: 67.58%] [G loss: 0.870390]\n",
      "463 [D loss: 0.619478, acc.: 71.48%] [G loss: 0.887306]\n",
      "464 [D loss: 0.609959, acc.: 69.92%] [G loss: 0.889608]\n",
      "465 [D loss: 0.603453, acc.: 69.92%] [G loss: 0.899615]\n",
      "466 [D loss: 0.607060, acc.: 66.80%] [G loss: 0.928830]\n",
      "467 [D loss: 0.610410, acc.: 66.41%] [G loss: 0.869467]\n",
      "468 [D loss: 0.640396, acc.: 62.50%] [G loss: 0.859351]\n",
      "469 [D loss: 0.671654, acc.: 56.25%] [G loss: 0.802810]\n",
      "470 [D loss: 0.679647, acc.: 52.34%] [G loss: 0.840579]\n",
      "471 [D loss: 0.649730, acc.: 58.98%] [G loss: 0.893545]\n",
      "472 [D loss: 0.625001, acc.: 70.31%] [G loss: 0.961365]\n",
      "473 [D loss: 0.631975, acc.: 66.02%] [G loss: 0.976482]\n",
      "474 [D loss: 0.623021, acc.: 67.58%] [G loss: 1.008480]\n",
      "475 [D loss: 0.643856, acc.: 62.89%] [G loss: 0.960268]\n",
      "476 [D loss: 0.641309, acc.: 64.06%] [G loss: 0.931868]\n",
      "477 [D loss: 0.655979, acc.: 61.33%] [G loss: 0.914823]\n",
      "478 [D loss: 0.643066, acc.: 64.45%] [G loss: 0.959370]\n",
      "479 [D loss: 0.637461, acc.: 64.06%] [G loss: 0.964732]\n",
      "480 [D loss: 0.637099, acc.: 64.84%] [G loss: 0.988906]\n",
      "481 [D loss: 0.628188, acc.: 65.62%] [G loss: 0.989403]\n",
      "482 [D loss: 0.622532, acc.: 66.02%] [G loss: 0.966401]\n",
      "483 [D loss: 0.621782, acc.: 66.02%] [G loss: 0.934598]\n",
      "484 [D loss: 0.627182, acc.: 66.02%] [G loss: 0.897747]\n",
      "485 [D loss: 0.629524, acc.: 67.19%] [G loss: 0.883586]\n",
      "486 [D loss: 0.621386, acc.: 67.19%] [G loss: 0.869268]\n",
      "487 [D loss: 0.619760, acc.: 67.58%] [G loss: 0.846207]\n",
      "488 [D loss: 0.620240, acc.: 67.19%] [G loss: 0.847422]\n",
      "489 [D loss: 0.627856, acc.: 65.23%] [G loss: 0.842866]\n",
      "490 [D loss: 0.624632, acc.: 63.67%] [G loss: 0.860256]\n",
      "491 [D loss: 0.611694, acc.: 67.58%] [G loss: 0.841528]\n",
      "492 [D loss: 0.644725, acc.: 62.11%] [G loss: 0.821189]\n",
      "493 [D loss: 0.633979, acc.: 62.50%] [G loss: 0.836170]\n",
      "494 [D loss: 0.630899, acc.: 66.41%] [G loss: 0.831082]\n",
      "495 [D loss: 0.632303, acc.: 68.36%] [G loss: 0.849765]\n",
      "496 [D loss: 0.658649, acc.: 62.89%] [G loss: 0.834352]\n",
      "497 [D loss: 0.654784, acc.: 66.80%] [G loss: 0.805382]\n",
      "498 [D loss: 0.667267, acc.: 62.11%] [G loss: 0.825487]\n",
      "499 [D loss: 0.633843, acc.: 62.89%] [G loss: 0.903734]\n",
      "500 [D loss: 0.622747, acc.: 68.36%] [G loss: 0.927431]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "501 [D loss: 0.636020, acc.: 65.23%] [G loss: 0.972223]\n",
      "502 [D loss: 0.647387, acc.: 62.11%] [G loss: 0.957511]\n",
      "503 [D loss: 0.672643, acc.: 57.42%] [G loss: 0.931501]\n",
      "504 [D loss: 0.669195, acc.: 57.81%] [G loss: 0.964218]\n",
      "505 [D loss: 0.637437, acc.: 65.23%] [G loss: 1.016309]\n",
      "506 [D loss: 0.594047, acc.: 74.22%] [G loss: 1.066438]\n",
      "507 [D loss: 0.589095, acc.: 73.83%] [G loss: 1.107940]\n",
      "508 [D loss: 0.574691, acc.: 73.44%] [G loss: 1.088698]\n",
      "509 [D loss: 0.575395, acc.: 75.39%] [G loss: 1.067023]\n",
      "510 [D loss: 0.585824, acc.: 72.66%] [G loss: 1.034431]\n",
      "511 [D loss: 0.597759, acc.: 72.66%] [G loss: 1.005527]\n",
      "512 [D loss: 0.605498, acc.: 70.70%] [G loss: 0.962858]\n",
      "513 [D loss: 0.622540, acc.: 67.58%] [G loss: 0.931774]\n",
      "514 [D loss: 0.630507, acc.: 63.67%] [G loss: 0.908749]\n",
      "515 [D loss: 0.618927, acc.: 66.41%] [G loss: 0.876155]\n",
      "516 [D loss: 0.640537, acc.: 61.33%] [G loss: 0.867370]\n",
      "517 [D loss: 0.651855, acc.: 63.28%] [G loss: 0.832355]\n",
      "518 [D loss: 0.663240, acc.: 60.16%] [G loss: 0.873812]\n",
      "519 [D loss: 0.649855, acc.: 62.50%] [G loss: 0.921095]\n",
      "520 [D loss: 0.652044, acc.: 62.89%] [G loss: 0.931939]\n",
      "521 [D loss: 0.662528, acc.: 58.20%] [G loss: 0.930091]\n",
      "522 [D loss: 0.672991, acc.: 57.81%] [G loss: 0.913906]\n",
      "523 [D loss: 0.681857, acc.: 55.47%] [G loss: 0.930791]\n",
      "524 [D loss: 0.667348, acc.: 55.08%] [G loss: 0.974478]\n",
      "525 [D loss: 0.650956, acc.: 61.72%] [G loss: 0.972955]\n",
      "526 [D loss: 0.647633, acc.: 61.33%] [G loss: 0.976533]\n",
      "527 [D loss: 0.643642, acc.: 64.45%] [G loss: 0.973782]\n",
      "528 [D loss: 0.634741, acc.: 65.23%] [G loss: 0.968859]\n",
      "529 [D loss: 0.635154, acc.: 64.45%] [G loss: 0.953941]\n",
      "530 [D loss: 0.617472, acc.: 65.62%] [G loss: 0.950895]\n",
      "531 [D loss: 0.621996, acc.: 63.28%] [G loss: 0.934108]\n",
      "532 [D loss: 0.629204, acc.: 65.62%] [G loss: 0.890477]\n",
      "533 [D loss: 0.626632, acc.: 62.11%] [G loss: 0.913303]\n",
      "534 [D loss: 0.621865, acc.: 66.41%] [G loss: 0.882909]\n",
      "535 [D loss: 0.622584, acc.: 65.62%] [G loss: 0.869037]\n",
      "536 [D loss: 0.634304, acc.: 62.11%] [G loss: 0.827258]\n",
      "537 [D loss: 0.612844, acc.: 64.84%] [G loss: 0.848736]\n",
      "538 [D loss: 0.625653, acc.: 62.89%] [G loss: 0.865069]\n",
      "539 [D loss: 0.606501, acc.: 69.92%] [G loss: 0.868517]\n",
      "540 [D loss: 0.608032, acc.: 64.45%] [G loss: 0.862839]\n",
      "541 [D loss: 0.614828, acc.: 62.50%] [G loss: 0.935582]\n",
      "542 [D loss: 0.578753, acc.: 72.27%] [G loss: 1.012980]\n",
      "543 [D loss: 0.572640, acc.: 72.27%] [G loss: 1.029322]\n",
      "544 [D loss: 0.570290, acc.: 73.44%] [G loss: 1.027775]\n",
      "545 [D loss: 0.556020, acc.: 75.00%] [G loss: 1.024364]\n",
      "546 [D loss: 0.550325, acc.: 78.52%] [G loss: 0.982735]\n",
      "547 [D loss: 0.564568, acc.: 73.83%] [G loss: 0.944700]\n",
      "548 [D loss: 0.568871, acc.: 74.22%] [G loss: 0.915528]\n",
      "549 [D loss: 0.581986, acc.: 67.19%] [G loss: 0.868629]\n",
      "550 [D loss: 0.621961, acc.: 55.86%] [G loss: 0.842869]\n",
      "551 [D loss: 0.677988, acc.: 46.88%] [G loss: 0.787540]\n",
      "552 [D loss: 0.702820, acc.: 41.02%] [G loss: 0.807680]\n",
      "553 [D loss: 0.716353, acc.: 41.02%] [G loss: 0.864430]\n",
      "554 [D loss: 0.686784, acc.: 48.44%] [G loss: 0.870820]\n",
      "555 [D loss: 0.686126, acc.: 48.05%] [G loss: 0.978676]\n",
      "556 [D loss: 0.630906, acc.: 62.11%] [G loss: 1.161397]\n",
      "557 [D loss: 0.567961, acc.: 67.19%] [G loss: 1.254520]\n",
      "558 [D loss: 0.564771, acc.: 68.36%] [G loss: 1.222548]\n",
      "559 [D loss: 0.561082, acc.: 70.31%] [G loss: 1.164330]\n",
      "560 [D loss: 0.577026, acc.: 69.14%] [G loss: 1.034376]\n",
      "561 [D loss: 0.602131, acc.: 65.23%] [G loss: 0.960242]\n",
      "562 [D loss: 0.595786, acc.: 69.92%] [G loss: 0.917894]\n",
      "563 [D loss: 0.617031, acc.: 67.19%] [G loss: 0.886316]\n",
      "564 [D loss: 0.613360, acc.: 66.80%] [G loss: 0.874825]\n",
      "565 [D loss: 0.619578, acc.: 68.36%] [G loss: 0.851901]\n",
      "566 [D loss: 0.636515, acc.: 64.06%] [G loss: 0.842232]\n",
      "567 [D loss: 0.621098, acc.: 63.28%] [G loss: 0.838833]\n",
      "568 [D loss: 0.622148, acc.: 66.41%] [G loss: 0.838932]\n",
      "569 [D loss: 0.639020, acc.: 61.72%] [G loss: 0.834201]\n",
      "570 [D loss: 0.612984, acc.: 67.58%] [G loss: 0.850414]\n",
      "571 [D loss: 0.613062, acc.: 68.36%] [G loss: 0.854565]\n",
      "572 [D loss: 0.625981, acc.: 63.67%] [G loss: 0.886926]\n",
      "573 [D loss: 0.617377, acc.: 65.23%] [G loss: 0.894678]\n",
      "574 [D loss: 0.601173, acc.: 67.97%] [G loss: 0.905739]\n",
      "575 [D loss: 0.589501, acc.: 67.97%] [G loss: 0.932327]\n",
      "576 [D loss: 0.589739, acc.: 69.53%] [G loss: 0.937741]\n",
      "577 [D loss: 0.598527, acc.: 66.02%] [G loss: 0.943523]\n",
      "578 [D loss: 0.580998, acc.: 69.92%] [G loss: 0.946739]\n",
      "579 [D loss: 0.585602, acc.: 68.36%] [G loss: 0.974507]\n",
      "580 [D loss: 0.578346, acc.: 66.80%] [G loss: 1.008271]\n",
      "581 [D loss: 0.574639, acc.: 69.92%] [G loss: 1.009726]\n",
      "582 [D loss: 0.588997, acc.: 69.92%] [G loss: 0.962830]\n",
      "583 [D loss: 0.593060, acc.: 70.31%] [G loss: 0.964921]\n",
      "584 [D loss: 0.598187, acc.: 69.53%] [G loss: 0.959802]\n",
      "585 [D loss: 0.597247, acc.: 68.75%] [G loss: 0.949234]\n",
      "586 [D loss: 0.596434, acc.: 69.53%] [G loss: 0.965819]\n",
      "587 [D loss: 0.598799, acc.: 69.14%] [G loss: 0.991665]\n",
      "588 [D loss: 0.598421, acc.: 66.80%] [G loss: 1.014091]\n",
      "589 [D loss: 0.586983, acc.: 66.02%] [G loss: 1.043815]\n",
      "590 [D loss: 0.587455, acc.: 66.80%] [G loss: 1.106811]\n",
      "591 [D loss: 0.587810, acc.: 66.80%] [G loss: 1.088118]\n",
      "592 [D loss: 0.593102, acc.: 64.45%] [G loss: 1.024788]\n",
      "593 [D loss: 0.605354, acc.: 66.80%] [G loss: 0.954952]\n",
      "594 [D loss: 0.630149, acc.: 68.36%] [G loss: 0.915872]\n",
      "595 [D loss: 0.631001, acc.: 66.02%] [G loss: 0.942730]\n",
      "596 [D loss: 0.628618, acc.: 64.06%] [G loss: 1.001523]\n",
      "597 [D loss: 0.587974, acc.: 66.02%] [G loss: 1.075309]\n",
      "598 [D loss: 0.591980, acc.: 66.80%] [G loss: 1.048299]\n",
      "599 [D loss: 0.589516, acc.: 67.19%] [G loss: 0.996700]\n",
      "600 [D loss: 0.606461, acc.: 65.62%] [G loss: 0.959715]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "601 [D loss: 0.606418, acc.: 64.06%] [G loss: 0.907908]\n",
      "602 [D loss: 0.615969, acc.: 64.06%] [G loss: 0.895284]\n",
      "603 [D loss: 0.607945, acc.: 65.23%] [G loss: 0.882150]\n",
      "604 [D loss: 0.622431, acc.: 64.45%] [G loss: 0.881160]\n",
      "605 [D loss: 0.608671, acc.: 66.80%] [G loss: 0.918230]\n",
      "606 [D loss: 0.586756, acc.: 71.48%] [G loss: 0.937169]\n",
      "607 [D loss: 0.573556, acc.: 72.66%] [G loss: 1.010333]\n",
      "608 [D loss: 0.560209, acc.: 73.05%] [G loss: 1.003763]\n",
      "609 [D loss: 0.560316, acc.: 71.48%] [G loss: 0.977657]\n",
      "610 [D loss: 0.564685, acc.: 71.09%] [G loss: 1.010773]\n",
      "611 [D loss: 0.562756, acc.: 71.88%] [G loss: 0.939521]\n",
      "612 [D loss: 0.571826, acc.: 69.92%] [G loss: 0.966613]\n",
      "613 [D loss: 0.597223, acc.: 68.75%] [G loss: 0.953179]\n",
      "614 [D loss: 0.593412, acc.: 68.75%] [G loss: 0.957116]\n",
      "615 [D loss: 0.649538, acc.: 60.94%] [G loss: 0.954943]\n",
      "616 [D loss: 0.629938, acc.: 64.45%] [G loss: 0.993088]\n",
      "617 [D loss: 0.654494, acc.: 60.94%] [G loss: 0.945422]\n",
      "618 [D loss: 0.660763, acc.: 60.55%] [G loss: 0.930473]\n",
      "619 [D loss: 0.663167, acc.: 55.47%] [G loss: 0.904497]\n",
      "620 [D loss: 0.651789, acc.: 60.55%] [G loss: 0.914899]\n",
      "621 [D loss: 0.628100, acc.: 68.75%] [G loss: 0.992664]\n",
      "622 [D loss: 0.610964, acc.: 69.14%] [G loss: 1.000265]\n",
      "623 [D loss: 0.567619, acc.: 71.09%] [G loss: 1.065425]\n",
      "624 [D loss: 0.537919, acc.: 74.22%] [G loss: 1.100637]\n",
      "625 [D loss: 0.537476, acc.: 73.83%] [G loss: 1.150717]\n",
      "626 [D loss: 0.542433, acc.: 78.12%] [G loss: 1.161554]\n",
      "627 [D loss: 0.558109, acc.: 77.34%] [G loss: 1.191242]\n",
      "628 [D loss: 0.534203, acc.: 78.91%] [G loss: 1.124587]\n",
      "629 [D loss: 0.544526, acc.: 80.08%] [G loss: 1.161433]\n",
      "630 [D loss: 0.547024, acc.: 78.12%] [G loss: 1.151912]\n",
      "631 [D loss: 0.548736, acc.: 76.56%] [G loss: 1.136657]\n",
      "632 [D loss: 0.541462, acc.: 77.73%] [G loss: 1.155549]\n",
      "633 [D loss: 0.556058, acc.: 75.00%] [G loss: 1.071021]\n",
      "634 [D loss: 0.562457, acc.: 73.44%] [G loss: 1.048841]\n",
      "635 [D loss: 0.570910, acc.: 71.48%] [G loss: 1.013196]\n",
      "636 [D loss: 0.593842, acc.: 66.80%] [G loss: 0.987379]\n",
      "637 [D loss: 0.583583, acc.: 67.19%] [G loss: 0.994347]\n",
      "638 [D loss: 0.615797, acc.: 59.77%] [G loss: 0.999748]\n",
      "639 [D loss: 0.592066, acc.: 62.11%] [G loss: 0.958027]\n",
      "640 [D loss: 0.598272, acc.: 60.16%] [G loss: 0.946318]\n",
      "641 [D loss: 0.582161, acc.: 66.80%] [G loss: 0.966006]\n",
      "642 [D loss: 0.569359, acc.: 71.88%] [G loss: 0.983251]\n",
      "643 [D loss: 0.541743, acc.: 76.95%] [G loss: 0.972510]\n",
      "644 [D loss: 0.533995, acc.: 76.56%] [G loss: 1.016271]\n",
      "645 [D loss: 0.534915, acc.: 77.34%] [G loss: 1.008067]\n",
      "646 [D loss: 0.543235, acc.: 77.73%] [G loss: 1.105981]\n",
      "647 [D loss: 0.501474, acc.: 77.73%] [G loss: 1.276291]\n",
      "648 [D loss: 0.468102, acc.: 80.86%] [G loss: 1.292815]\n",
      "649 [D loss: 0.482781, acc.: 80.86%] [G loss: 1.230781]\n",
      "650 [D loss: 0.521063, acc.: 72.27%] [G loss: 1.201686]\n",
      "651 [D loss: 0.546582, acc.: 72.27%] [G loss: 1.265258]\n",
      "652 [D loss: 0.559875, acc.: 72.27%] [G loss: 1.273313]\n",
      "653 [D loss: 0.600908, acc.: 69.14%] [G loss: 1.237591]\n",
      "654 [D loss: 0.591285, acc.: 72.27%] [G loss: 1.156329]\n",
      "655 [D loss: 0.658525, acc.: 62.50%] [G loss: 1.087033]\n",
      "656 [D loss: 0.660233, acc.: 63.28%] [G loss: 1.037420]\n",
      "657 [D loss: 0.659269, acc.: 62.50%] [G loss: 1.073591]\n",
      "658 [D loss: 0.637831, acc.: 66.02%] [G loss: 1.072146]\n",
      "659 [D loss: 0.653291, acc.: 63.67%] [G loss: 1.104658]\n",
      "660 [D loss: 0.650789, acc.: 64.06%] [G loss: 1.089168]\n",
      "661 [D loss: 0.641655, acc.: 62.11%] [G loss: 1.077768]\n",
      "662 [D loss: 0.651831, acc.: 59.38%] [G loss: 1.148418]\n",
      "663 [D loss: 0.600361, acc.: 66.41%] [G loss: 1.193144]\n",
      "664 [D loss: 0.592574, acc.: 67.97%] [G loss: 1.148928]\n",
      "665 [D loss: 0.570966, acc.: 69.53%] [G loss: 1.128217]\n",
      "666 [D loss: 0.551171, acc.: 72.66%] [G loss: 1.128310]\n",
      "667 [D loss: 0.555596, acc.: 71.88%] [G loss: 1.089260]\n",
      "668 [D loss: 0.538459, acc.: 75.00%] [G loss: 1.101401]\n",
      "669 [D loss: 0.515222, acc.: 75.39%] [G loss: 1.090539]\n",
      "670 [D loss: 0.546706, acc.: 71.88%] [G loss: 1.101168]\n",
      "671 [D loss: 0.540519, acc.: 71.88%] [G loss: 1.073029]\n",
      "672 [D loss: 0.563493, acc.: 69.14%] [G loss: 1.013069]\n",
      "673 [D loss: 0.580328, acc.: 67.58%] [G loss: 0.990074]\n",
      "674 [D loss: 0.619881, acc.: 65.62%] [G loss: 0.954068]\n",
      "675 [D loss: 0.665829, acc.: 58.20%] [G loss: 0.998959]\n",
      "676 [D loss: 0.628600, acc.: 62.50%] [G loss: 0.999722]\n",
      "677 [D loss: 0.706970, acc.: 57.42%] [G loss: 1.003415]\n",
      "678 [D loss: 0.679760, acc.: 56.25%] [G loss: 0.979111]\n",
      "679 [D loss: 0.685108, acc.: 54.69%] [G loss: 1.094720]\n",
      "680 [D loss: 0.655020, acc.: 60.94%] [G loss: 1.156318]\n",
      "681 [D loss: 0.594577, acc.: 63.67%] [G loss: 1.232715]\n",
      "682 [D loss: 0.538215, acc.: 72.27%] [G loss: 1.298977]\n",
      "683 [D loss: 0.513577, acc.: 74.22%] [G loss: 1.232792]\n",
      "684 [D loss: 0.515300, acc.: 75.78%] [G loss: 1.163082]\n",
      "685 [D loss: 0.520125, acc.: 76.56%] [G loss: 1.128810]\n",
      "686 [D loss: 0.547504, acc.: 74.22%] [G loss: 1.093580]\n",
      "687 [D loss: 0.566630, acc.: 73.83%] [G loss: 0.991960]\n",
      "688 [D loss: 0.589936, acc.: 66.41%] [G loss: 1.008972]\n",
      "689 [D loss: 0.603247, acc.: 67.58%] [G loss: 1.066734]\n",
      "690 [D loss: 0.587411, acc.: 66.80%] [G loss: 1.117758]\n",
      "691 [D loss: 0.613638, acc.: 62.89%] [G loss: 1.173200]\n",
      "692 [D loss: 0.563720, acc.: 69.14%] [G loss: 1.259885]\n",
      "693 [D loss: 0.587776, acc.: 65.23%] [G loss: 1.251852]\n",
      "694 [D loss: 0.612510, acc.: 63.67%] [G loss: 1.196335]\n",
      "695 [D loss: 0.587799, acc.: 66.41%] [G loss: 1.322158]\n",
      "696 [D loss: 0.579539, acc.: 67.58%] [G loss: 1.309664]\n",
      "697 [D loss: 0.572731, acc.: 69.92%] [G loss: 1.294914]\n",
      "698 [D loss: 0.585710, acc.: 67.58%] [G loss: 1.184789]\n",
      "699 [D loss: 0.577978, acc.: 69.53%] [G loss: 1.069047]\n",
      "700 [D loss: 0.601646, acc.: 66.80%] [G loss: 1.076620]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "701 [D loss: 0.609743, acc.: 65.62%] [G loss: 0.974776]\n",
      "702 [D loss: 0.622274, acc.: 61.33%] [G loss: 1.017759]\n",
      "703 [D loss: 0.635628, acc.: 60.55%] [G loss: 0.965189]\n",
      "704 [D loss: 0.602345, acc.: 64.06%] [G loss: 1.000608]\n",
      "705 [D loss: 0.596586, acc.: 64.84%] [G loss: 0.942123]\n",
      "706 [D loss: 0.597703, acc.: 65.23%] [G loss: 0.983612]\n",
      "707 [D loss: 0.556323, acc.: 74.22%] [G loss: 1.023284]\n",
      "708 [D loss: 0.566532, acc.: 71.09%] [G loss: 1.038001]\n",
      "709 [D loss: 0.553349, acc.: 71.88%] [G loss: 1.068739]\n",
      "710 [D loss: 0.551564, acc.: 73.83%] [G loss: 1.017786]\n",
      "711 [D loss: 0.547188, acc.: 74.61%] [G loss: 1.007367]\n",
      "712 [D loss: 0.557972, acc.: 73.44%] [G loss: 0.996210]\n",
      "713 [D loss: 0.558796, acc.: 73.44%] [G loss: 0.998292]\n",
      "714 [D loss: 0.569740, acc.: 71.09%] [G loss: 1.018047]\n",
      "715 [D loss: 0.545327, acc.: 74.61%] [G loss: 1.020890]\n",
      "716 [D loss: 0.545977, acc.: 76.56%] [G loss: 1.018194]\n",
      "717 [D loss: 0.534754, acc.: 78.52%] [G loss: 1.057989]\n",
      "718 [D loss: 0.512654, acc.: 77.73%] [G loss: 1.038855]\n",
      "719 [D loss: 0.516631, acc.: 77.73%] [G loss: 1.071777]\n",
      "720 [D loss: 0.511613, acc.: 79.69%] [G loss: 1.094288]\n",
      "721 [D loss: 0.502793, acc.: 78.12%] [G loss: 1.115207]\n",
      "722 [D loss: 0.537294, acc.: 71.48%] [G loss: 1.169063]\n",
      "723 [D loss: 0.552089, acc.: 71.09%] [G loss: 1.299599]\n",
      "724 [D loss: 0.588672, acc.: 65.62%] [G loss: 1.432202]\n",
      "725 [D loss: 0.507486, acc.: 76.56%] [G loss: 1.544847]\n",
      "726 [D loss: 0.522097, acc.: 76.95%] [G loss: 1.512146]\n",
      "727 [D loss: 0.503704, acc.: 75.39%] [G loss: 1.386129]\n",
      "728 [D loss: 0.527094, acc.: 75.39%] [G loss: 1.218368]\n",
      "729 [D loss: 0.582382, acc.: 69.53%] [G loss: 1.126034]\n",
      "730 [D loss: 0.631685, acc.: 63.28%] [G loss: 1.097820]\n",
      "731 [D loss: 0.587426, acc.: 69.53%] [G loss: 1.106391]\n",
      "732 [D loss: 0.596778, acc.: 68.75%] [G loss: 1.100023]\n",
      "733 [D loss: 0.623702, acc.: 66.02%] [G loss: 1.167896]\n",
      "734 [D loss: 0.580638, acc.: 68.75%] [G loss: 1.295489]\n",
      "735 [D loss: 0.564704, acc.: 71.09%] [G loss: 1.268415]\n",
      "736 [D loss: 0.590903, acc.: 68.75%] [G loss: 1.281237]\n",
      "737 [D loss: 0.560845, acc.: 71.88%] [G loss: 1.219165]\n",
      "738 [D loss: 0.562958, acc.: 71.48%] [G loss: 1.168323]\n",
      "739 [D loss: 0.569524, acc.: 66.80%] [G loss: 1.246996]\n",
      "740 [D loss: 0.576226, acc.: 67.19%] [G loss: 1.294937]\n",
      "741 [D loss: 0.530612, acc.: 73.05%] [G loss: 1.322445]\n",
      "742 [D loss: 0.538416, acc.: 72.66%] [G loss: 1.228158]\n",
      "743 [D loss: 0.546957, acc.: 73.83%] [G loss: 1.204353]\n",
      "744 [D loss: 0.531204, acc.: 74.61%] [G loss: 1.139925]\n",
      "745 [D loss: 0.533835, acc.: 75.39%] [G loss: 1.114044]\n",
      "746 [D loss: 0.541851, acc.: 71.09%] [G loss: 1.084432]\n",
      "747 [D loss: 0.532301, acc.: 76.17%] [G loss: 1.053446]\n",
      "748 [D loss: 0.530914, acc.: 75.00%] [G loss: 1.063394]\n",
      "749 [D loss: 0.534642, acc.: 72.66%] [G loss: 1.112927]\n",
      "750 [D loss: 0.529668, acc.: 73.83%] [G loss: 1.195774]\n",
      "751 [D loss: 0.512980, acc.: 73.83%] [G loss: 1.168340]\n",
      "752 [D loss: 0.515518, acc.: 73.05%] [G loss: 1.159539]\n",
      "753 [D loss: 0.547655, acc.: 68.75%] [G loss: 1.102914]\n",
      "754 [D loss: 0.535048, acc.: 71.48%] [G loss: 1.110031]\n",
      "755 [D loss: 0.540173, acc.: 69.14%] [G loss: 1.110054]\n",
      "756 [D loss: 0.567392, acc.: 66.80%] [G loss: 1.019247]\n",
      "757 [D loss: 0.593954, acc.: 64.45%] [G loss: 1.062309]\n",
      "758 [D loss: 0.671879, acc.: 58.59%] [G loss: 1.033442]\n",
      "759 [D loss: 0.689314, acc.: 57.03%] [G loss: 1.164766]\n",
      "760 [D loss: 0.640871, acc.: 60.16%] [G loss: 1.275444]\n",
      "761 [D loss: 0.621990, acc.: 64.45%] [G loss: 1.236432]\n",
      "762 [D loss: 0.693851, acc.: 60.94%] [G loss: 1.072841]\n",
      "763 [D loss: 0.664514, acc.: 61.33%] [G loss: 1.016521]\n",
      "764 [D loss: 0.646794, acc.: 63.67%] [G loss: 0.956784]\n",
      "765 [D loss: 0.632554, acc.: 63.67%] [G loss: 0.940715]\n",
      "766 [D loss: 0.618952, acc.: 61.33%] [G loss: 0.982013]\n",
      "767 [D loss: 0.605520, acc.: 66.41%] [G loss: 0.990656]\n",
      "768 [D loss: 0.588990, acc.: 66.02%] [G loss: 0.973806]\n",
      "769 [D loss: 0.577255, acc.: 70.70%] [G loss: 0.918475]\n",
      "770 [D loss: 0.579009, acc.: 69.53%] [G loss: 0.909368]\n",
      "771 [D loss: 0.550846, acc.: 73.83%] [G loss: 0.942195]\n",
      "772 [D loss: 0.558567, acc.: 71.09%] [G loss: 0.931904]\n",
      "773 [D loss: 0.535743, acc.: 73.83%] [G loss: 0.967278]\n",
      "774 [D loss: 0.538484, acc.: 75.00%] [G loss: 0.927009]\n",
      "775 [D loss: 0.539549, acc.: 74.61%] [G loss: 0.924265]\n",
      "776 [D loss: 0.576520, acc.: 73.83%] [G loss: 0.903811]\n",
      "777 [D loss: 0.576194, acc.: 71.88%] [G loss: 0.908075]\n",
      "778 [D loss: 0.585704, acc.: 68.36%] [G loss: 0.974721]\n",
      "779 [D loss: 0.573896, acc.: 68.75%] [G loss: 1.039033]\n",
      "780 [D loss: 0.551737, acc.: 73.05%] [G loss: 1.028756]\n",
      "781 [D loss: 0.560399, acc.: 71.88%] [G loss: 1.052213]\n",
      "782 [D loss: 0.529564, acc.: 76.17%] [G loss: 0.999502]\n",
      "783 [D loss: 0.513750, acc.: 77.73%] [G loss: 1.030657]\n",
      "784 [D loss: 0.514171, acc.: 76.17%] [G loss: 1.005961]\n",
      "785 [D loss: 0.500777, acc.: 78.91%] [G loss: 1.020526]\n",
      "786 [D loss: 0.515616, acc.: 76.95%] [G loss: 0.994958]\n",
      "787 [D loss: 0.514819, acc.: 78.52%] [G loss: 1.002284]\n",
      "788 [D loss: 0.525906, acc.: 74.61%] [G loss: 0.913700]\n",
      "789 [D loss: 0.568621, acc.: 69.92%] [G loss: 0.879282]\n",
      "790 [D loss: 0.585220, acc.: 67.58%] [G loss: 0.883882]\n",
      "791 [D loss: 0.594052, acc.: 67.19%] [G loss: 0.931249]\n",
      "792 [D loss: 0.567190, acc.: 70.70%] [G loss: 0.956619]\n",
      "793 [D loss: 0.596333, acc.: 65.62%] [G loss: 1.071109]\n",
      "794 [D loss: 0.552485, acc.: 71.09%] [G loss: 1.135798]\n",
      "795 [D loss: 0.553161, acc.: 69.53%] [G loss: 1.089180]\n",
      "796 [D loss: 0.550629, acc.: 69.53%] [G loss: 1.079032]\n",
      "797 [D loss: 0.556117, acc.: 69.92%] [G loss: 1.135137]\n",
      "798 [D loss: 0.524226, acc.: 72.66%] [G loss: 1.079392]\n",
      "799 [D loss: 0.571563, acc.: 66.80%] [G loss: 1.064823]\n",
      "800 [D loss: 0.576455, acc.: 66.80%] [G loss: 1.121866]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "801 [D loss: 0.584194, acc.: 68.36%] [G loss: 1.143129]\n",
      "802 [D loss: 0.590881, acc.: 67.58%] [G loss: 1.117400]\n",
      "803 [D loss: 0.600958, acc.: 66.41%] [G loss: 1.037210]\n",
      "804 [D loss: 0.618594, acc.: 61.33%] [G loss: 1.014333]\n",
      "805 [D loss: 0.597806, acc.: 68.36%] [G loss: 1.013094]\n",
      "806 [D loss: 0.591729, acc.: 68.75%] [G loss: 0.943808]\n",
      "807 [D loss: 0.588962, acc.: 68.75%] [G loss: 0.992106]\n",
      "808 [D loss: 0.564892, acc.: 70.70%] [G loss: 1.036291]\n",
      "809 [D loss: 0.553394, acc.: 71.48%] [G loss: 1.055449]\n",
      "810 [D loss: 0.532465, acc.: 75.00%] [G loss: 1.015154]\n",
      "811 [D loss: 0.534599, acc.: 73.83%] [G loss: 1.017204]\n",
      "812 [D loss: 0.539909, acc.: 73.05%] [G loss: 1.023518]\n",
      "813 [D loss: 0.555501, acc.: 73.83%] [G loss: 1.030609]\n",
      "814 [D loss: 0.584932, acc.: 66.80%] [G loss: 1.002447]\n",
      "815 [D loss: 0.599621, acc.: 68.75%] [G loss: 0.963574]\n",
      "816 [D loss: 0.622762, acc.: 62.50%] [G loss: 0.965052]\n",
      "817 [D loss: 0.623819, acc.: 62.11%] [G loss: 0.995447]\n",
      "818 [D loss: 0.626164, acc.: 61.33%] [G loss: 1.042064]\n",
      "819 [D loss: 0.534263, acc.: 71.48%] [G loss: 1.203500]\n",
      "820 [D loss: 0.546073, acc.: 69.53%] [G loss: 1.197904]\n",
      "821 [D loss: 0.562262, acc.: 67.58%] [G loss: 1.139642]\n",
      "822 [D loss: 0.562441, acc.: 68.75%] [G loss: 1.016091]\n",
      "823 [D loss: 0.574740, acc.: 68.75%] [G loss: 0.988888]\n",
      "824 [D loss: 0.612610, acc.: 60.55%] [G loss: 0.979187]\n",
      "825 [D loss: 0.601242, acc.: 62.89%] [G loss: 1.024750]\n",
      "826 [D loss: 0.588012, acc.: 65.62%] [G loss: 1.074106]\n",
      "827 [D loss: 0.575280, acc.: 69.92%] [G loss: 1.037658]\n",
      "828 [D loss: 0.585574, acc.: 66.80%] [G loss: 1.022998]\n",
      "829 [D loss: 0.578329, acc.: 67.97%] [G loss: 0.979363]\n",
      "830 [D loss: 0.594747, acc.: 66.41%] [G loss: 1.007310]\n",
      "831 [D loss: 0.588908, acc.: 67.97%] [G loss: 0.976919]\n",
      "832 [D loss: 0.631818, acc.: 63.28%] [G loss: 1.028827]\n",
      "833 [D loss: 0.628321, acc.: 62.89%] [G loss: 1.026682]\n",
      "834 [D loss: 0.608679, acc.: 66.80%] [G loss: 1.050720]\n",
      "835 [D loss: 0.611796, acc.: 66.41%] [G loss: 1.040087]\n",
      "836 [D loss: 0.610797, acc.: 62.89%] [G loss: 1.011239]\n",
      "837 [D loss: 0.626216, acc.: 62.89%] [G loss: 1.045949]\n",
      "838 [D loss: 0.640307, acc.: 60.94%] [G loss: 1.173098]\n",
      "839 [D loss: 0.571159, acc.: 67.58%] [G loss: 1.163368]\n",
      "840 [D loss: 0.592423, acc.: 67.19%] [G loss: 1.152535]\n",
      "841 [D loss: 0.604330, acc.: 64.45%] [G loss: 1.057061]\n",
      "842 [D loss: 0.585222, acc.: 69.53%] [G loss: 0.968607]\n",
      "843 [D loss: 0.613188, acc.: 63.67%] [G loss: 0.999161]\n",
      "844 [D loss: 0.590786, acc.: 67.97%] [G loss: 0.994961]\n",
      "845 [D loss: 0.576942, acc.: 68.36%] [G loss: 1.075976]\n",
      "846 [D loss: 0.571761, acc.: 67.97%] [G loss: 1.022873]\n",
      "847 [D loss: 0.573327, acc.: 66.80%] [G loss: 1.052909]\n",
      "848 [D loss: 0.577352, acc.: 68.36%] [G loss: 1.035998]\n",
      "849 [D loss: 0.590867, acc.: 67.19%] [G loss: 1.066320]\n",
      "850 [D loss: 0.574131, acc.: 67.19%] [G loss: 1.055304]\n",
      "851 [D loss: 0.587128, acc.: 68.75%] [G loss: 1.012863]\n",
      "852 [D loss: 0.580936, acc.: 68.36%] [G loss: 0.985647]\n",
      "853 [D loss: 0.592402, acc.: 67.19%] [G loss: 1.008124]\n",
      "854 [D loss: 0.564056, acc.: 69.92%] [G loss: 1.023558]\n",
      "855 [D loss: 0.572397, acc.: 68.36%] [G loss: 0.979503]\n",
      "856 [D loss: 0.588623, acc.: 66.41%] [G loss: 1.027881]\n",
      "857 [D loss: 0.568686, acc.: 67.58%] [G loss: 1.032091]\n",
      "858 [D loss: 0.571966, acc.: 69.53%] [G loss: 1.024852]\n",
      "859 [D loss: 0.579227, acc.: 63.67%] [G loss: 1.104239]\n",
      "860 [D loss: 0.539075, acc.: 71.48%] [G loss: 1.102280]\n",
      "861 [D loss: 0.540913, acc.: 74.22%] [G loss: 1.053999]\n",
      "862 [D loss: 0.556684, acc.: 71.48%] [G loss: 1.052707]\n",
      "863 [D loss: 0.555593, acc.: 72.66%] [G loss: 1.076104]\n",
      "864 [D loss: 0.555309, acc.: 73.44%] [G loss: 1.043967]\n",
      "865 [D loss: 0.559872, acc.: 73.83%] [G loss: 1.020219]\n",
      "866 [D loss: 0.564631, acc.: 73.83%] [G loss: 0.974192]\n",
      "867 [D loss: 0.568402, acc.: 73.44%] [G loss: 0.967487]\n",
      "868 [D loss: 0.587404, acc.: 69.53%] [G loss: 0.950219]\n",
      "869 [D loss: 0.578239, acc.: 70.31%] [G loss: 0.956617]\n",
      "870 [D loss: 0.553010, acc.: 77.34%] [G loss: 1.006137]\n",
      "871 [D loss: 0.551112, acc.: 76.17%] [G loss: 0.992092]\n",
      "872 [D loss: 0.542082, acc.: 76.95%] [G loss: 1.001943]\n",
      "873 [D loss: 0.529390, acc.: 76.56%] [G loss: 1.003577]\n",
      "874 [D loss: 0.515195, acc.: 76.17%] [G loss: 1.017045]\n",
      "875 [D loss: 0.518264, acc.: 76.17%] [G loss: 1.044606]\n",
      "876 [D loss: 0.493487, acc.: 79.69%] [G loss: 1.066240]\n",
      "877 [D loss: 0.497731, acc.: 79.69%] [G loss: 1.089663]\n",
      "878 [D loss: 0.500772, acc.: 80.47%] [G loss: 1.090966]\n",
      "879 [D loss: 0.526226, acc.: 73.83%] [G loss: 1.078617]\n",
      "880 [D loss: 0.551597, acc.: 71.48%] [G loss: 1.068323]\n",
      "881 [D loss: 0.547917, acc.: 72.66%] [G loss: 1.061988]\n",
      "882 [D loss: 0.579514, acc.: 66.41%] [G loss: 0.995638]\n",
      "883 [D loss: 0.577459, acc.: 68.75%] [G loss: 1.051600]\n",
      "884 [D loss: 0.569704, acc.: 67.58%] [G loss: 1.170793]\n",
      "885 [D loss: 0.531661, acc.: 71.48%] [G loss: 1.250870]\n",
      "886 [D loss: 0.522928, acc.: 72.66%] [G loss: 1.260364]\n",
      "887 [D loss: 0.515024, acc.: 71.88%] [G loss: 1.186884]\n",
      "888 [D loss: 0.524150, acc.: 72.27%] [G loss: 1.149263]\n",
      "889 [D loss: 0.556431, acc.: 68.36%] [G loss: 1.175074]\n",
      "890 [D loss: 0.547554, acc.: 70.31%] [G loss: 1.118860]\n",
      "891 [D loss: 0.592280, acc.: 64.84%] [G loss: 1.060384]\n",
      "892 [D loss: 0.586998, acc.: 61.72%] [G loss: 1.037726]\n",
      "893 [D loss: 0.593346, acc.: 60.55%] [G loss: 1.036934]\n",
      "894 [D loss: 0.568509, acc.: 68.36%] [G loss: 1.039212]\n",
      "895 [D loss: 0.553522, acc.: 72.27%] [G loss: 1.066893]\n",
      "896 [D loss: 0.543793, acc.: 73.05%] [G loss: 1.119717]\n",
      "897 [D loss: 0.523458, acc.: 72.66%] [G loss: 1.188276]\n",
      "898 [D loss: 0.499679, acc.: 75.39%] [G loss: 1.179406]\n",
      "899 [D loss: 0.538512, acc.: 76.17%] [G loss: 1.135274]\n",
      "900 [D loss: 0.538759, acc.: 73.83%] [G loss: 1.125453]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "901 [D loss: 0.532594, acc.: 74.22%] [G loss: 1.095069]\n",
      "902 [D loss: 0.530994, acc.: 76.56%] [G loss: 1.045902]\n",
      "903 [D loss: 0.523587, acc.: 78.52%] [G loss: 1.081618]\n",
      "904 [D loss: 0.578445, acc.: 71.88%] [G loss: 1.150638]\n",
      "905 [D loss: 0.539762, acc.: 74.22%] [G loss: 1.420202]\n",
      "906 [D loss: 0.486730, acc.: 77.34%] [G loss: 1.464209]\n",
      "907 [D loss: 0.488268, acc.: 77.73%] [G loss: 1.372009]\n",
      "908 [D loss: 0.499777, acc.: 77.34%] [G loss: 1.177466]\n",
      "909 [D loss: 0.493948, acc.: 77.73%] [G loss: 1.180285]\n",
      "910 [D loss: 0.527248, acc.: 73.44%] [G loss: 1.063294]\n",
      "911 [D loss: 0.516747, acc.: 75.00%] [G loss: 1.027521]\n",
      "912 [D loss: 0.575870, acc.: 67.97%] [G loss: 1.025626]\n",
      "913 [D loss: 0.584644, acc.: 67.58%] [G loss: 1.055076]\n",
      "914 [D loss: 0.599189, acc.: 64.45%] [G loss: 1.092832]\n",
      "915 [D loss: 0.570867, acc.: 71.48%] [G loss: 1.104993]\n",
      "916 [D loss: 0.576225, acc.: 67.58%] [G loss: 1.062807]\n",
      "917 [D loss: 0.550120, acc.: 69.53%] [G loss: 1.005638]\n",
      "918 [D loss: 0.562691, acc.: 71.09%] [G loss: 1.050622]\n",
      "919 [D loss: 0.579844, acc.: 67.58%] [G loss: 0.988849]\n",
      "920 [D loss: 0.544070, acc.: 72.66%] [G loss: 1.025310]\n",
      "921 [D loss: 0.559965, acc.: 71.09%] [G loss: 1.048172]\n",
      "922 [D loss: 0.543899, acc.: 72.27%] [G loss: 1.010053]\n",
      "923 [D loss: 0.531157, acc.: 74.61%] [G loss: 1.062109]\n",
      "924 [D loss: 0.546943, acc.: 74.22%] [G loss: 1.062029]\n",
      "925 [D loss: 0.546067, acc.: 70.31%] [G loss: 1.180334]\n",
      "926 [D loss: 0.541671, acc.: 74.22%] [G loss: 1.117431]\n",
      "927 [D loss: 0.543968, acc.: 71.09%] [G loss: 1.153827]\n",
      "928 [D loss: 0.536969, acc.: 75.39%] [G loss: 1.194707]\n",
      "929 [D loss: 0.545633, acc.: 71.48%] [G loss: 1.221316]\n",
      "930 [D loss: 0.514600, acc.: 76.95%] [G loss: 1.157311]\n",
      "931 [D loss: 0.524192, acc.: 74.61%] [G loss: 1.172474]\n",
      "932 [D loss: 0.507902, acc.: 76.56%] [G loss: 1.140466]\n",
      "933 [D loss: 0.524631, acc.: 73.83%] [G loss: 1.131127]\n",
      "934 [D loss: 0.532961, acc.: 74.61%] [G loss: 1.051795]\n",
      "935 [D loss: 0.556301, acc.: 71.09%] [G loss: 1.157707]\n",
      "936 [D loss: 0.536007, acc.: 72.27%] [G loss: 1.156289]\n",
      "937 [D loss: 0.542034, acc.: 71.88%] [G loss: 1.163472]\n",
      "938 [D loss: 0.543290, acc.: 71.88%] [G loss: 1.107315]\n",
      "939 [D loss: 0.549243, acc.: 71.09%] [G loss: 1.052255]\n",
      "940 [D loss: 0.565443, acc.: 68.36%] [G loss: 1.045354]\n",
      "941 [D loss: 0.557460, acc.: 68.75%] [G loss: 1.055588]\n",
      "942 [D loss: 0.592847, acc.: 65.23%] [G loss: 1.059834]\n",
      "943 [D loss: 0.565865, acc.: 69.53%] [G loss: 1.084531]\n",
      "944 [D loss: 0.554934, acc.: 71.88%] [G loss: 1.072979]\n",
      "945 [D loss: 0.548598, acc.: 71.48%] [G loss: 1.069757]\n",
      "946 [D loss: 0.544897, acc.: 74.61%] [G loss: 1.126740]\n",
      "947 [D loss: 0.512686, acc.: 75.39%] [G loss: 1.170782]\n",
      "948 [D loss: 0.506712, acc.: 78.12%] [G loss: 1.174565]\n",
      "949 [D loss: 0.495107, acc.: 81.25%] [G loss: 1.125127]\n",
      "950 [D loss: 0.508832, acc.: 77.34%] [G loss: 1.202592]\n",
      "951 [D loss: 0.493152, acc.: 79.30%] [G loss: 1.204090]\n",
      "952 [D loss: 0.495628, acc.: 78.91%] [G loss: 1.203466]\n",
      "953 [D loss: 0.483581, acc.: 78.91%] [G loss: 1.222645]\n",
      "954 [D loss: 0.487082, acc.: 79.69%] [G loss: 1.179107]\n",
      "955 [D loss: 0.495737, acc.: 78.12%] [G loss: 1.133987]\n",
      "956 [D loss: 0.518443, acc.: 76.56%] [G loss: 1.146919]\n",
      "957 [D loss: 0.512563, acc.: 75.78%] [G loss: 1.145964]\n",
      "958 [D loss: 0.538655, acc.: 74.22%] [G loss: 1.185274]\n",
      "959 [D loss: 0.526102, acc.: 74.22%] [G loss: 1.266894]\n",
      "960 [D loss: 0.521159, acc.: 73.83%] [G loss: 1.264711]\n",
      "961 [D loss: 0.518179, acc.: 77.34%] [G loss: 1.224198]\n",
      "962 [D loss: 0.533188, acc.: 72.66%] [G loss: 1.181983]\n",
      "963 [D loss: 0.551330, acc.: 68.36%] [G loss: 1.214923]\n",
      "964 [D loss: 0.544355, acc.: 70.70%] [G loss: 1.218502]\n",
      "965 [D loss: 0.534923, acc.: 71.88%] [G loss: 1.181167]\n",
      "966 [D loss: 0.561236, acc.: 68.36%] [G loss: 1.128624]\n",
      "967 [D loss: 0.539880, acc.: 71.48%] [G loss: 1.118865]\n",
      "968 [D loss: 0.558890, acc.: 69.53%] [G loss: 1.064132]\n",
      "969 [D loss: 0.552710, acc.: 69.53%] [G loss: 1.094037]\n",
      "970 [D loss: 0.532952, acc.: 72.66%] [G loss: 1.118388]\n",
      "971 [D loss: 0.535539, acc.: 71.48%] [G loss: 1.119695]\n",
      "972 [D loss: 0.513503, acc.: 73.83%] [G loss: 1.092519]\n",
      "973 [D loss: 0.524041, acc.: 72.66%] [G loss: 1.139203]\n",
      "974 [D loss: 0.512927, acc.: 72.27%] [G loss: 1.121190]\n",
      "975 [D loss: 0.510401, acc.: 75.00%] [G loss: 1.108809]\n",
      "976 [D loss: 0.524544, acc.: 73.05%] [G loss: 1.124492]\n",
      "977 [D loss: 0.529222, acc.: 73.05%] [G loss: 1.085003]\n",
      "978 [D loss: 0.522887, acc.: 74.61%] [G loss: 1.108940]\n",
      "979 [D loss: 0.526731, acc.: 73.05%] [G loss: 1.071314]\n",
      "980 [D loss: 0.527328, acc.: 72.27%] [G loss: 1.064668]\n",
      "981 [D loss: 0.532713, acc.: 73.44%] [G loss: 1.094138]\n",
      "982 [D loss: 0.543214, acc.: 70.31%] [G loss: 1.082233]\n",
      "983 [D loss: 0.530481, acc.: 71.09%] [G loss: 1.107372]\n",
      "984 [D loss: 0.527039, acc.: 73.83%] [G loss: 1.108480]\n",
      "985 [D loss: 0.574177, acc.: 67.58%] [G loss: 1.153102]\n",
      "986 [D loss: 0.510757, acc.: 76.56%] [G loss: 1.190932]\n",
      "987 [D loss: 0.516512, acc.: 73.44%] [G loss: 1.233930]\n",
      "988 [D loss: 0.504445, acc.: 76.17%] [G loss: 1.239623]\n",
      "989 [D loss: 0.487074, acc.: 77.73%] [G loss: 1.225944]\n",
      "990 [D loss: 0.481809, acc.: 78.52%] [G loss: 1.221414]\n",
      "991 [D loss: 0.479344, acc.: 78.52%] [G loss: 1.226092]\n",
      "992 [D loss: 0.496746, acc.: 76.95%] [G loss: 1.248818]\n",
      "993 [D loss: 0.526252, acc.: 73.83%] [G loss: 1.189674]\n",
      "994 [D loss: 0.535773, acc.: 73.83%] [G loss: 1.173462]\n",
      "995 [D loss: 0.529474, acc.: 74.61%] [G loss: 1.166256]\n",
      "996 [D loss: 0.558123, acc.: 71.09%] [G loss: 1.170029]\n",
      "997 [D loss: 0.550501, acc.: 71.88%] [G loss: 1.209883]\n",
      "998 [D loss: 0.532427, acc.: 73.44%] [G loss: 1.194717]\n",
      "999 [D loss: 0.528491, acc.: 73.05%] [G loss: 1.170585]\n"
     ]
    }
   ],
   "source": [
    "#Training the GAN model chosen: Vanilla GAN, CGAN, DCGAN, etc.\n",
    "synthesizer = model(gan_args)\n",
    "synthesizer.train(train_sample, train_args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fabiana/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models/gan/saved/generator_fraud/assets\n"
     ]
    }
   ],
   "source": [
    "#You can easily save the trained generator and loaded it aftwerwards\n",
    "synthesizer.save('models/gan/saved', 'generator_fraud')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<models.gan.model.Generator at 0x7f9d98707048>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training results visualization\n",
    "#Adapt this code\n",
    "model_steps = [ 0, 200, 500, 1000, 5000]\n",
    "rows = len(model_steps)\n",
    "columns = 5\n",
    "\n",
    "axarr = [[]]*len(model_steps)\n",
    "\n",
    "fig = plt.figure(figsize=(14,rows*3))\n",
    "\n",
    "for model_step_ix, model_step in enumerate( model_steps ):\n",
    "    print(model_step)\n",
    "        \n",
    "    axarr[model_step_ix] = plt.subplot(rows, columns, model_step_ix*columns + 1)\n",
    "    \n",
    "    for group, color, marker, label in zip(real_samples.groupby('Class_1'), colors, markers, class_labels ):\n",
    "        plt.scatter( group[1][[col1]], group[1][[col2]], \n",
    "                         label=label, marker=marker, edgecolors=color, facecolors='none' )\n",
    "    \n",
    "    plt.title('Actual Fraud Data')\n",
    "    plt.ylabel(col2) # Only add y label to left plot\n",
    "    plt.xlabel(col1)\n",
    "    xlims, ylims = axarr[model_step_ix].get_xlim(), axarr[model_step_ix].get_ylim()\n",
    "        \n",
    "    if model_step_ix == 0: \n",
    "        legend = plt.legend()\n",
    "        legend.get_frame().set_facecolor('white')\n",
    "    \n",
    "    for i, model_name in enumerate( model_names[:] ):\n",
    "        \n",
    "        [ model_name, with_class, type0, generator_model ] = models[model_name]\n",
    "        \n",
    "        generator_model.load_weights( base_dir + model_name + '_generator_model_weights_step_'+str(model_step)+'.h5')\n",
    "\n",
    "        ax = plt.subplot(rows, columns, model_step_ix*columns + 1 + (i+1) )\n",
    "        \n",
    "        if with_class:\n",
    "            g_z = generator_model.predict([z, labels])\n",
    "            gen_samples = pd.DataFrame(g_z, columns=data_cols+label_cols)\n",
    "            for group, color, marker, label in zip( gen_samples.groupby('Class_1'), colors, markers, class_labels ):\n",
    "                plt.scatter( group[1][[col1]], group[1][[col2]], \n",
    "                                 label=label, marker=marker, edgecolors=color, facecolors='none' )\n",
    "        else:\n",
    "            g_z = generator_model.predict(z)\n",
    "            gen_samples = pd.DataFrame(g_z, columns=data_cols)\n",
    "            gen_samples.to_csv('Generated_sample.csv')\n",
    "            plt.scatter( gen_samples[[col1]], gen_samples[[col2]], \n",
    "                             label=class_labels[0], marker=markers[0], edgecolors=colors[0], facecolors='none' )\n",
    "        plt.title(model_name)   \n",
    "        plt.xlabel(data_cols[0])\n",
    "        ax.set_xlim(xlims), ax.set_ylim(ylims)\n",
    "\n",
    "\n",
    "plt.suptitle('Comparison of GAN outputs', size=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0.075,0,1,0.95])\n",
    "\n",
    "# Adding text labels for traning steps\n",
    "vpositions = np.array([ i._position.bounds[1] for i in axarr ])\n",
    "vpositions += ((vpositions[0] - vpositions[1]) * 0.35 )\n",
    "for model_step_ix, model_step in enumerate( model_steps ):\n",
    "    fig.text( 0.05, vpositions[model_step_ix], 'training\\nstep\\n'+str(model_step), ha='center', va='center', size=12)\n",
    "\n",
    "plt.savefig('Comparison_of_GAN_outputs.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Generator' object has no attribute 'compile'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-12-409750a71de6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mnew_synthesizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompile\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Generator' object has no attribute 'compile'"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python2",
   "language": "python",
   "display_name": "Python 2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}