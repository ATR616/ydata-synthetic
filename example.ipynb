{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# The credit fraud dataset - Synthesizing the minority class\n",
    "In this notebook it's presented a practical exercise of how to use the avilable library GANs to synthesize tabular data.\n",
    "For the purpose of this exercise it has been used the Credit Fraud dataset from Kaggle, that you can find here:https: //www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.cluster as cluster\n",
    "\n",
    "from models.gan import model\n",
    "importlib.reload(model)\n",
    "\n",
    "from models.gan.model import GAN\n",
    "model = GAN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n"
     ]
    }
   ],
   "source": [
    "#Read the data\n",
    "data = pd.read_csv('data/data_processed.csv', index_col=[0])\n",
    "data_cols = list(data.columns[ data.columns != 'Class' ])\n",
    "label_cols = ['Class']\n",
    "\n",
    "print('Dataset columns: {}'.format(data_cols))\n",
    "sorted_cols = ['V14', 'V4', 'V10', 'V17', 'Time', 'V12', 'V26', 'Amount', 'V21', 'V8', 'V11', 'V7', 'V28', 'V19', 'V3', 'V22', 'V6', 'V20', 'V27', 'V16', 'V13', 'V25', 'V24', 'V18', 'V2', 'V1', 'V5', 'V15', 'V9', 'V23', 'Class']\n",
    "data = data[ sorted_cols ].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info: Number of records - 492 Number of varibles - 31\n",
      "   count\n",
      "0    357\n",
      "1    135\n"
     ]
    }
   ],
   "source": [
    "#For the purpose of this example we will only synthesize the minority class\n",
    "train_data = data.loc[ data['Class']==1 ].copy()\n",
    "\n",
    "print(\"Dataset info: Number of records - {} Number of varibles - {}\".format(train_data.shape[0], train_data.shape[1]))\n",
    "\n",
    "algorithm = cluster.KMeans\n",
    "args, kwds = (), {'n_clusters':2, 'random_state':0}\n",
    "labels = algorithm(*args, **kwds).fit_predict(train_data[ data_cols ])\n",
    "\n",
    "print( pd.DataFrame( [ [np.sum(labels==i)] for i in np.unique(labels) ], columns=['count'], index=np.unique(labels) ) )\n",
    "\n",
    "fraud_w_classes = train_data.copy()\n",
    "fraud_w_classes['Class'] = labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#Define the GAN and training parameters\n",
    "noise_dim = 32\n",
    "dim = 128\n",
    "batch_size = 128\n",
    "\n",
    "log_step = 100\n",
    "epochs = 1000\n",
    "learning_rate = 5e-4\n",
    "models_dir = './cache'\n",
    "\n",
    "train_sample = fraud_w_classes.copy().reset_index(drop=True)\n",
    "label_cols = [ i for i in train_sample.columns if 'Class' in i ]\n",
    "data_cols = [ i for i in train_sample.columns if i not in label_cols ]\n",
    "train_sample[ data_cols ] = train_sample[ data_cols ] / 10 # scale to random noise size, one less thing to learn\n",
    "train_no_label = train_sample[ data_cols ]\n",
    "\n",
    "gan_args = [batch_size, learning_rate, noise_dim, train_sample.shape[1], dim]\n",
    "train_args = ['./cache', epochs, log_step, '']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.716243, acc.: 43.75%] [G loss: 0.647072]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_4:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "1 [D loss: 0.673969, acc.: 50.00%] [G loss: 0.661306]\n",
      "2 [D loss: 0.649647, acc.: 50.00%] [G loss: 0.673505]\n",
      "3 [D loss: 0.642885, acc.: 50.00%] [G loss: 0.695522]\n",
      "4 [D loss: 0.637001, acc.: 50.00%] [G loss: 0.735539]\n",
      "5 [D loss: 0.651138, acc.: 50.39%] [G loss: 0.751465]\n",
      "6 [D loss: 0.705646, acc.: 50.00%] [G loss: 0.739906]\n",
      "7 [D loss: 0.809797, acc.: 49.22%] [G loss: 0.675067]\n",
      "8 [D loss: 0.856178, acc.: 44.53%] [G loss: 0.705571]\n",
      "9 [D loss: 0.794763, acc.: 16.41%] [G loss: 0.889026]\n",
      "10 [D loss: 0.690325, acc.: 51.17%] [G loss: 1.180811]\n",
      "11 [D loss: 0.643485, acc.: 52.34%] [G loss: 1.330645]\n",
      "12 [D loss: 0.632706, acc.: 55.08%] [G loss: 1.262061]\n",
      "13 [D loss: 0.640438, acc.: 57.03%] [G loss: 1.170083]\n",
      "14 [D loss: 0.634663, acc.: 64.84%] [G loss: 1.099782]\n",
      "15 [D loss: 0.631170, acc.: 71.88%] [G loss: 1.023942]\n",
      "16 [D loss: 0.640533, acc.: 82.42%] [G loss: 0.922991]\n",
      "17 [D loss: 0.640538, acc.: 76.17%] [G loss: 0.853670]\n",
      "18 [D loss: 0.628183, acc.: 69.14%] [G loss: 0.826852]\n",
      "19 [D loss: 0.614684, acc.: 70.31%] [G loss: 0.828724]\n",
      "20 [D loss: 0.612293, acc.: 61.33%] [G loss: 0.831601]\n",
      "21 [D loss: 0.629521, acc.: 55.08%] [G loss: 0.775741]\n",
      "22 [D loss: 0.655383, acc.: 50.00%] [G loss: 0.719689]\n",
      "23 [D loss: 0.668685, acc.: 48.05%] [G loss: 0.695601]\n",
      "24 [D loss: 0.674987, acc.: 46.88%] [G loss: 0.710813]\n",
      "25 [D loss: 0.657698, acc.: 47.27%] [G loss: 0.746950]\n",
      "26 [D loss: 0.646605, acc.: 49.22%] [G loss: 0.773259]\n",
      "27 [D loss: 0.636484, acc.: 55.86%] [G loss: 0.827382]\n",
      "28 [D loss: 0.635836, acc.: 58.59%] [G loss: 0.839950]\n",
      "29 [D loss: 0.630796, acc.: 61.33%] [G loss: 0.866108]\n",
      "30 [D loss: 0.609579, acc.: 73.05%] [G loss: 0.901763]\n",
      "31 [D loss: 0.597347, acc.: 76.17%] [G loss: 0.917714]\n",
      "32 [D loss: 0.571635, acc.: 82.81%] [G loss: 0.915860]\n",
      "33 [D loss: 0.550989, acc.: 83.20%] [G loss: 0.947544]\n",
      "34 [D loss: 0.546012, acc.: 83.98%] [G loss: 0.943978]\n",
      "35 [D loss: 0.527072, acc.: 84.77%] [G loss: 0.949547]\n",
      "36 [D loss: 0.526056, acc.: 80.86%] [G loss: 0.962481]\n",
      "37 [D loss: 0.524548, acc.: 83.59%] [G loss: 0.926256]\n",
      "38 [D loss: 0.526456, acc.: 76.56%] [G loss: 0.904001]\n",
      "39 [D loss: 0.546196, acc.: 71.48%] [G loss: 0.869742]\n",
      "40 [D loss: 0.561358, acc.: 62.89%] [G loss: 0.821009]\n",
      "41 [D loss: 0.618728, acc.: 50.39%] [G loss: 0.784324]\n",
      "42 [D loss: 0.629239, acc.: 47.27%] [G loss: 0.826527]\n",
      "43 [D loss: 0.570396, acc.: 54.30%] [G loss: 1.012578]\n",
      "44 [D loss: 0.460279, acc.: 86.33%] [G loss: 1.290280]\n",
      "45 [D loss: 0.406930, acc.: 87.11%] [G loss: 1.393186]\n",
      "46 [D loss: 0.407201, acc.: 87.11%] [G loss: 1.332608]\n",
      "47 [D loss: 0.420286, acc.: 87.89%] [G loss: 1.203193]\n",
      "48 [D loss: 0.453280, acc.: 87.50%] [G loss: 1.058208]\n",
      "49 [D loss: 0.501235, acc.: 79.69%] [G loss: 0.921966]\n",
      "50 [D loss: 0.583702, acc.: 57.03%] [G loss: 0.795651]\n",
      "51 [D loss: 0.692097, acc.: 42.19%] [G loss: 0.669348]\n",
      "52 [D loss: 0.805719, acc.: 36.72%] [G loss: 0.605910]\n",
      "53 [D loss: 0.846335, acc.: 33.98%] [G loss: 0.727960]\n",
      "54 [D loss: 0.838343, acc.: 26.95%] [G loss: 0.974380]\n",
      "55 [D loss: 0.824878, acc.: 26.17%] [G loss: 1.091556]\n",
      "56 [D loss: 0.826782, acc.: 32.42%] [G loss: 1.053212]\n",
      "57 [D loss: 0.856997, acc.: 17.58%] [G loss: 0.959614]\n",
      "58 [D loss: 0.891596, acc.: 13.67%] [G loss: 0.977451]\n",
      "59 [D loss: 0.819440, acc.: 33.59%] [G loss: 1.109391]\n",
      "60 [D loss: 0.774499, acc.: 52.34%] [G loss: 1.192121]\n",
      "61 [D loss: 0.742454, acc.: 53.12%] [G loss: 1.193957]\n",
      "62 [D loss: 0.696856, acc.: 53.91%] [G loss: 1.115499]\n",
      "63 [D loss: 0.675652, acc.: 58.98%] [G loss: 1.098571]\n",
      "64 [D loss: 0.643504, acc.: 60.55%] [G loss: 1.017411]\n",
      "65 [D loss: 0.635752, acc.: 63.67%] [G loss: 0.998048]\n",
      "66 [D loss: 0.622673, acc.: 67.19%] [G loss: 0.961205]\n",
      "67 [D loss: 0.617961, acc.: 69.92%] [G loss: 0.930197]\n",
      "68 [D loss: 0.619389, acc.: 70.31%] [G loss: 0.926161]\n",
      "69 [D loss: 0.618255, acc.: 71.48%] [G loss: 0.904790]\n",
      "70 [D loss: 0.630757, acc.: 60.16%] [G loss: 0.881604]\n",
      "71 [D loss: 0.619305, acc.: 65.23%] [G loss: 0.898498]\n",
      "72 [D loss: 0.613943, acc.: 70.70%] [G loss: 0.921577]\n",
      "73 [D loss: 0.616394, acc.: 68.36%] [G loss: 0.924144]\n",
      "74 [D loss: 0.631268, acc.: 62.89%] [G loss: 0.925733]\n",
      "75 [D loss: 0.638290, acc.: 62.11%] [G loss: 0.921265]\n",
      "76 [D loss: 0.669256, acc.: 52.34%] [G loss: 0.947129]\n",
      "77 [D loss: 0.648308, acc.: 64.06%] [G loss: 1.006523]\n",
      "78 [D loss: 0.608497, acc.: 76.17%] [G loss: 1.039021]\n",
      "79 [D loss: 0.571388, acc.: 83.20%] [G loss: 1.132775]\n",
      "80 [D loss: 0.529346, acc.: 87.50%] [G loss: 1.136100]\n",
      "81 [D loss: 0.531594, acc.: 86.72%] [G loss: 1.049868]\n",
      "82 [D loss: 0.546602, acc.: 78.52%] [G loss: 0.971184]\n",
      "83 [D loss: 0.583623, acc.: 64.45%] [G loss: 0.914298]\n",
      "84 [D loss: 0.607365, acc.: 57.42%] [G loss: 0.873677]\n",
      "85 [D loss: 0.613429, acc.: 51.95%] [G loss: 0.887199]\n",
      "86 [D loss: 0.616453, acc.: 52.73%] [G loss: 0.944319]\n",
      "87 [D loss: 0.617024, acc.: 53.91%] [G loss: 1.004685]\n",
      "88 [D loss: 0.589553, acc.: 69.92%] [G loss: 1.080069]\n",
      "89 [D loss: 0.579742, acc.: 75.00%] [G loss: 1.170379]\n",
      "90 [D loss: 0.561823, acc.: 77.34%] [G loss: 1.224334]\n",
      "91 [D loss: 0.540862, acc.: 79.69%] [G loss: 1.269331]\n",
      "92 [D loss: 0.539520, acc.: 75.78%] [G loss: 1.275696]\n",
      "93 [D loss: 0.521790, acc.: 76.56%] [G loss: 1.277598]\n",
      "94 [D loss: 0.540729, acc.: 75.78%] [G loss: 1.213545]\n",
      "95 [D loss: 0.543621, acc.: 72.66%] [G loss: 1.173605]\n",
      "96 [D loss: 0.552366, acc.: 72.27%] [G loss: 1.176402]\n",
      "97 [D loss: 0.571905, acc.: 70.31%] [G loss: 1.205025]\n",
      "98 [D loss: 0.516432, acc.: 80.08%] [G loss: 1.233088]\n",
      "99 [D loss: 0.497480, acc.: 82.42%] [G loss: 1.236419]\n",
      "100 [D loss: 0.465640, acc.: 84.77%] [G loss: 1.227642]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_4:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "101 [D loss: 0.443241, acc.: 85.16%] [G loss: 1.245307]\n",
      "102 [D loss: 0.431787, acc.: 85.55%] [G loss: 1.223141]\n",
      "103 [D loss: 0.425282, acc.: 85.94%] [G loss: 1.167276]\n",
      "104 [D loss: 0.426110, acc.: 87.89%] [G loss: 1.126047]\n",
      "105 [D loss: 0.440177, acc.: 87.89%] [G loss: 1.087348]\n",
      "106 [D loss: 0.467855, acc.: 83.20%] [G loss: 1.003486]\n",
      "107 [D loss: 0.596781, acc.: 58.59%] [G loss: 0.860320]\n",
      "108 [D loss: 0.889044, acc.: 38.67%] [G loss: 0.691418]\n",
      "109 [D loss: 1.090753, acc.: 37.50%] [G loss: 0.688790]\n",
      "110 [D loss: 1.029932, acc.: 27.73%] [G loss: 1.091835]\n",
      "111 [D loss: 0.921743, acc.: 27.73%] [G loss: 1.686582]\n",
      "112 [D loss: 1.004169, acc.: 44.14%] [G loss: 1.654524]\n",
      "113 [D loss: 0.984474, acc.: 49.61%] [G loss: 1.444637]\n",
      "114 [D loss: 0.917816, acc.: 50.78%] [G loss: 1.399181]\n",
      "115 [D loss: 0.826241, acc.: 51.17%] [G loss: 1.389362]\n",
      "116 [D loss: 0.761207, acc.: 51.56%] [G loss: 1.338914]\n",
      "117 [D loss: 0.706719, acc.: 51.17%] [G loss: 1.261988]\n",
      "118 [D loss: 0.673923, acc.: 52.73%] [G loss: 1.233764]\n",
      "119 [D loss: 0.627337, acc.: 55.86%] [G loss: 1.204637]\n",
      "120 [D loss: 0.602476, acc.: 60.94%] [G loss: 1.181415]\n",
      "121 [D loss: 0.584991, acc.: 66.02%] [G loss: 1.143870]\n",
      "122 [D loss: 0.577126, acc.: 70.31%] [G loss: 1.094385]\n",
      "123 [D loss: 0.579605, acc.: 76.17%] [G loss: 1.059137]\n",
      "124 [D loss: 0.587497, acc.: 75.00%] [G loss: 1.009313]\n",
      "125 [D loss: 0.613888, acc.: 73.05%] [G loss: 0.974219]\n",
      "126 [D loss: 0.622257, acc.: 69.53%] [G loss: 0.919554]\n",
      "127 [D loss: 0.645255, acc.: 62.50%] [G loss: 0.896675]\n",
      "128 [D loss: 0.651481, acc.: 60.94%] [G loss: 0.890317]\n",
      "129 [D loss: 0.649035, acc.: 62.50%] [G loss: 0.906938]\n",
      "130 [D loss: 0.633117, acc.: 69.53%] [G loss: 0.939217]\n",
      "131 [D loss: 0.617636, acc.: 70.70%] [G loss: 0.973461]\n",
      "132 [D loss: 0.599040, acc.: 75.00%] [G loss: 1.004617]\n",
      "133 [D loss: 0.579735, acc.: 78.52%] [G loss: 1.031181]\n",
      "134 [D loss: 0.564204, acc.: 79.30%] [G loss: 1.061932]\n",
      "135 [D loss: 0.539989, acc.: 80.47%] [G loss: 1.092899]\n",
      "136 [D loss: 0.526008, acc.: 81.64%] [G loss: 1.104709]\n",
      "137 [D loss: 0.497855, acc.: 82.42%] [G loss: 1.147560]\n",
      "138 [D loss: 0.470234, acc.: 84.38%] [G loss: 1.185812]\n",
      "139 [D loss: 0.437518, acc.: 86.72%] [G loss: 1.194663]\n",
      "140 [D loss: 0.414787, acc.: 90.23%] [G loss: 1.175524]\n",
      "141 [D loss: 0.386705, acc.: 92.19%] [G loss: 1.178130]\n",
      "142 [D loss: 0.394079, acc.: 94.92%] [G loss: 1.110868]\n",
      "143 [D loss: 0.406051, acc.: 92.19%] [G loss: 1.063952]\n",
      "144 [D loss: 0.415543, acc.: 91.80%] [G loss: 1.057154]\n",
      "145 [D loss: 0.436124, acc.: 89.45%] [G loss: 1.097534]\n",
      "146 [D loss: 0.449040, acc.: 88.28%] [G loss: 1.080357]\n",
      "147 [D loss: 0.463936, acc.: 82.03%] [G loss: 1.033031]\n",
      "148 [D loss: 0.561127, acc.: 66.41%] [G loss: 0.963704]\n",
      "149 [D loss: 0.671904, acc.: 50.78%] [G loss: 1.001886]\n",
      "150 [D loss: 0.635818, acc.: 51.56%] [G loss: 1.104879]\n",
      "151 [D loss: 0.583579, acc.: 62.89%] [G loss: 1.208292]\n",
      "152 [D loss: 0.547655, acc.: 78.12%] [G loss: 1.277812]\n",
      "153 [D loss: 0.542849, acc.: 75.39%] [G loss: 1.344851]\n",
      "154 [D loss: 0.550056, acc.: 71.88%] [G loss: 1.284307]\n",
      "155 [D loss: 0.610558, acc.: 66.80%] [G loss: 1.087987]\n",
      "156 [D loss: 0.682080, acc.: 60.16%] [G loss: 1.116310]\n",
      "157 [D loss: 0.744389, acc.: 57.42%] [G loss: 1.300989]\n",
      "158 [D loss: 0.655874, acc.: 63.28%] [G loss: 1.497533]\n",
      "159 [D loss: 0.654997, acc.: 64.84%] [G loss: 1.345440]\n",
      "160 [D loss: 0.753149, acc.: 51.95%] [G loss: 1.224576]\n",
      "161 [D loss: 0.747450, acc.: 51.17%] [G loss: 1.182307]\n",
      "162 [D loss: 0.787285, acc.: 47.66%] [G loss: 1.098981]\n",
      "163 [D loss: 0.816186, acc.: 46.09%] [G loss: 0.911347]\n",
      "164 [D loss: 0.889770, acc.: 38.67%] [G loss: 0.834111]\n",
      "165 [D loss: 0.888994, acc.: 42.97%] [G loss: 0.913417]\n",
      "166 [D loss: 0.817561, acc.: 42.19%] [G loss: 1.035193]\n",
      "167 [D loss: 0.775843, acc.: 48.83%] [G loss: 1.074732]\n",
      "168 [D loss: 0.770771, acc.: 52.34%] [G loss: 1.133870]\n",
      "169 [D loss: 0.712259, acc.: 57.42%] [G loss: 1.139348]\n",
      "170 [D loss: 0.722966, acc.: 57.42%] [G loss: 1.098561]\n",
      "171 [D loss: 0.712093, acc.: 58.20%] [G loss: 1.047379]\n",
      "172 [D loss: 0.754355, acc.: 50.78%] [G loss: 0.993713]\n",
      "173 [D loss: 0.764189, acc.: 48.44%] [G loss: 0.990910]\n",
      "174 [D loss: 0.793726, acc.: 39.06%] [G loss: 1.024460]\n",
      "175 [D loss: 0.771506, acc.: 46.48%] [G loss: 1.038190]\n",
      "176 [D loss: 0.772421, acc.: 45.70%] [G loss: 1.056829]\n",
      "177 [D loss: 0.741237, acc.: 57.03%] [G loss: 1.087462]\n",
      "178 [D loss: 0.724813, acc.: 56.25%] [G loss: 1.108871]\n",
      "179 [D loss: 0.724082, acc.: 55.08%] [G loss: 1.070118]\n",
      "180 [D loss: 0.739178, acc.: 52.34%] [G loss: 0.981523]\n",
      "181 [D loss: 0.749471, acc.: 51.17%] [G loss: 0.941335]\n",
      "182 [D loss: 0.749389, acc.: 51.56%] [G loss: 0.899037]\n",
      "183 [D loss: 0.751051, acc.: 51.56%] [G loss: 0.874985]\n",
      "184 [D loss: 0.739703, acc.: 50.00%] [G loss: 0.874386]\n",
      "185 [D loss: 0.729728, acc.: 51.95%] [G loss: 0.880570]\n",
      "186 [D loss: 0.718104, acc.: 54.30%] [G loss: 0.849016]\n",
      "187 [D loss: 0.708952, acc.: 54.30%] [G loss: 0.836535]\n",
      "188 [D loss: 0.709185, acc.: 53.52%] [G loss: 0.810788]\n",
      "189 [D loss: 0.707525, acc.: 51.56%] [G loss: 0.813092]\n",
      "190 [D loss: 0.701808, acc.: 52.34%] [G loss: 0.800377]\n",
      "191 [D loss: 0.697175, acc.: 53.91%] [G loss: 0.795445]\n",
      "192 [D loss: 0.713614, acc.: 47.27%] [G loss: 0.792804]\n",
      "193 [D loss: 0.694408, acc.: 50.78%] [G loss: 0.787998]\n",
      "194 [D loss: 0.690719, acc.: 53.12%] [G loss: 0.776728]\n",
      "195 [D loss: 0.700792, acc.: 49.22%] [G loss: 0.760968]\n",
      "196 [D loss: 0.690048, acc.: 51.17%] [G loss: 0.762168]\n",
      "197 [D loss: 0.692602, acc.: 53.91%] [G loss: 0.758161]\n",
      "198 [D loss: 0.691708, acc.: 48.44%] [G loss: 0.753107]\n",
      "199 [D loss: 0.680021, acc.: 58.20%] [G loss: 0.756824]\n",
      "200 [D loss: 0.683120, acc.: 58.20%] [G loss: 0.754621]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_4:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "201 [D loss: 0.678312, acc.: 61.33%] [G loss: 0.757022]\n",
      "202 [D loss: 0.674473, acc.: 64.06%] [G loss: 0.762690]\n",
      "203 [D loss: 0.668502, acc.: 67.97%] [G loss: 0.761903]\n",
      "204 [D loss: 0.669419, acc.: 66.41%] [G loss: 0.762819]\n",
      "205 [D loss: 0.659534, acc.: 69.14%] [G loss: 0.768817]\n",
      "206 [D loss: 0.660131, acc.: 66.80%] [G loss: 0.769172]\n",
      "207 [D loss: 0.651722, acc.: 69.92%] [G loss: 0.782193]\n",
      "208 [D loss: 0.649160, acc.: 67.19%] [G loss: 0.778566]\n",
      "209 [D loss: 0.652499, acc.: 66.80%] [G loss: 0.770055]\n",
      "210 [D loss: 0.645708, acc.: 69.14%] [G loss: 0.766284]\n",
      "211 [D loss: 0.645489, acc.: 64.45%] [G loss: 0.753477]\n",
      "212 [D loss: 0.649571, acc.: 62.89%] [G loss: 0.742834]\n",
      "213 [D loss: 0.650791, acc.: 58.59%] [G loss: 0.730228]\n",
      "214 [D loss: 0.656199, acc.: 58.98%] [G loss: 0.723786]\n",
      "215 [D loss: 0.647439, acc.: 60.55%] [G loss: 0.697222]\n",
      "216 [D loss: 0.656921, acc.: 50.78%] [G loss: 0.687080]\n",
      "217 [D loss: 0.668942, acc.: 47.27%] [G loss: 0.681135]\n",
      "218 [D loss: 0.665018, acc.: 48.44%] [G loss: 0.684880]\n",
      "219 [D loss: 0.672796, acc.: 49.22%] [G loss: 0.700123]\n",
      "220 [D loss: 0.666016, acc.: 48.05%] [G loss: 0.708666]\n",
      "221 [D loss: 0.676187, acc.: 48.05%] [G loss: 0.721680]\n",
      "222 [D loss: 0.653944, acc.: 62.11%] [G loss: 0.759357]\n",
      "223 [D loss: 0.635803, acc.: 73.05%] [G loss: 0.771540]\n",
      "224 [D loss: 0.628662, acc.: 74.22%] [G loss: 0.806247]\n",
      "225 [D loss: 0.624997, acc.: 71.48%] [G loss: 0.832711]\n",
      "226 [D loss: 0.612504, acc.: 73.83%] [G loss: 0.842158]\n",
      "227 [D loss: 0.615096, acc.: 75.39%] [G loss: 0.823072]\n",
      "228 [D loss: 0.656303, acc.: 67.97%] [G loss: 0.822074]\n",
      "229 [D loss: 0.643154, acc.: 69.92%] [G loss: 0.810850]\n",
      "230 [D loss: 0.654211, acc.: 65.62%] [G loss: 0.808174]\n",
      "231 [D loss: 0.664640, acc.: 58.98%] [G loss: 0.813954]\n",
      "232 [D loss: 0.657897, acc.: 67.58%] [G loss: 0.802857]\n",
      "233 [D loss: 0.660585, acc.: 66.41%] [G loss: 0.804601]\n",
      "234 [D loss: 0.664540, acc.: 65.23%] [G loss: 0.813895]\n",
      "235 [D loss: 0.660731, acc.: 65.23%] [G loss: 0.814418]\n",
      "236 [D loss: 0.648246, acc.: 71.48%] [G loss: 0.827200]\n",
      "237 [D loss: 0.638175, acc.: 72.66%] [G loss: 0.819314]\n",
      "238 [D loss: 0.631292, acc.: 75.39%] [G loss: 0.840829]\n",
      "239 [D loss: 0.628698, acc.: 77.34%] [G loss: 0.824027]\n",
      "240 [D loss: 0.616084, acc.: 81.64%] [G loss: 0.825067]\n",
      "241 [D loss: 0.616078, acc.: 78.91%] [G loss: 0.828904]\n",
      "242 [D loss: 0.602073, acc.: 82.81%] [G loss: 0.818350]\n",
      "243 [D loss: 0.604291, acc.: 78.52%] [G loss: 0.830379]\n",
      "244 [D loss: 0.593649, acc.: 80.86%] [G loss: 0.830519]\n",
      "245 [D loss: 0.593344, acc.: 75.39%] [G loss: 0.842385]\n",
      "246 [D loss: 0.592722, acc.: 78.91%] [G loss: 0.851133]\n",
      "247 [D loss: 0.620039, acc.: 64.06%] [G loss: 0.834467]\n",
      "248 [D loss: 0.626358, acc.: 59.38%] [G loss: 0.841359]\n",
      "249 [D loss: 0.646706, acc.: 55.47%] [G loss: 0.850397]\n",
      "250 [D loss: 0.627303, acc.: 60.16%] [G loss: 0.952727]\n",
      "251 [D loss: 0.550980, acc.: 74.22%] [G loss: 1.181546]\n",
      "252 [D loss: 0.542542, acc.: 73.05%] [G loss: 1.254940]\n",
      "253 [D loss: 0.554474, acc.: 72.27%] [G loss: 1.125978]\n",
      "254 [D loss: 0.615636, acc.: 64.84%] [G loss: 0.960387]\n",
      "255 [D loss: 0.774542, acc.: 49.22%] [G loss: 0.926166]\n",
      "256 [D loss: 0.755114, acc.: 48.05%] [G loss: 0.984048]\n",
      "257 [D loss: 0.744365, acc.: 52.73%] [G loss: 0.956048]\n",
      "258 [D loss: 0.789365, acc.: 43.75%] [G loss: 0.931888]\n",
      "259 [D loss: 0.776654, acc.: 44.92%] [G loss: 0.938843]\n",
      "260 [D loss: 0.732749, acc.: 56.64%] [G loss: 0.919412]\n",
      "261 [D loss: 0.719727, acc.: 54.30%] [G loss: 0.939272]\n",
      "262 [D loss: 0.676137, acc.: 56.25%] [G loss: 0.968021]\n",
      "263 [D loss: 0.667807, acc.: 53.52%] [G loss: 0.955339]\n",
      "264 [D loss: 0.672295, acc.: 57.42%] [G loss: 1.004581]\n",
      "265 [D loss: 0.657964, acc.: 56.25%] [G loss: 1.025691]\n",
      "266 [D loss: 0.647067, acc.: 56.25%] [G loss: 1.000202]\n",
      "267 [D loss: 0.650783, acc.: 57.81%] [G loss: 0.966422]\n",
      "268 [D loss: 0.655945, acc.: 56.64%] [G loss: 0.923795]\n",
      "269 [D loss: 0.668331, acc.: 60.16%] [G loss: 0.961702]\n",
      "270 [D loss: 0.629046, acc.: 63.28%] [G loss: 0.946457]\n",
      "271 [D loss: 0.643747, acc.: 65.23%] [G loss: 0.904807]\n",
      "272 [D loss: 0.642869, acc.: 69.53%] [G loss: 0.873455]\n",
      "273 [D loss: 0.640492, acc.: 69.53%] [G loss: 0.841995]\n",
      "274 [D loss: 0.658877, acc.: 66.80%] [G loss: 0.839578]\n",
      "275 [D loss: 0.654462, acc.: 64.84%] [G loss: 0.825037]\n",
      "276 [D loss: 0.665936, acc.: 60.94%] [G loss: 0.795731]\n",
      "277 [D loss: 0.662003, acc.: 61.33%] [G loss: 0.762589]\n",
      "278 [D loss: 0.670888, acc.: 54.69%] [G loss: 0.736665]\n",
      "279 [D loss: 0.666692, acc.: 54.69%] [G loss: 0.741314]\n",
      "280 [D loss: 0.657753, acc.: 55.86%] [G loss: 0.782812]\n",
      "281 [D loss: 0.636619, acc.: 67.97%] [G loss: 0.807715]\n",
      "282 [D loss: 0.623261, acc.: 77.34%] [G loss: 0.825619]\n",
      "283 [D loss: 0.606188, acc.: 81.64%] [G loss: 0.852640]\n",
      "284 [D loss: 0.589726, acc.: 81.25%] [G loss: 0.870382]\n",
      "285 [D loss: 0.590040, acc.: 78.91%] [G loss: 0.905579]\n",
      "286 [D loss: 0.576174, acc.: 82.03%] [G loss: 0.909070]\n",
      "287 [D loss: 0.576509, acc.: 82.03%] [G loss: 0.883963]\n",
      "288 [D loss: 0.572842, acc.: 82.42%] [G loss: 0.884833]\n",
      "289 [D loss: 0.581474, acc.: 81.25%] [G loss: 0.877046]\n",
      "290 [D loss: 0.599707, acc.: 76.56%] [G loss: 0.862576]\n",
      "291 [D loss: 0.605597, acc.: 74.61%] [G loss: 0.859847]\n",
      "292 [D loss: 0.610254, acc.: 73.83%] [G loss: 0.861800]\n",
      "293 [D loss: 0.623830, acc.: 70.31%] [G loss: 0.825593]\n",
      "294 [D loss: 0.639635, acc.: 67.19%] [G loss: 0.818687]\n",
      "295 [D loss: 0.640494, acc.: 64.06%] [G loss: 0.809314]\n",
      "296 [D loss: 0.638314, acc.: 62.50%] [G loss: 0.817226]\n",
      "297 [D loss: 0.633746, acc.: 64.84%] [G loss: 0.816668]\n",
      "298 [D loss: 0.629761, acc.: 66.80%] [G loss: 0.856494]\n",
      "299 [D loss: 0.623658, acc.: 67.19%] [G loss: 0.948014]\n",
      "300 [D loss: 0.588057, acc.: 72.66%] [G loss: 1.023354]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_4:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "301 [D loss: 0.582295, acc.: 72.27%] [G loss: 0.985767]\n",
      "302 [D loss: 0.595180, acc.: 72.27%] [G loss: 0.975616]\n",
      "303 [D loss: 0.600523, acc.: 69.92%] [G loss: 0.961861]\n",
      "304 [D loss: 0.621798, acc.: 64.84%] [G loss: 0.939267]\n",
      "305 [D loss: 0.628274, acc.: 62.89%] [G loss: 0.949887]\n",
      "306 [D loss: 0.618801, acc.: 67.58%] [G loss: 1.038834]\n",
      "307 [D loss: 0.592950, acc.: 71.48%] [G loss: 1.153826]\n",
      "308 [D loss: 0.584700, acc.: 72.27%] [G loss: 1.159937]\n",
      "309 [D loss: 0.592935, acc.: 72.66%] [G loss: 1.132502]\n",
      "310 [D loss: 0.622541, acc.: 71.48%] [G loss: 1.057623]\n",
      "311 [D loss: 0.651764, acc.: 65.23%] [G loss: 0.981162]\n",
      "312 [D loss: 0.676217, acc.: 62.89%] [G loss: 0.915040]\n",
      "313 [D loss: 0.686958, acc.: 57.81%] [G loss: 0.858457]\n",
      "314 [D loss: 0.708170, acc.: 54.69%] [G loss: 0.805642]\n",
      "315 [D loss: 0.692338, acc.: 55.08%] [G loss: 0.820984]\n",
      "316 [D loss: 0.672653, acc.: 53.12%] [G loss: 0.818719]\n",
      "317 [D loss: 0.701923, acc.: 45.70%] [G loss: 0.824452]\n",
      "318 [D loss: 0.710335, acc.: 44.92%] [G loss: 0.818167]\n",
      "319 [D loss: 0.721610, acc.: 42.97%] [G loss: 0.787301]\n",
      "320 [D loss: 0.714897, acc.: 40.62%] [G loss: 0.809428]\n",
      "321 [D loss: 0.678697, acc.: 49.22%] [G loss: 0.874351]\n",
      "322 [D loss: 0.621896, acc.: 63.67%] [G loss: 1.065246]\n",
      "323 [D loss: 0.531162, acc.: 72.66%] [G loss: 1.306894]\n",
      "324 [D loss: 0.518556, acc.: 71.88%] [G loss: 1.393721]\n",
      "325 [D loss: 0.498307, acc.: 79.69%] [G loss: 1.307621]\n",
      "326 [D loss: 0.549797, acc.: 73.83%] [G loss: 1.173793]\n",
      "327 [D loss: 0.586771, acc.: 72.66%] [G loss: 1.119011]\n",
      "328 [D loss: 0.643296, acc.: 64.84%] [G loss: 1.103216]\n",
      "329 [D loss: 0.723774, acc.: 53.52%] [G loss: 0.972144]\n",
      "330 [D loss: 0.860269, acc.: 28.91%] [G loss: 0.759456]\n",
      "331 [D loss: 0.958997, acc.: 18.75%] [G loss: 0.710926]\n",
      "332 [D loss: 0.836566, acc.: 16.80%] [G loss: 0.879520]\n",
      "333 [D loss: 0.700751, acc.: 57.81%] [G loss: 1.045883]\n",
      "334 [D loss: 0.653030, acc.: 57.81%] [G loss: 1.164009]\n",
      "335 [D loss: 0.633064, acc.: 59.77%] [G loss: 1.098141]\n",
      "336 [D loss: 0.632816, acc.: 62.11%] [G loss: 1.085461]\n",
      "337 [D loss: 0.640146, acc.: 61.33%] [G loss: 1.012373]\n",
      "338 [D loss: 0.628252, acc.: 63.67%] [G loss: 1.013703]\n",
      "339 [D loss: 0.615244, acc.: 64.06%] [G loss: 1.024463]\n",
      "340 [D loss: 0.606572, acc.: 63.28%] [G loss: 1.075820]\n",
      "341 [D loss: 0.584332, acc.: 66.02%] [G loss: 1.148732]\n",
      "342 [D loss: 0.569570, acc.: 66.80%] [G loss: 1.142372]\n",
      "343 [D loss: 0.572786, acc.: 70.31%] [G loss: 1.077250]\n",
      "344 [D loss: 0.576577, acc.: 71.88%] [G loss: 1.027077]\n",
      "345 [D loss: 0.580337, acc.: 73.44%] [G loss: 0.970664]\n",
      "346 [D loss: 0.601546, acc.: 72.66%] [G loss: 0.954237]\n",
      "347 [D loss: 0.598480, acc.: 70.70%] [G loss: 0.933659]\n",
      "348 [D loss: 0.596082, acc.: 71.88%] [G loss: 0.934767]\n",
      "349 [D loss: 0.585698, acc.: 73.83%] [G loss: 0.869799]\n",
      "350 [D loss: 0.581682, acc.: 77.34%] [G loss: 0.872306]\n",
      "351 [D loss: 0.607812, acc.: 68.75%] [G loss: 0.860470]\n",
      "352 [D loss: 0.607877, acc.: 69.92%] [G loss: 0.859922]\n",
      "353 [D loss: 0.636894, acc.: 64.45%] [G loss: 0.800541]\n",
      "354 [D loss: 0.663373, acc.: 58.20%] [G loss: 0.761972]\n",
      "355 [D loss: 0.679169, acc.: 57.03%] [G loss: 0.753341]\n",
      "356 [D loss: 0.698742, acc.: 50.39%] [G loss: 0.757468]\n",
      "357 [D loss: 0.687271, acc.: 48.44%] [G loss: 0.762913]\n",
      "358 [D loss: 0.706956, acc.: 41.41%] [G loss: 0.745991]\n",
      "359 [D loss: 0.724579, acc.: 43.36%] [G loss: 0.761449]\n",
      "360 [D loss: 0.727232, acc.: 35.94%] [G loss: 0.733106]\n",
      "361 [D loss: 0.720861, acc.: 33.98%] [G loss: 0.715334]\n",
      "362 [D loss: 0.716380, acc.: 31.25%] [G loss: 0.781866]\n",
      "363 [D loss: 0.658768, acc.: 60.16%] [G loss: 0.912569]\n",
      "364 [D loss: 0.613057, acc.: 63.28%] [G loss: 1.001353]\n",
      "365 [D loss: 0.593472, acc.: 64.45%] [G loss: 1.026775]\n",
      "366 [D loss: 0.599972, acc.: 64.45%] [G loss: 0.972531]\n",
      "367 [D loss: 0.625903, acc.: 63.67%] [G loss: 0.905343]\n",
      "368 [D loss: 0.659587, acc.: 58.59%] [G loss: 0.869096]\n",
      "369 [D loss: 0.677059, acc.: 60.55%] [G loss: 0.877240]\n",
      "370 [D loss: 0.660701, acc.: 62.11%] [G loss: 0.858121]\n",
      "371 [D loss: 0.669000, acc.: 62.11%] [G loss: 0.843756]\n",
      "372 [D loss: 0.677931, acc.: 58.59%] [G loss: 0.805432]\n",
      "373 [D loss: 0.693673, acc.: 56.64%] [G loss: 0.824510]\n",
      "374 [D loss: 0.677125, acc.: 57.81%] [G loss: 0.838172]\n",
      "375 [D loss: 0.666621, acc.: 57.42%] [G loss: 0.859095]\n",
      "376 [D loss: 0.678539, acc.: 55.08%] [G loss: 0.870455]\n",
      "377 [D loss: 0.669256, acc.: 55.86%] [G loss: 0.844783]\n",
      "378 [D loss: 0.666613, acc.: 54.30%] [G loss: 0.853844]\n",
      "379 [D loss: 0.669173, acc.: 55.86%] [G loss: 0.835660]\n",
      "380 [D loss: 0.677486, acc.: 53.91%] [G loss: 0.842331]\n",
      "381 [D loss: 0.678386, acc.: 55.47%] [G loss: 0.836195]\n",
      "382 [D loss: 0.670873, acc.: 60.55%] [G loss: 0.849763]\n",
      "383 [D loss: 0.665874, acc.: 60.16%] [G loss: 0.842629]\n",
      "384 [D loss: 0.671852, acc.: 61.33%] [G loss: 0.835515]\n",
      "385 [D loss: 0.653822, acc.: 64.06%] [G loss: 0.851430]\n",
      "386 [D loss: 0.645664, acc.: 65.23%] [G loss: 0.842618]\n",
      "387 [D loss: 0.644503, acc.: 64.84%] [G loss: 0.850592]\n",
      "388 [D loss: 0.643579, acc.: 64.45%] [G loss: 0.840170]\n",
      "389 [D loss: 0.627931, acc.: 67.19%] [G loss: 0.832460]\n",
      "390 [D loss: 0.646039, acc.: 61.72%] [G loss: 0.833828]\n",
      "391 [D loss: 0.628586, acc.: 63.67%] [G loss: 0.829717]\n",
      "392 [D loss: 0.632696, acc.: 64.06%] [G loss: 0.825062]\n",
      "393 [D loss: 0.631709, acc.: 62.11%] [G loss: 0.822717]\n",
      "394 [D loss: 0.629880, acc.: 63.28%] [G loss: 0.827935]\n",
      "395 [D loss: 0.649940, acc.: 59.77%] [G loss: 0.815879]\n",
      "396 [D loss: 0.634451, acc.: 64.84%] [G loss: 0.800792]\n",
      "397 [D loss: 0.633815, acc.: 62.11%] [G loss: 0.805829]\n",
      "398 [D loss: 0.624542, acc.: 66.02%] [G loss: 0.785516]\n",
      "399 [D loss: 0.623114, acc.: 68.36%] [G loss: 0.790464]\n",
      "400 [D loss: 0.631081, acc.: 66.41%] [G loss: 0.786981]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_4:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "401 [D loss: 0.631628, acc.: 67.58%] [G loss: 0.807966]\n",
      "402 [D loss: 0.633908, acc.: 68.75%] [G loss: 0.793976]\n",
      "403 [D loss: 0.631497, acc.: 68.75%] [G loss: 0.774722]\n",
      "404 [D loss: 0.633542, acc.: 67.58%] [G loss: 0.793163]\n",
      "405 [D loss: 0.628292, acc.: 71.09%] [G loss: 0.779046]\n",
      "406 [D loss: 0.621525, acc.: 69.14%] [G loss: 0.790136]\n",
      "407 [D loss: 0.611512, acc.: 69.53%] [G loss: 0.789775]\n",
      "408 [D loss: 0.608439, acc.: 71.48%] [G loss: 0.803084]\n",
      "409 [D loss: 0.606573, acc.: 70.70%] [G loss: 0.816855]\n",
      "410 [D loss: 0.596250, acc.: 76.56%] [G loss: 0.811766]\n",
      "411 [D loss: 0.591958, acc.: 73.44%] [G loss: 0.817863]\n",
      "412 [D loss: 0.587813, acc.: 75.78%] [G loss: 0.814434]\n",
      "413 [D loss: 0.594720, acc.: 72.66%] [G loss: 0.811873]\n",
      "414 [D loss: 0.600356, acc.: 71.09%] [G loss: 0.805599]\n",
      "415 [D loss: 0.608810, acc.: 66.80%] [G loss: 0.801615]\n",
      "416 [D loss: 0.610814, acc.: 66.41%] [G loss: 0.805348]\n",
      "417 [D loss: 0.633761, acc.: 64.84%] [G loss: 0.810517]\n",
      "418 [D loss: 0.617462, acc.: 67.58%] [G loss: 0.810725]\n",
      "419 [D loss: 0.652914, acc.: 64.45%] [G loss: 0.815698]\n",
      "420 [D loss: 0.655607, acc.: 61.33%] [G loss: 0.805316]\n",
      "421 [D loss: 0.670288, acc.: 59.38%] [G loss: 0.859420]\n",
      "422 [D loss: 0.636551, acc.: 62.89%] [G loss: 0.899547]\n",
      "423 [D loss: 0.623738, acc.: 66.41%] [G loss: 0.967532]\n",
      "424 [D loss: 0.633550, acc.: 66.02%] [G loss: 0.992694]\n",
      "425 [D loss: 0.614310, acc.: 66.41%] [G loss: 1.078038]\n",
      "426 [D loss: 0.618354, acc.: 64.06%] [G loss: 1.109756]\n",
      "427 [D loss: 0.599678, acc.: 67.58%] [G loss: 1.096871]\n",
      "428 [D loss: 0.636775, acc.: 61.72%] [G loss: 1.032830]\n",
      "429 [D loss: 0.664560, acc.: 59.77%] [G loss: 0.991368]\n",
      "430 [D loss: 0.657560, acc.: 60.94%] [G loss: 0.988197]\n",
      "431 [D loss: 0.639231, acc.: 63.28%] [G loss: 0.951795]\n",
      "432 [D loss: 0.649016, acc.: 65.23%] [G loss: 0.956279]\n",
      "433 [D loss: 0.636663, acc.: 64.45%] [G loss: 0.934042]\n",
      "434 [D loss: 0.642837, acc.: 62.11%] [G loss: 0.922386]\n",
      "435 [D loss: 0.644445, acc.: 58.59%] [G loss: 0.902506]\n",
      "436 [D loss: 0.650565, acc.: 58.59%] [G loss: 0.876091]\n",
      "437 [D loss: 0.651507, acc.: 58.98%] [G loss: 0.838239]\n",
      "438 [D loss: 0.653233, acc.: 57.81%] [G loss: 0.814866]\n",
      "439 [D loss: 0.644017, acc.: 66.41%] [G loss: 0.836839]\n",
      "440 [D loss: 0.637984, acc.: 69.14%] [G loss: 0.813504]\n",
      "441 [D loss: 0.633142, acc.: 68.36%] [G loss: 0.833173]\n",
      "442 [D loss: 0.624979, acc.: 66.41%] [G loss: 0.857816]\n",
      "443 [D loss: 0.622769, acc.: 67.97%] [G loss: 0.876076]\n",
      "444 [D loss: 0.605571, acc.: 67.97%] [G loss: 0.890550]\n",
      "445 [D loss: 0.603818, acc.: 68.36%] [G loss: 0.898703]\n",
      "446 [D loss: 0.599063, acc.: 69.92%] [G loss: 0.855274]\n",
      "447 [D loss: 0.632310, acc.: 62.50%] [G loss: 0.841445]\n",
      "448 [D loss: 0.618797, acc.: 65.23%] [G loss: 0.848851]\n",
      "449 [D loss: 0.651165, acc.: 57.42%] [G loss: 0.887933]\n",
      "450 [D loss: 0.630858, acc.: 61.33%] [G loss: 0.888052]\n",
      "451 [D loss: 0.625508, acc.: 64.45%] [G loss: 0.897179]\n",
      "452 [D loss: 0.641462, acc.: 63.28%] [G loss: 0.853514]\n",
      "453 [D loss: 0.634806, acc.: 67.97%] [G loss: 0.859876]\n",
      "454 [D loss: 0.630741, acc.: 67.19%] [G loss: 0.849730]\n",
      "455 [D loss: 0.634026, acc.: 66.02%] [G loss: 0.826920]\n",
      "456 [D loss: 0.637009, acc.: 68.75%] [G loss: 0.837542]\n",
      "457 [D loss: 0.619001, acc.: 69.14%] [G loss: 0.841921]\n",
      "458 [D loss: 0.622021, acc.: 67.19%] [G loss: 0.849862]\n",
      "459 [D loss: 0.613912, acc.: 66.80%] [G loss: 0.862634]\n",
      "460 [D loss: 0.620545, acc.: 64.84%] [G loss: 0.852020]\n",
      "461 [D loss: 0.617579, acc.: 66.80%] [G loss: 0.847791]\n",
      "462 [D loss: 0.630118, acc.: 66.02%] [G loss: 0.873779]\n",
      "463 [D loss: 0.629556, acc.: 64.84%] [G loss: 0.864555]\n",
      "464 [D loss: 0.623949, acc.: 66.41%] [G loss: 0.872595]\n",
      "465 [D loss: 0.617766, acc.: 64.84%] [G loss: 0.891160]\n",
      "466 [D loss: 0.630157, acc.: 64.06%] [G loss: 0.882128]\n",
      "467 [D loss: 0.623354, acc.: 68.75%] [G loss: 0.854672]\n",
      "468 [D loss: 0.638368, acc.: 64.45%] [G loss: 0.846890]\n",
      "469 [D loss: 0.638294, acc.: 65.23%] [G loss: 0.823441]\n",
      "470 [D loss: 0.626407, acc.: 66.02%] [G loss: 0.832952]\n",
      "471 [D loss: 0.618665, acc.: 65.62%] [G loss: 0.852690]\n",
      "472 [D loss: 0.618273, acc.: 65.62%] [G loss: 0.862192]\n",
      "473 [D loss: 0.625870, acc.: 62.89%] [G loss: 0.838636]\n",
      "474 [D loss: 0.639793, acc.: 56.25%] [G loss: 0.830125]\n",
      "475 [D loss: 0.670846, acc.: 53.12%] [G loss: 0.841301]\n",
      "476 [D loss: 0.646700, acc.: 57.42%] [G loss: 0.826203]\n",
      "477 [D loss: 0.684089, acc.: 48.44%] [G loss: 0.804479]\n",
      "478 [D loss: 0.686500, acc.: 49.61%] [G loss: 0.870833]\n",
      "479 [D loss: 0.680799, acc.: 51.56%] [G loss: 0.909602]\n",
      "480 [D loss: 0.630913, acc.: 62.50%] [G loss: 0.927402]\n",
      "481 [D loss: 0.622967, acc.: 67.19%] [G loss: 0.951349]\n",
      "482 [D loss: 0.608756, acc.: 67.19%] [G loss: 0.953252]\n",
      "483 [D loss: 0.596015, acc.: 71.09%] [G loss: 0.948332]\n",
      "484 [D loss: 0.591453, acc.: 68.36%] [G loss: 0.949549]\n",
      "485 [D loss: 0.586887, acc.: 72.66%] [G loss: 0.915269]\n",
      "486 [D loss: 0.586102, acc.: 71.48%] [G loss: 0.900423]\n",
      "487 [D loss: 0.610524, acc.: 67.58%] [G loss: 0.887791]\n",
      "488 [D loss: 0.627118, acc.: 67.97%] [G loss: 0.863540]\n",
      "489 [D loss: 0.642437, acc.: 62.11%] [G loss: 0.857025]\n",
      "490 [D loss: 0.651030, acc.: 58.59%] [G loss: 0.842514]\n",
      "491 [D loss: 0.682304, acc.: 48.83%] [G loss: 0.813181]\n",
      "492 [D loss: 0.682374, acc.: 51.95%] [G loss: 0.811725]\n",
      "493 [D loss: 0.685164, acc.: 51.17%] [G loss: 0.868792]\n",
      "494 [D loss: 0.660182, acc.: 60.16%] [G loss: 0.899165]\n",
      "495 [D loss: 0.642133, acc.: 62.11%] [G loss: 0.955025]\n",
      "496 [D loss: 0.617415, acc.: 64.84%] [G loss: 0.940959]\n",
      "497 [D loss: 0.613560, acc.: 65.23%] [G loss: 0.940126]\n",
      "498 [D loss: 0.629372, acc.: 59.77%] [G loss: 0.931229]\n",
      "499 [D loss: 0.622711, acc.: 62.50%] [G loss: 0.903150]\n",
      "500 [D loss: 0.633533, acc.: 62.50%] [G loss: 0.915815]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_4:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "501 [D loss: 0.626252, acc.: 64.06%] [G loss: 0.923687]\n",
      "502 [D loss: 0.608732, acc.: 68.75%] [G loss: 0.948566]\n",
      "503 [D loss: 0.600544, acc.: 71.48%] [G loss: 0.921712]\n",
      "504 [D loss: 0.616512, acc.: 67.19%] [G loss: 0.888690]\n",
      "505 [D loss: 0.618317, acc.: 66.80%] [G loss: 0.859637]\n",
      "506 [D loss: 0.626163, acc.: 60.16%] [G loss: 0.821797]\n",
      "507 [D loss: 0.627760, acc.: 58.59%] [G loss: 0.816351]\n",
      "508 [D loss: 0.631561, acc.: 56.64%] [G loss: 0.785819]\n",
      "509 [D loss: 0.635561, acc.: 55.47%] [G loss: 0.769021]\n",
      "510 [D loss: 0.623310, acc.: 59.77%] [G loss: 0.790302]\n",
      "511 [D loss: 0.633801, acc.: 58.20%] [G loss: 0.764132]\n",
      "512 [D loss: 0.616399, acc.: 64.84%] [G loss: 0.802825]\n",
      "513 [D loss: 0.612387, acc.: 67.19%] [G loss: 0.809406]\n",
      "514 [D loss: 0.612529, acc.: 71.48%] [G loss: 0.820172]\n",
      "515 [D loss: 0.607124, acc.: 69.92%] [G loss: 0.828052]\n",
      "516 [D loss: 0.602067, acc.: 70.31%] [G loss: 0.863492]\n",
      "517 [D loss: 0.602867, acc.: 69.92%] [G loss: 0.853090]\n",
      "518 [D loss: 0.612159, acc.: 68.36%] [G loss: 0.846344]\n",
      "519 [D loss: 0.613251, acc.: 67.97%] [G loss: 0.880087]\n",
      "520 [D loss: 0.614249, acc.: 69.14%] [G loss: 0.906219]\n",
      "521 [D loss: 0.579707, acc.: 73.05%] [G loss: 0.886042]\n",
      "522 [D loss: 0.597437, acc.: 69.14%] [G loss: 0.867961]\n",
      "523 [D loss: 0.604331, acc.: 69.14%] [G loss: 0.838439]\n",
      "524 [D loss: 0.628578, acc.: 64.06%] [G loss: 0.818468]\n",
      "525 [D loss: 0.634548, acc.: 64.06%] [G loss: 0.858736]\n",
      "526 [D loss: 0.615422, acc.: 67.97%] [G loss: 0.845520]\n",
      "527 [D loss: 0.621820, acc.: 66.41%] [G loss: 0.847809]\n",
      "528 [D loss: 0.632185, acc.: 65.23%] [G loss: 0.867262]\n",
      "529 [D loss: 0.630207, acc.: 67.19%] [G loss: 0.838072]\n",
      "530 [D loss: 0.619320, acc.: 63.28%] [G loss: 0.855960]\n",
      "531 [D loss: 0.646756, acc.: 59.77%] [G loss: 0.879391]\n",
      "532 [D loss: 0.613991, acc.: 65.23%] [G loss: 0.923695]\n",
      "533 [D loss: 0.615514, acc.: 64.06%] [G loss: 0.934159]\n",
      "534 [D loss: 0.616394, acc.: 61.72%] [G loss: 0.977444]\n",
      "535 [D loss: 0.598815, acc.: 67.97%] [G loss: 0.995446]\n",
      "536 [D loss: 0.619795, acc.: 64.84%] [G loss: 0.975274]\n",
      "537 [D loss: 0.636100, acc.: 60.94%] [G loss: 0.927993]\n",
      "538 [D loss: 0.653904, acc.: 57.81%] [G loss: 0.884034]\n",
      "539 [D loss: 0.691198, acc.: 50.39%] [G loss: 0.943734]\n",
      "540 [D loss: 0.659217, acc.: 59.77%] [G loss: 1.047438]\n",
      "541 [D loss: 0.634579, acc.: 58.59%] [G loss: 1.094706]\n",
      "542 [D loss: 0.610493, acc.: 62.89%] [G loss: 1.099172]\n",
      "543 [D loss: 0.586175, acc.: 66.41%] [G loss: 1.094061]\n",
      "544 [D loss: 0.588821, acc.: 65.62%] [G loss: 1.216221]\n",
      "545 [D loss: 0.555157, acc.: 70.70%] [G loss: 1.171892]\n",
      "546 [D loss: 0.570319, acc.: 72.27%] [G loss: 1.104536]\n",
      "547 [D loss: 0.618699, acc.: 64.84%] [G loss: 0.978064]\n",
      "548 [D loss: 0.634857, acc.: 63.28%] [G loss: 0.895859]\n",
      "549 [D loss: 0.642841, acc.: 64.84%] [G loss: 0.840781]\n",
      "550 [D loss: 0.641639, acc.: 58.98%] [G loss: 0.828945]\n",
      "551 [D loss: 0.657614, acc.: 52.73%] [G loss: 0.827275]\n",
      "552 [D loss: 0.632061, acc.: 60.16%] [G loss: 0.858442]\n",
      "553 [D loss: 0.615743, acc.: 61.33%] [G loss: 0.867725]\n",
      "554 [D loss: 0.603714, acc.: 65.62%] [G loss: 0.896522]\n",
      "555 [D loss: 0.614082, acc.: 64.45%] [G loss: 0.919999]\n",
      "556 [D loss: 0.598941, acc.: 65.23%] [G loss: 0.885759]\n",
      "557 [D loss: 0.592939, acc.: 66.41%] [G loss: 0.907674]\n",
      "558 [D loss: 0.591232, acc.: 65.62%] [G loss: 0.901082]\n",
      "559 [D loss: 0.592882, acc.: 64.06%] [G loss: 0.930636]\n",
      "560 [D loss: 0.593713, acc.: 66.80%] [G loss: 0.937405]\n",
      "561 [D loss: 0.587382, acc.: 67.19%] [G loss: 0.892100]\n",
      "562 [D loss: 0.598916, acc.: 68.75%] [G loss: 0.876124]\n",
      "563 [D loss: 0.594723, acc.: 68.75%] [G loss: 0.857393]\n",
      "564 [D loss: 0.603061, acc.: 67.58%] [G loss: 0.858904]\n",
      "565 [D loss: 0.607816, acc.: 66.41%] [G loss: 0.814131]\n",
      "566 [D loss: 0.633651, acc.: 61.33%] [G loss: 0.801322]\n",
      "567 [D loss: 0.637629, acc.: 59.77%] [G loss: 0.788372]\n",
      "568 [D loss: 0.612128, acc.: 68.36%] [G loss: 0.847741]\n",
      "569 [D loss: 0.614361, acc.: 64.84%] [G loss: 0.840088]\n",
      "570 [D loss: 0.621217, acc.: 62.50%] [G loss: 0.880872]\n",
      "571 [D loss: 0.606603, acc.: 65.62%] [G loss: 0.888223]\n",
      "572 [D loss: 0.589402, acc.: 70.70%] [G loss: 0.938795]\n",
      "573 [D loss: 0.584805, acc.: 71.09%] [G loss: 0.927580]\n",
      "574 [D loss: 0.600034, acc.: 66.41%] [G loss: 0.920492]\n",
      "575 [D loss: 0.629124, acc.: 58.59%] [G loss: 0.909175]\n",
      "576 [D loss: 0.620515, acc.: 62.11%] [G loss: 0.918278]\n",
      "577 [D loss: 0.657976, acc.: 52.73%] [G loss: 0.958987]\n",
      "578 [D loss: 0.620661, acc.: 59.38%] [G loss: 1.039166]\n",
      "579 [D loss: 0.586881, acc.: 66.41%] [G loss: 1.032872]\n",
      "580 [D loss: 0.563166, acc.: 68.75%] [G loss: 1.105985]\n",
      "581 [D loss: 0.546055, acc.: 73.05%] [G loss: 1.098441]\n",
      "582 [D loss: 0.533155, acc.: 75.39%] [G loss: 1.055567]\n",
      "583 [D loss: 0.553316, acc.: 75.00%] [G loss: 0.955108]\n",
      "584 [D loss: 0.562062, acc.: 74.22%] [G loss: 0.861213]\n",
      "585 [D loss: 0.609488, acc.: 63.28%] [G loss: 0.818814]\n",
      "586 [D loss: 0.623920, acc.: 62.11%] [G loss: 0.836598]\n",
      "587 [D loss: 0.624331, acc.: 56.25%] [G loss: 0.864459]\n",
      "588 [D loss: 0.635327, acc.: 57.03%] [G loss: 0.851740]\n",
      "589 [D loss: 0.671737, acc.: 51.17%] [G loss: 0.917355]\n",
      "590 [D loss: 0.648436, acc.: 59.77%] [G loss: 0.908205]\n",
      "591 [D loss: 0.641131, acc.: 60.55%] [G loss: 0.910635]\n",
      "592 [D loss: 0.656899, acc.: 58.59%] [G loss: 0.945499]\n",
      "593 [D loss: 0.632124, acc.: 62.89%] [G loss: 0.931233]\n",
      "594 [D loss: 0.636433, acc.: 60.55%] [G loss: 0.968552]\n",
      "595 [D loss: 0.643705, acc.: 58.59%] [G loss: 0.963838]\n",
      "596 [D loss: 0.600912, acc.: 61.72%] [G loss: 0.989889]\n",
      "597 [D loss: 0.586632, acc.: 63.67%] [G loss: 1.016293]\n",
      "598 [D loss: 0.592800, acc.: 65.62%] [G loss: 1.007445]\n",
      "599 [D loss: 0.570669, acc.: 72.66%] [G loss: 0.970613]\n",
      "600 [D loss: 0.569651, acc.: 75.00%] [G loss: 0.952116]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_4:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "601 [D loss: 0.578768, acc.: 71.09%] [G loss: 0.926177]\n",
      "602 [D loss: 0.584753, acc.: 69.53%] [G loss: 0.924315]\n",
      "603 [D loss: 0.586647, acc.: 65.23%] [G loss: 0.917539]\n",
      "604 [D loss: 0.590553, acc.: 66.41%] [G loss: 0.910633]\n",
      "605 [D loss: 0.620731, acc.: 60.55%] [G loss: 0.927372]\n",
      "606 [D loss: 0.606859, acc.: 60.94%] [G loss: 0.918698]\n",
      "607 [D loss: 0.626126, acc.: 59.38%] [G loss: 0.944848]\n",
      "608 [D loss: 0.612357, acc.: 69.14%] [G loss: 0.941778]\n",
      "609 [D loss: 0.590692, acc.: 69.92%] [G loss: 0.946402]\n",
      "610 [D loss: 0.592097, acc.: 68.75%] [G loss: 0.947670]\n",
      "611 [D loss: 0.596886, acc.: 67.19%] [G loss: 0.934074]\n",
      "612 [D loss: 0.568034, acc.: 71.48%] [G loss: 0.923811]\n",
      "613 [D loss: 0.593269, acc.: 66.02%] [G loss: 0.901274]\n",
      "614 [D loss: 0.585211, acc.: 68.75%] [G loss: 0.875713]\n",
      "615 [D loss: 0.617931, acc.: 67.19%] [G loss: 0.884135]\n",
      "616 [D loss: 0.584165, acc.: 70.31%] [G loss: 0.882498]\n",
      "617 [D loss: 0.594976, acc.: 67.58%] [G loss: 0.894145]\n",
      "618 [D loss: 0.591727, acc.: 69.14%] [G loss: 0.889613]\n",
      "619 [D loss: 0.590870, acc.: 67.58%] [G loss: 0.855821]\n",
      "620 [D loss: 0.608193, acc.: 64.45%] [G loss: 0.907679]\n",
      "621 [D loss: 0.608220, acc.: 61.72%] [G loss: 0.919252]\n",
      "622 [D loss: 0.598822, acc.: 67.19%] [G loss: 0.905411]\n",
      "623 [D loss: 0.580637, acc.: 69.92%] [G loss: 0.852517]\n",
      "624 [D loss: 0.588515, acc.: 67.58%] [G loss: 0.889585]\n",
      "625 [D loss: 0.579831, acc.: 71.88%] [G loss: 0.903382]\n",
      "626 [D loss: 0.576563, acc.: 72.66%] [G loss: 0.907183]\n",
      "627 [D loss: 0.581051, acc.: 70.70%] [G loss: 0.860733]\n",
      "628 [D loss: 0.575705, acc.: 72.27%] [G loss: 0.883523]\n",
      "629 [D loss: 0.588241, acc.: 71.88%] [G loss: 0.917166]\n",
      "630 [D loss: 0.592926, acc.: 69.14%] [G loss: 0.893926]\n",
      "631 [D loss: 0.597966, acc.: 64.84%] [G loss: 0.894206]\n",
      "632 [D loss: 0.585569, acc.: 67.97%] [G loss: 0.921695]\n",
      "633 [D loss: 0.578746, acc.: 70.70%] [G loss: 0.919859]\n",
      "634 [D loss: 0.607909, acc.: 66.41%] [G loss: 0.910923]\n",
      "635 [D loss: 0.594348, acc.: 69.53%] [G loss: 0.955995]\n",
      "636 [D loss: 0.595118, acc.: 69.14%] [G loss: 0.996375]\n",
      "637 [D loss: 0.573651, acc.: 70.31%] [G loss: 1.037416]\n",
      "638 [D loss: 0.601362, acc.: 63.28%] [G loss: 1.077642]\n",
      "639 [D loss: 0.596963, acc.: 66.80%] [G loss: 1.054887]\n",
      "640 [D loss: 0.619971, acc.: 63.28%] [G loss: 1.030369]\n",
      "641 [D loss: 0.606935, acc.: 64.06%] [G loss: 1.084368]\n",
      "642 [D loss: 0.600308, acc.: 67.19%] [G loss: 1.086268]\n",
      "643 [D loss: 0.590311, acc.: 66.41%] [G loss: 1.095166]\n",
      "644 [D loss: 0.576641, acc.: 67.97%] [G loss: 1.111130]\n",
      "645 [D loss: 0.583090, acc.: 66.41%] [G loss: 1.015833]\n",
      "646 [D loss: 0.607756, acc.: 64.84%] [G loss: 1.003111]\n",
      "647 [D loss: 0.593701, acc.: 65.62%] [G loss: 1.035992]\n",
      "648 [D loss: 0.603792, acc.: 64.06%] [G loss: 0.974908]\n",
      "649 [D loss: 0.651297, acc.: 61.33%] [G loss: 1.012374]\n",
      "650 [D loss: 0.604949, acc.: 64.06%] [G loss: 1.051054]\n",
      "651 [D loss: 0.581768, acc.: 68.36%] [G loss: 1.091438]\n",
      "652 [D loss: 0.591204, acc.: 68.75%] [G loss: 1.048746]\n",
      "653 [D loss: 0.606206, acc.: 66.02%] [G loss: 1.065722]\n",
      "654 [D loss: 0.586352, acc.: 68.36%] [G loss: 1.022932]\n",
      "655 [D loss: 0.601279, acc.: 67.58%] [G loss: 1.008281]\n",
      "656 [D loss: 0.608402, acc.: 64.84%] [G loss: 0.980509]\n",
      "657 [D loss: 0.622177, acc.: 62.89%] [G loss: 0.933469]\n",
      "658 [D loss: 0.614534, acc.: 65.23%] [G loss: 0.961362]\n",
      "659 [D loss: 0.602644, acc.: 69.92%] [G loss: 0.910635]\n",
      "660 [D loss: 0.614069, acc.: 67.58%] [G loss: 0.902192]\n",
      "661 [D loss: 0.606519, acc.: 69.53%] [G loss: 0.875030]\n",
      "662 [D loss: 0.601995, acc.: 66.02%] [G loss: 0.890726]\n",
      "663 [D loss: 0.624707, acc.: 66.02%] [G loss: 0.903994]\n",
      "664 [D loss: 0.592871, acc.: 71.09%] [G loss: 0.909449]\n",
      "665 [D loss: 0.597496, acc.: 71.09%] [G loss: 0.925458]\n",
      "666 [D loss: 0.583643, acc.: 72.66%] [G loss: 0.934764]\n",
      "667 [D loss: 0.588420, acc.: 72.27%] [G loss: 0.910140]\n",
      "668 [D loss: 0.592748, acc.: 71.48%] [G loss: 0.912302]\n",
      "669 [D loss: 0.604962, acc.: 71.88%] [G loss: 0.864319]\n",
      "670 [D loss: 0.600198, acc.: 69.14%] [G loss: 0.878440]\n",
      "671 [D loss: 0.601795, acc.: 68.75%] [G loss: 0.911765]\n",
      "672 [D loss: 0.609340, acc.: 64.45%] [G loss: 0.922498]\n",
      "673 [D loss: 0.624189, acc.: 61.33%] [G loss: 0.972544]\n",
      "674 [D loss: 0.606709, acc.: 63.28%] [G loss: 0.931705]\n",
      "675 [D loss: 0.624333, acc.: 62.89%] [G loss: 0.957227]\n",
      "676 [D loss: 0.608361, acc.: 64.06%] [G loss: 0.951526]\n",
      "677 [D loss: 0.596824, acc.: 67.58%] [G loss: 0.946946]\n",
      "678 [D loss: 0.585259, acc.: 71.88%] [G loss: 0.966142]\n",
      "679 [D loss: 0.578504, acc.: 71.48%] [G loss: 0.986822]\n",
      "680 [D loss: 0.587106, acc.: 71.48%] [G loss: 0.995330]\n",
      "681 [D loss: 0.567131, acc.: 70.70%] [G loss: 0.971623]\n",
      "682 [D loss: 0.583946, acc.: 67.97%] [G loss: 0.918837]\n",
      "683 [D loss: 0.613385, acc.: 60.94%] [G loss: 0.920398]\n",
      "684 [D loss: 0.619227, acc.: 58.59%] [G loss: 0.891807]\n",
      "685 [D loss: 0.629121, acc.: 62.89%] [G loss: 0.952180]\n",
      "686 [D loss: 0.590466, acc.: 67.58%] [G loss: 0.979224]\n",
      "687 [D loss: 0.639223, acc.: 58.98%] [G loss: 0.978598]\n",
      "688 [D loss: 0.591645, acc.: 67.97%] [G loss: 0.950837]\n",
      "689 [D loss: 0.638793, acc.: 61.72%] [G loss: 0.989248]\n",
      "690 [D loss: 0.594989, acc.: 66.41%] [G loss: 1.010706]\n",
      "691 [D loss: 0.615582, acc.: 63.28%] [G loss: 0.982246]\n",
      "692 [D loss: 0.590058, acc.: 68.36%] [G loss: 0.981928]\n",
      "693 [D loss: 0.600027, acc.: 67.19%] [G loss: 0.952649]\n",
      "694 [D loss: 0.593384, acc.: 66.41%] [G loss: 0.948866]\n",
      "695 [D loss: 0.598963, acc.: 67.58%] [G loss: 0.980708]\n",
      "696 [D loss: 0.613320, acc.: 62.50%] [G loss: 0.971959]\n",
      "697 [D loss: 0.602075, acc.: 65.62%] [G loss: 0.935939]\n",
      "698 [D loss: 0.587554, acc.: 66.80%] [G loss: 0.919531]\n",
      "699 [D loss: 0.599427, acc.: 66.41%] [G loss: 0.918403]\n",
      "700 [D loss: 0.602696, acc.: 65.62%] [G loss: 0.908661]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_4:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "701 [D loss: 0.606567, acc.: 62.11%] [G loss: 0.903339]\n",
      "702 [D loss: 0.609410, acc.: 65.62%] [G loss: 0.903016]\n",
      "703 [D loss: 0.593770, acc.: 67.58%] [G loss: 0.894459]\n",
      "704 [D loss: 0.606124, acc.: 64.45%] [G loss: 0.897658]\n",
      "705 [D loss: 0.590219, acc.: 65.62%] [G loss: 0.925330]\n",
      "706 [D loss: 0.575956, acc.: 68.36%] [G loss: 0.913538]\n",
      "707 [D loss: 0.588900, acc.: 65.23%] [G loss: 0.892346]\n",
      "708 [D loss: 0.562309, acc.: 68.36%] [G loss: 0.924243]\n",
      "709 [D loss: 0.586708, acc.: 69.14%] [G loss: 0.950831]\n",
      "710 [D loss: 0.578310, acc.: 64.84%] [G loss: 0.963398]\n",
      "711 [D loss: 0.576150, acc.: 71.09%] [G loss: 0.876624]\n",
      "712 [D loss: 0.604070, acc.: 66.80%] [G loss: 0.921532]\n",
      "713 [D loss: 0.591933, acc.: 68.36%] [G loss: 0.912519]\n",
      "714 [D loss: 0.587167, acc.: 71.88%] [G loss: 0.891031]\n",
      "715 [D loss: 0.602188, acc.: 65.62%] [G loss: 0.887371]\n",
      "716 [D loss: 0.587892, acc.: 67.97%] [G loss: 0.895624]\n",
      "717 [D loss: 0.603239, acc.: 67.19%] [G loss: 0.940882]\n",
      "718 [D loss: 0.602921, acc.: 66.02%] [G loss: 0.945321]\n",
      "719 [D loss: 0.604805, acc.: 67.97%] [G loss: 0.934321]\n",
      "720 [D loss: 0.606692, acc.: 66.80%] [G loss: 0.935915]\n",
      "721 [D loss: 0.603560, acc.: 67.97%] [G loss: 0.931570]\n",
      "722 [D loss: 0.606308, acc.: 68.36%] [G loss: 0.959943]\n",
      "723 [D loss: 0.592555, acc.: 67.58%] [G loss: 0.946639]\n",
      "724 [D loss: 0.602256, acc.: 64.45%] [G loss: 0.964763]\n",
      "725 [D loss: 0.583524, acc.: 67.58%] [G loss: 0.984214]\n",
      "726 [D loss: 0.590494, acc.: 66.80%] [G loss: 0.938194]\n",
      "727 [D loss: 0.595405, acc.: 71.88%] [G loss: 0.907328]\n",
      "728 [D loss: 0.599775, acc.: 68.36%] [G loss: 0.963761]\n",
      "729 [D loss: 0.572041, acc.: 73.05%] [G loss: 0.937001]\n",
      "730 [D loss: 0.578010, acc.: 69.53%] [G loss: 0.939909]\n",
      "731 [D loss: 0.582142, acc.: 69.53%] [G loss: 0.936352]\n",
      "732 [D loss: 0.566052, acc.: 74.61%] [G loss: 0.934287]\n",
      "733 [D loss: 0.566039, acc.: 73.05%] [G loss: 0.928199]\n",
      "734 [D loss: 0.569491, acc.: 71.48%] [G loss: 0.919344]\n",
      "735 [D loss: 0.585083, acc.: 71.88%] [G loss: 0.949848]\n",
      "736 [D loss: 0.587598, acc.: 67.58%] [G loss: 0.961297]\n",
      "737 [D loss: 0.588119, acc.: 66.41%] [G loss: 0.998907]\n",
      "738 [D loss: 0.599796, acc.: 63.28%] [G loss: 1.024350]\n",
      "739 [D loss: 0.591139, acc.: 66.80%] [G loss: 1.097564]\n",
      "740 [D loss: 0.584266, acc.: 67.97%] [G loss: 1.079565]\n",
      "741 [D loss: 0.578551, acc.: 69.53%] [G loss: 1.017568]\n",
      "742 [D loss: 0.564496, acc.: 72.27%] [G loss: 1.032144]\n",
      "743 [D loss: 0.561697, acc.: 67.97%] [G loss: 1.015620]\n",
      "744 [D loss: 0.546883, acc.: 71.48%] [G loss: 0.989346]\n",
      "745 [D loss: 0.561455, acc.: 70.70%] [G loss: 0.989402]\n",
      "746 [D loss: 0.621874, acc.: 61.33%] [G loss: 0.997446]\n",
      "747 [D loss: 0.591844, acc.: 67.97%] [G loss: 1.034555]\n",
      "748 [D loss: 0.594885, acc.: 67.97%] [G loss: 1.045249]\n",
      "749 [D loss: 0.609497, acc.: 64.84%] [G loss: 1.077123]\n",
      "750 [D loss: 0.591587, acc.: 69.92%] [G loss: 1.020442]\n",
      "751 [D loss: 0.607580, acc.: 66.02%] [G loss: 0.979375]\n",
      "752 [D loss: 0.585621, acc.: 69.53%] [G loss: 0.913263]\n",
      "753 [D loss: 0.617044, acc.: 64.45%] [G loss: 0.923677]\n",
      "754 [D loss: 0.589693, acc.: 65.62%] [G loss: 0.889588]\n",
      "755 [D loss: 0.606805, acc.: 65.23%] [G loss: 0.953111]\n",
      "756 [D loss: 0.590680, acc.: 69.14%] [G loss: 0.948337]\n",
      "757 [D loss: 0.564184, acc.: 69.92%] [G loss: 0.961502]\n",
      "758 [D loss: 0.577825, acc.: 66.80%] [G loss: 0.962787]\n",
      "759 [D loss: 0.555970, acc.: 72.27%] [G loss: 0.958180]\n",
      "760 [D loss: 0.560067, acc.: 73.44%] [G loss: 0.966146]\n",
      "761 [D loss: 0.544977, acc.: 76.95%] [G loss: 0.958657]\n",
      "762 [D loss: 0.555390, acc.: 72.66%] [G loss: 0.951447]\n",
      "763 [D loss: 0.561866, acc.: 72.66%] [G loss: 1.005490]\n",
      "764 [D loss: 0.540153, acc.: 71.88%] [G loss: 0.999281]\n",
      "765 [D loss: 0.562018, acc.: 75.00%] [G loss: 1.000246]\n",
      "766 [D loss: 0.536816, acc.: 75.00%] [G loss: 0.954801]\n",
      "767 [D loss: 0.563867, acc.: 71.48%] [G loss: 0.976434]\n",
      "768 [D loss: 0.582893, acc.: 68.36%] [G loss: 0.969460]\n",
      "769 [D loss: 0.581710, acc.: 69.14%] [G loss: 0.989059]\n",
      "770 [D loss: 0.609707, acc.: 62.50%] [G loss: 0.980367]\n",
      "771 [D loss: 0.596381, acc.: 69.53%] [G loss: 1.022231]\n",
      "772 [D loss: 0.590410, acc.: 70.70%] [G loss: 1.023093]\n",
      "773 [D loss: 0.587689, acc.: 67.58%] [G loss: 1.079258]\n",
      "774 [D loss: 0.585215, acc.: 69.53%] [G loss: 1.099540]\n",
      "775 [D loss: 0.570442, acc.: 70.31%] [G loss: 1.097835]\n",
      "776 [D loss: 0.564829, acc.: 71.48%] [G loss: 1.091695]\n",
      "777 [D loss: 0.591851, acc.: 67.19%] [G loss: 1.098022]\n",
      "778 [D loss: 0.560567, acc.: 70.31%] [G loss: 1.147117]\n",
      "779 [D loss: 0.543882, acc.: 71.48%] [G loss: 1.119490]\n",
      "780 [D loss: 0.569531, acc.: 73.05%] [G loss: 1.082095]\n",
      "781 [D loss: 0.565442, acc.: 71.48%] [G loss: 1.024515]\n",
      "782 [D loss: 0.619779, acc.: 63.67%] [G loss: 1.049247]\n",
      "783 [D loss: 0.576158, acc.: 71.48%] [G loss: 1.070423]\n",
      "784 [D loss: 0.615211, acc.: 65.62%] [G loss: 1.068152]\n",
      "785 [D loss: 0.575188, acc.: 69.92%] [G loss: 1.111575]\n",
      "786 [D loss: 0.576076, acc.: 67.19%] [G loss: 1.085344]\n",
      "787 [D loss: 0.562298, acc.: 69.92%] [G loss: 1.068384]\n",
      "788 [D loss: 0.565725, acc.: 69.14%] [G loss: 1.011657]\n",
      "789 [D loss: 0.546172, acc.: 75.39%] [G loss: 0.954912]\n",
      "790 [D loss: 0.580088, acc.: 68.75%] [G loss: 0.938283]\n",
      "791 [D loss: 0.573531, acc.: 67.97%] [G loss: 0.943089]\n",
      "792 [D loss: 0.612733, acc.: 60.94%] [G loss: 0.978955]\n",
      "793 [D loss: 0.568074, acc.: 71.48%] [G loss: 1.035158]\n",
      "794 [D loss: 0.562251, acc.: 70.70%] [G loss: 1.085469]\n",
      "795 [D loss: 0.540709, acc.: 73.83%] [G loss: 1.032080]\n",
      "796 [D loss: 0.557889, acc.: 71.48%] [G loss: 1.042034]\n",
      "797 [D loss: 0.574593, acc.: 73.44%] [G loss: 1.032361]\n",
      "798 [D loss: 0.569593, acc.: 71.09%] [G loss: 1.023673]\n",
      "799 [D loss: 0.580834, acc.: 69.14%] [G loss: 1.032483]\n",
      "800 [D loss: 0.586529, acc.: 68.36%] [G loss: 0.958454]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_4:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "801 [D loss: 0.582882, acc.: 67.19%] [G loss: 1.011533]\n",
      "802 [D loss: 0.590924, acc.: 67.97%] [G loss: 0.951214]\n",
      "803 [D loss: 0.579185, acc.: 66.80%] [G loss: 0.990991]\n",
      "804 [D loss: 0.577938, acc.: 68.36%] [G loss: 1.007152]\n",
      "805 [D loss: 0.558612, acc.: 72.27%] [G loss: 1.007803]\n",
      "806 [D loss: 0.551763, acc.: 72.27%] [G loss: 1.040786]\n",
      "807 [D loss: 0.571587, acc.: 70.31%] [G loss: 1.041248]\n",
      "808 [D loss: 0.567067, acc.: 68.75%] [G loss: 1.087610]\n",
      "809 [D loss: 0.554350, acc.: 71.48%] [G loss: 1.064863]\n",
      "810 [D loss: 0.558797, acc.: 69.92%] [G loss: 1.035909]\n",
      "811 [D loss: 0.560944, acc.: 71.88%] [G loss: 1.037624]\n",
      "812 [D loss: 0.582825, acc.: 66.41%] [G loss: 0.986082]\n",
      "813 [D loss: 0.659096, acc.: 63.28%] [G loss: 1.049579]\n",
      "814 [D loss: 0.579796, acc.: 68.36%] [G loss: 1.208519]\n",
      "815 [D loss: 0.540719, acc.: 70.31%] [G loss: 1.260139]\n",
      "816 [D loss: 0.545406, acc.: 71.09%] [G loss: 1.090708]\n",
      "817 [D loss: 0.556266, acc.: 71.48%] [G loss: 1.041710]\n",
      "818 [D loss: 0.586077, acc.: 68.36%] [G loss: 0.953429]\n",
      "819 [D loss: 0.586518, acc.: 67.58%] [G loss: 0.951189]\n",
      "820 [D loss: 0.580854, acc.: 67.58%] [G loss: 0.980667]\n",
      "821 [D loss: 0.572999, acc.: 69.92%] [G loss: 0.930660]\n",
      "822 [D loss: 0.581945, acc.: 69.53%] [G loss: 0.884857]\n",
      "823 [D loss: 0.583841, acc.: 66.02%] [G loss: 0.923570]\n",
      "824 [D loss: 0.589131, acc.: 65.23%] [G loss: 0.935628]\n",
      "825 [D loss: 0.597633, acc.: 66.80%] [G loss: 0.952971]\n",
      "826 [D loss: 0.600606, acc.: 64.84%] [G loss: 0.994822]\n",
      "827 [D loss: 0.560015, acc.: 71.88%] [G loss: 0.977453]\n",
      "828 [D loss: 0.575710, acc.: 69.92%] [G loss: 0.954018]\n",
      "829 [D loss: 0.569407, acc.: 71.48%] [G loss: 0.946041]\n",
      "830 [D loss: 0.557011, acc.: 69.92%] [G loss: 0.968447]\n",
      "831 [D loss: 0.569646, acc.: 69.92%] [G loss: 0.968221]\n",
      "832 [D loss: 0.566418, acc.: 70.70%] [G loss: 0.981634]\n",
      "833 [D loss: 0.568519, acc.: 69.92%] [G loss: 1.040726]\n",
      "834 [D loss: 0.551422, acc.: 73.44%] [G loss: 1.078920]\n",
      "835 [D loss: 0.554913, acc.: 72.66%] [G loss: 1.080451]\n",
      "836 [D loss: 0.536120, acc.: 75.00%] [G loss: 1.082391]\n",
      "837 [D loss: 0.564187, acc.: 69.92%] [G loss: 1.062306]\n",
      "838 [D loss: 0.563842, acc.: 69.92%] [G loss: 1.119244]\n",
      "839 [D loss: 0.554425, acc.: 73.44%] [G loss: 1.077585]\n",
      "840 [D loss: 0.563656, acc.: 72.27%] [G loss: 0.993681]\n",
      "841 [D loss: 0.545610, acc.: 71.88%] [G loss: 1.003642]\n",
      "842 [D loss: 0.545288, acc.: 73.05%] [G loss: 0.965242]\n",
      "843 [D loss: 0.545708, acc.: 70.70%] [G loss: 1.017098]\n",
      "844 [D loss: 0.540959, acc.: 72.27%] [G loss: 1.065911]\n",
      "845 [D loss: 0.529972, acc.: 74.61%] [G loss: 1.058602]\n",
      "846 [D loss: 0.528872, acc.: 73.05%] [G loss: 1.019142]\n",
      "847 [D loss: 0.545431, acc.: 70.70%] [G loss: 0.985773]\n",
      "848 [D loss: 0.546813, acc.: 70.70%] [G loss: 0.991776]\n",
      "849 [D loss: 0.541849, acc.: 70.70%] [G loss: 0.965638]\n",
      "850 [D loss: 0.568780, acc.: 69.53%] [G loss: 1.032525]\n",
      "851 [D loss: 0.531325, acc.: 71.88%] [G loss: 1.045780]\n",
      "852 [D loss: 0.526847, acc.: 74.22%] [G loss: 1.002796]\n",
      "853 [D loss: 0.553462, acc.: 72.27%] [G loss: 1.085398]\n",
      "854 [D loss: 0.556281, acc.: 72.27%] [G loss: 1.106764]\n",
      "855 [D loss: 0.520892, acc.: 76.17%] [G loss: 1.166106]\n",
      "856 [D loss: 0.566717, acc.: 68.36%] [G loss: 1.136059]\n",
      "857 [D loss: 0.530861, acc.: 73.83%] [G loss: 1.118538]\n",
      "858 [D loss: 0.584006, acc.: 68.75%] [G loss: 1.035946]\n",
      "859 [D loss: 0.580595, acc.: 67.19%] [G loss: 1.061373]\n",
      "860 [D loss: 0.605439, acc.: 62.89%] [G loss: 1.101241]\n",
      "861 [D loss: 0.591730, acc.: 66.02%] [G loss: 1.066653]\n",
      "862 [D loss: 0.580816, acc.: 65.23%] [G loss: 1.053527]\n",
      "863 [D loss: 0.579331, acc.: 69.53%] [G loss: 1.056419]\n",
      "864 [D loss: 0.558153, acc.: 71.48%] [G loss: 1.075803]\n",
      "865 [D loss: 0.536870, acc.: 71.88%] [G loss: 1.107336]\n",
      "866 [D loss: 0.542725, acc.: 71.88%] [G loss: 1.091437]\n",
      "867 [D loss: 0.538597, acc.: 73.05%] [G loss: 1.081528]\n",
      "868 [D loss: 0.540938, acc.: 72.66%] [G loss: 1.067156]\n",
      "869 [D loss: 0.550364, acc.: 70.31%] [G loss: 1.043332]\n",
      "870 [D loss: 0.543265, acc.: 73.83%] [G loss: 1.058939]\n",
      "871 [D loss: 0.545155, acc.: 71.09%] [G loss: 1.044608]\n",
      "872 [D loss: 0.551868, acc.: 68.36%] [G loss: 0.987808]\n",
      "873 [D loss: 0.547573, acc.: 71.88%] [G loss: 1.042067]\n",
      "874 [D loss: 0.550858, acc.: 73.44%] [G loss: 1.063499]\n",
      "875 [D loss: 0.533545, acc.: 72.66%] [G loss: 1.099931]\n",
      "876 [D loss: 0.547442, acc.: 70.70%] [G loss: 1.103646]\n",
      "877 [D loss: 0.524727, acc.: 75.39%] [G loss: 1.062743]\n",
      "878 [D loss: 0.540599, acc.: 73.44%] [G loss: 1.089104]\n",
      "879 [D loss: 0.542504, acc.: 72.27%] [G loss: 1.088964]\n",
      "880 [D loss: 0.508992, acc.: 73.44%] [G loss: 1.124738]\n",
      "881 [D loss: 0.509568, acc.: 73.83%] [G loss: 1.140628]\n",
      "882 [D loss: 0.499298, acc.: 74.61%] [G loss: 1.122811]\n",
      "883 [D loss: 0.506659, acc.: 74.22%] [G loss: 1.122158]\n",
      "884 [D loss: 0.499640, acc.: 76.56%] [G loss: 1.106638]\n",
      "885 [D loss: 0.497026, acc.: 76.95%] [G loss: 1.132326]\n",
      "886 [D loss: 0.501615, acc.: 78.12%] [G loss: 1.121187]\n",
      "887 [D loss: 0.514778, acc.: 78.52%] [G loss: 1.088413]\n",
      "888 [D loss: 0.558267, acc.: 69.14%] [G loss: 1.128224]\n",
      "889 [D loss: 0.553324, acc.: 72.66%] [G loss: 1.221278]\n",
      "890 [D loss: 0.561374, acc.: 68.75%] [G loss: 1.217610]\n",
      "891 [D loss: 0.580292, acc.: 67.97%] [G loss: 1.208171]\n",
      "892 [D loss: 0.568688, acc.: 70.31%] [G loss: 1.135070]\n",
      "893 [D loss: 0.596093, acc.: 66.02%] [G loss: 1.147146]\n",
      "894 [D loss: 0.555917, acc.: 69.53%] [G loss: 1.166653]\n",
      "895 [D loss: 0.542127, acc.: 71.48%] [G loss: 1.137845]\n",
      "896 [D loss: 0.554683, acc.: 68.75%] [G loss: 1.097706]\n",
      "897 [D loss: 0.531156, acc.: 72.66%] [G loss: 1.080156]\n",
      "898 [D loss: 0.555395, acc.: 68.36%] [G loss: 1.048153]\n",
      "899 [D loss: 0.522296, acc.: 75.00%] [G loss: 1.042128]\n",
      "900 [D loss: 0.522759, acc.: 71.88%] [G loss: 1.035440]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_4:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "901 [D loss: 0.521931, acc.: 74.22%] [G loss: 1.028193]\n",
      "902 [D loss: 0.519979, acc.: 75.39%] [G loss: 1.047144]\n",
      "903 [D loss: 0.492556, acc.: 80.08%] [G loss: 1.077362]\n",
      "904 [D loss: 0.504782, acc.: 76.56%] [G loss: 1.071256]\n",
      "905 [D loss: 0.501703, acc.: 76.56%] [G loss: 1.018418]\n",
      "906 [D loss: 0.541626, acc.: 75.00%] [G loss: 0.994345]\n",
      "907 [D loss: 0.544626, acc.: 70.31%] [G loss: 1.055228]\n",
      "908 [D loss: 0.568114, acc.: 67.58%] [G loss: 1.071835]\n",
      "909 [D loss: 0.559085, acc.: 71.88%] [G loss: 1.053383]\n",
      "910 [D loss: 0.572183, acc.: 69.14%] [G loss: 1.068744]\n",
      "911 [D loss: 0.544403, acc.: 70.70%] [G loss: 1.128945]\n",
      "912 [D loss: 0.538732, acc.: 69.53%] [G loss: 1.220947]\n",
      "913 [D loss: 0.546348, acc.: 71.09%] [G loss: 1.140044]\n",
      "914 [D loss: 0.542786, acc.: 69.92%] [G loss: 1.120001]\n",
      "915 [D loss: 0.519032, acc.: 73.44%] [G loss: 1.092400]\n",
      "916 [D loss: 0.540785, acc.: 70.31%] [G loss: 1.135875]\n",
      "917 [D loss: 0.507100, acc.: 73.05%] [G loss: 1.178388]\n",
      "918 [D loss: 0.518376, acc.: 73.44%] [G loss: 1.170992]\n",
      "919 [D loss: 0.488764, acc.: 77.73%] [G loss: 1.145368]\n",
      "920 [D loss: 0.502921, acc.: 76.56%] [G loss: 1.134540]\n",
      "921 [D loss: 0.524668, acc.: 74.22%] [G loss: 1.183083]\n",
      "922 [D loss: 0.528945, acc.: 74.61%] [G loss: 1.138072]\n",
      "923 [D loss: 0.534724, acc.: 74.61%] [G loss: 1.149663]\n",
      "924 [D loss: 0.537193, acc.: 74.61%] [G loss: 1.145791]\n",
      "925 [D loss: 0.528317, acc.: 73.05%] [G loss: 1.159613]\n",
      "926 [D loss: 0.533833, acc.: 74.61%] [G loss: 1.126035]\n",
      "927 [D loss: 0.516948, acc.: 72.66%] [G loss: 1.161641]\n",
      "928 [D loss: 0.520170, acc.: 72.66%] [G loss: 1.180863]\n",
      "929 [D loss: 0.534748, acc.: 71.09%] [G loss: 1.194393]\n",
      "930 [D loss: 0.532792, acc.: 70.31%] [G loss: 1.162952]\n",
      "931 [D loss: 0.561752, acc.: 69.53%] [G loss: 1.153304]\n",
      "932 [D loss: 0.531552, acc.: 70.31%] [G loss: 1.207786]\n",
      "933 [D loss: 0.506112, acc.: 73.83%] [G loss: 1.242028]\n",
      "934 [D loss: 0.542903, acc.: 71.09%] [G loss: 1.326963]\n",
      "935 [D loss: 0.502754, acc.: 75.78%] [G loss: 1.230050]\n",
      "936 [D loss: 0.545044, acc.: 67.19%] [G loss: 1.196668]\n",
      "937 [D loss: 0.522447, acc.: 75.00%] [G loss: 1.186485]\n",
      "938 [D loss: 0.526302, acc.: 73.05%] [G loss: 1.217630]\n",
      "939 [D loss: 0.555119, acc.: 71.48%] [G loss: 1.191567]\n",
      "940 [D loss: 0.524868, acc.: 73.83%] [G loss: 1.356486]\n",
      "941 [D loss: 0.528723, acc.: 75.00%] [G loss: 1.370442]\n",
      "942 [D loss: 0.515496, acc.: 73.83%] [G loss: 1.232091]\n",
      "943 [D loss: 0.518545, acc.: 75.00%] [G loss: 1.175428]\n",
      "944 [D loss: 0.547750, acc.: 71.09%] [G loss: 1.178443]\n",
      "945 [D loss: 0.504179, acc.: 75.39%] [G loss: 1.184767]\n",
      "946 [D loss: 0.493358, acc.: 78.52%] [G loss: 1.138129]\n",
      "947 [D loss: 0.505259, acc.: 78.52%] [G loss: 1.117125]\n",
      "948 [D loss: 0.520311, acc.: 75.00%] [G loss: 1.089591]\n",
      "949 [D loss: 0.539151, acc.: 75.00%] [G loss: 1.021944]\n",
      "950 [D loss: 0.569120, acc.: 70.31%] [G loss: 1.134256]\n",
      "951 [D loss: 0.541115, acc.: 69.14%] [G loss: 1.196440]\n",
      "952 [D loss: 0.544315, acc.: 73.05%] [G loss: 1.323757]\n",
      "953 [D loss: 0.534127, acc.: 70.31%] [G loss: 1.324110]\n",
      "954 [D loss: 0.526992, acc.: 71.88%] [G loss: 1.258043]\n",
      "955 [D loss: 0.508357, acc.: 75.39%] [G loss: 1.238379]\n",
      "956 [D loss: 0.516627, acc.: 75.78%] [G loss: 1.200134]\n",
      "957 [D loss: 0.507111, acc.: 75.39%] [G loss: 1.158049]\n",
      "958 [D loss: 0.526948, acc.: 73.83%] [G loss: 1.144429]\n",
      "959 [D loss: 0.512981, acc.: 76.95%] [G loss: 1.184757]\n",
      "960 [D loss: 0.528700, acc.: 70.70%] [G loss: 1.234949]\n",
      "961 [D loss: 0.502758, acc.: 76.95%] [G loss: 1.178596]\n",
      "962 [D loss: 0.520051, acc.: 75.00%] [G loss: 1.155475]\n",
      "963 [D loss: 0.527122, acc.: 74.22%] [G loss: 1.099519]\n",
      "964 [D loss: 0.538256, acc.: 74.61%] [G loss: 1.091193]\n",
      "965 [D loss: 0.518005, acc.: 73.83%] [G loss: 1.117023]\n",
      "966 [D loss: 0.517478, acc.: 78.12%] [G loss: 1.109043]\n",
      "967 [D loss: 0.508088, acc.: 75.39%] [G loss: 1.164769]\n",
      "968 [D loss: 0.496340, acc.: 77.34%] [G loss: 1.157526]\n",
      "969 [D loss: 0.495927, acc.: 78.12%] [G loss: 1.145816]\n",
      "970 [D loss: 0.487907, acc.: 79.30%] [G loss: 1.127156]\n",
      "971 [D loss: 0.477244, acc.: 80.47%] [G loss: 1.056212]\n",
      "972 [D loss: 0.498953, acc.: 76.56%] [G loss: 1.173611]\n",
      "973 [D loss: 0.487760, acc.: 78.52%] [G loss: 1.214425]\n",
      "974 [D loss: 0.490029, acc.: 78.12%] [G loss: 1.141843]\n",
      "975 [D loss: 0.521412, acc.: 75.78%] [G loss: 1.137233]\n",
      "976 [D loss: 0.518316, acc.: 75.78%] [G loss: 1.209116]\n",
      "977 [D loss: 0.526014, acc.: 71.88%] [G loss: 1.111702]\n",
      "978 [D loss: 0.530265, acc.: 73.44%] [G loss: 1.151065]\n",
      "979 [D loss: 0.550323, acc.: 68.36%] [G loss: 1.240452]\n",
      "980 [D loss: 0.533104, acc.: 71.88%] [G loss: 1.209339]\n",
      "981 [D loss: 0.544622, acc.: 70.70%] [G loss: 1.148666]\n",
      "982 [D loss: 0.532577, acc.: 71.09%] [G loss: 1.141576]\n",
      "983 [D loss: 0.533047, acc.: 71.48%] [G loss: 1.155250]\n",
      "984 [D loss: 0.522901, acc.: 71.48%] [G loss: 1.239688]\n",
      "985 [D loss: 0.512676, acc.: 73.05%] [G loss: 1.174298]\n",
      "986 [D loss: 0.509774, acc.: 73.44%] [G loss: 1.185475]\n",
      "987 [D loss: 0.493729, acc.: 75.00%] [G loss: 1.193902]\n",
      "988 [D loss: 0.495832, acc.: 73.83%] [G loss: 1.161582]\n",
      "989 [D loss: 0.476590, acc.: 76.95%] [G loss: 1.169084]\n",
      "990 [D loss: 0.511223, acc.: 73.83%] [G loss: 1.215781]\n",
      "991 [D loss: 0.484928, acc.: 75.78%] [G loss: 1.231704]\n",
      "992 [D loss: 0.490525, acc.: 74.61%] [G loss: 1.203993]\n",
      "993 [D loss: 0.482780, acc.: 78.91%] [G loss: 1.300713]\n",
      "994 [D loss: 0.481784, acc.: 74.22%] [G loss: 1.251975]\n",
      "995 [D loss: 0.488602, acc.: 77.34%] [G loss: 1.191511]\n",
      "996 [D loss: 0.467791, acc.: 78.91%] [G loss: 1.253197]\n",
      "997 [D loss: 0.492269, acc.: 73.83%] [G loss: 1.247482]\n",
      "998 [D loss: 0.467566, acc.: 76.95%] [G loss: 1.301432]\n",
      "999 [D loss: 0.496657, acc.: 73.44%] [G loss: 1.291407]\n"
     ]
    }
   ],
   "source": [
    "#Training the GAN model chosen: Vanilla GAN, CGAN, DCGAN, etc.\n",
    "synthesizer = model(gan_args)\n",
    "synthesizer.train(train_sample, train_args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Check the results obtained by the model here\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}