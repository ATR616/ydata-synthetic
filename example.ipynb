{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# The credit fraud dataset - Synthesizing the minority class\n",
    "In this notebook it's presented a practical exercise of how to use the avilable library GANs to synthesize tabular data.\n",
    "For the purpose of this exercise it has been used the Credit Fraud dataset from Kaggle, that you can find here:https: //www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/fabiana/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.cluster as cluster\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.gan import model\n",
    "importlib.reload(model)\n",
    "\n",
    "from models.gan.model import GAN\n",
    "from preprocessing.credit_fraud import *\n",
    "\n",
    "model = GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n"
     ]
    }
   ],
   "source": [
    "#Read the data\n",
    "data = pd.read_csv('data/data_processed.csv', index_col=[0])\n",
    "data_cols = list(data.columns[ data.columns != 'Class' ])\n",
    "label_cols = ['Class']\n",
    "\n",
    "print('Dataset columns: {}'.format(data_cols))\n",
    "sorted_cols = ['V14', 'V4', 'V10', 'V17', 'Time', 'V12', 'V26', 'Amount', 'V21', 'V8', 'V11', 'V7', 'V28', 'V19', 'V3', 'V22', 'V6', 'V20', 'V27', 'V16', 'V13', 'V25', 'V24', 'V18', 'V2', 'V1', 'V5', 'V15', 'V9', 'V23', 'Class']\n",
    "data = data[ sorted_cols ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info: Number of records - 492 Number of varibles - 31\n",
      "   count\n",
      "0    384\n",
      "1    108\n"
     ]
    }
   ],
   "source": [
    "#Before training the GAN do not forget to apply the required data transformations\n",
    "#To ease here we've applied a PowerTransformation\n",
    "data = transformations(data)\n",
    "\n",
    "#For the purpose of this example we will only synthesize the minority class\n",
    "train_data = data.loc[ data['Class']==1 ].copy()\n",
    "\n",
    "print(\"Dataset info: Number of records - {} Number of varibles - {}\".format(train_data.shape[0], train_data.shape[1]))\n",
    "\n",
    "algorithm = cluster.KMeans\n",
    "args, kwds = (), {'n_clusters':2, 'random_state':0}\n",
    "labels = algorithm(*args, **kwds).fit_predict(train_data[ data_cols ])\n",
    "\n",
    "print( pd.DataFrame( [ [np.sum(labels==i)] for i in np.unique(labels) ], columns=['count'], index=np.unique(labels) ) )\n",
    "\n",
    "fraud_w_classes = train_data.copy()\n",
    "fraud_w_classes['Class'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN training\n",
    "\n",
    "Below you can try to train your own generators using the available GANs architectures. You can train it either with labels (created using KMeans) or with no labels at all. \n",
    "\n",
    "Remeber that for this exercise in particular we've decided to synthesize only the minority class from the Credit Fraud dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Define the GAN and training parameters\n",
    "noise_dim = 32\n",
    "dim = 128\n",
    "batch_size = 128\n",
    "\n",
    "log_step = 100\n",
    "epochs = 5000+1\n",
    "learning_rate = 5e-4\n",
    "models_dir = './cache'\n",
    "\n",
    "train_sample = fraud_w_classes.copy().reset_index(drop=True)\n",
    "train_sample = pd.get_dummies(train_sample, columns=['Class'], prefix='Class', drop_first=True)\n",
    "label_cols = [ i for i in train_sample.columns if 'Class' in i ]\n",
    "data_cols = [ i for i in train_sample.columns if i not in label_cols ]\n",
    "train_sample[ data_cols ] = train_sample[ data_cols ] / 10 # scale to random noise size, one less thing to learn\n",
    "train_no_label = train_sample[ data_cols ]\n",
    "\n",
    "gan_args = [batch_size, learning_rate, noise_dim, train_sample.shape[1], dim]\n",
    "train_args = ['', epochs, log_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V14</th>\n",
       "      <th>V4</th>\n",
       "      <th>V10</th>\n",
       "      <th>V17</th>\n",
       "      <th>Time</th>\n",
       "      <th>V12</th>\n",
       "      <th>V26</th>\n",
       "      <th>Amount</th>\n",
       "      <th>V21</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V25</th>\n",
       "      <th>V24</th>\n",
       "      <th>V18</th>\n",
       "      <th>V2</th>\n",
       "      <th>V1</th>\n",
       "      <th>V5</th>\n",
       "      <th>V15</th>\n",
       "      <th>V9</th>\n",
       "      <th>V23</th>\n",
       "      <th>Class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.364140</td>\n",
       "      <td>0.247360</td>\n",
       "      <td>-0.284719</td>\n",
       "      <td>-0.306976</td>\n",
       "      <td>-0.208503</td>\n",
       "      <td>-0.227107</td>\n",
       "      <td>0.045838</td>\n",
       "      <td>-0.199245</td>\n",
       "      <td>0.071872</td>\n",
       "      <td>0.148524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004413</td>\n",
       "      <td>0.047312</td>\n",
       "      <td>-0.004068</td>\n",
       "      <td>0.136173</td>\n",
       "      <td>-0.127201</td>\n",
       "      <td>-0.038863</td>\n",
       "      <td>0.038433</td>\n",
       "      <td>-0.282337</td>\n",
       "      <td>-0.075195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.166205</td>\n",
       "      <td>0.152887</td>\n",
       "      <td>-0.077801</td>\n",
       "      <td>0.071059</td>\n",
       "      <td>-0.208294</td>\n",
       "      <td>-0.067495</td>\n",
       "      <td>-0.022004</td>\n",
       "      <td>0.181204</td>\n",
       "      <td>0.090903</td>\n",
       "      <td>-0.014051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051311</td>\n",
       "      <td>-0.057084</td>\n",
       "      <td>0.212486</td>\n",
       "      <td>-0.181909</td>\n",
       "      <td>-0.150359</td>\n",
       "      <td>0.099108</td>\n",
       "      <td>0.240807</td>\n",
       "      <td>-0.019959</td>\n",
       "      <td>0.230717</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.147407</td>\n",
       "      <td>0.155290</td>\n",
       "      <td>-0.148970</td>\n",
       "      <td>-0.495555</td>\n",
       "      <td>-0.195564</td>\n",
       "      <td>-0.387155</td>\n",
       "      <td>-0.119771</td>\n",
       "      <td>0.137843</td>\n",
       "      <td>-0.039545</td>\n",
       "      <td>-0.044662</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033824</td>\n",
       "      <td>-0.024992</td>\n",
       "      <td>-0.298661</td>\n",
       "      <td>0.120709</td>\n",
       "      <td>-0.126902</td>\n",
       "      <td>-0.060265</td>\n",
       "      <td>-0.078791</td>\n",
       "      <td>-0.016853</td>\n",
       "      <td>0.026953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.530328</td>\n",
       "      <td>0.175246</td>\n",
       "      <td>-0.518688</td>\n",
       "      <td>-1.186117</td>\n",
       "      <td>-0.187380</td>\n",
       "      <td>-0.529034</td>\n",
       "      <td>-0.150364</td>\n",
       "      <td>0.059165</td>\n",
       "      <td>0.079316</td>\n",
       "      <td>-0.031046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045752</td>\n",
       "      <td>-0.019474</td>\n",
       "      <td>-0.569714</td>\n",
       "      <td>0.089320</td>\n",
       "      <td>-0.187225</td>\n",
       "      <td>-0.082049</td>\n",
       "      <td>-0.006559</td>\n",
       "      <td>-0.017755</td>\n",
       "      <td>-0.070646</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.485576</td>\n",
       "      <td>0.285807</td>\n",
       "      <td>-0.248664</td>\n",
       "      <td>0.978364</td>\n",
       "      <td>-0.185638</td>\n",
       "      <td>-0.308949</td>\n",
       "      <td>0.115880</td>\n",
       "      <td>-0.153064</td>\n",
       "      <td>-0.051722</td>\n",
       "      <td>-0.053250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314021</td>\n",
       "      <td>-0.228582</td>\n",
       "      <td>0.383094</td>\n",
       "      <td>0.225762</td>\n",
       "      <td>0.066542</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>-0.042050</td>\n",
       "      <td>-0.121053</td>\n",
       "      <td>-0.105072</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>-0.523543</td>\n",
       "      <td>0.121158</td>\n",
       "      <td>-0.612683</td>\n",
       "      <td>-0.475608</td>\n",
       "      <td>0.164417</td>\n",
       "      <td>-0.342881</td>\n",
       "      <td>0.152349</td>\n",
       "      <td>0.164564</td>\n",
       "      <td>0.106174</td>\n",
       "      <td>0.066094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104685</td>\n",
       "      <td>-0.057246</td>\n",
       "      <td>-0.154118</td>\n",
       "      <td>0.071608</td>\n",
       "      <td>-0.113806</td>\n",
       "      <td>-0.113019</td>\n",
       "      <td>0.039830</td>\n",
       "      <td>-0.204329</td>\n",
       "      <td>0.104693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>-0.427730</td>\n",
       "      <td>0.100689</td>\n",
       "      <td>-0.336493</td>\n",
       "      <td>-0.350142</td>\n",
       "      <td>0.165773</td>\n",
       "      <td>-0.237416</td>\n",
       "      <td>0.144464</td>\n",
       "      <td>-0.161527</td>\n",
       "      <td>0.052358</td>\n",
       "      <td>0.017518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101398</td>\n",
       "      <td>-0.023974</td>\n",
       "      <td>-0.082700</td>\n",
       "      <td>0.084029</td>\n",
       "      <td>0.079401</td>\n",
       "      <td>0.031137</td>\n",
       "      <td>-0.070237</td>\n",
       "      <td>-0.104976</td>\n",
       "      <td>-0.024638</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>-0.347682</td>\n",
       "      <td>0.040168</td>\n",
       "      <td>-0.362882</td>\n",
       "      <td>-0.519358</td>\n",
       "      <td>0.165799</td>\n",
       "      <td>-0.220426</td>\n",
       "      <td>0.099463</td>\n",
       "      <td>0.074928</td>\n",
       "      <td>0.102687</td>\n",
       "      <td>0.126219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138295</td>\n",
       "      <td>-0.005116</td>\n",
       "      <td>-0.061707</td>\n",
       "      <td>0.071662</td>\n",
       "      <td>-0.061751</td>\n",
       "      <td>-0.081511</td>\n",
       "      <td>-0.080171</td>\n",
       "      <td>-0.056997</td>\n",
       "      <td>0.029946</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>-0.507529</td>\n",
       "      <td>0.125210</td>\n",
       "      <td>-0.571613</td>\n",
       "      <td>-0.479788</td>\n",
       "      <td>0.169878</td>\n",
       "      <td>-0.326933</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.138998</td>\n",
       "      <td>0.080595</td>\n",
       "      <td>0.108006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065488</td>\n",
       "      <td>-0.040280</td>\n",
       "      <td>-0.172473</td>\n",
       "      <td>0.032122</td>\n",
       "      <td>-0.152446</td>\n",
       "      <td>-0.061618</td>\n",
       "      <td>0.009960</td>\n",
       "      <td>-0.157797</td>\n",
       "      <td>-0.073768</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>-0.187473</td>\n",
       "      <td>0.036138</td>\n",
       "      <td>-0.082890</td>\n",
       "      <td>0.110715</td>\n",
       "      <td>0.172419</td>\n",
       "      <td>0.077097</td>\n",
       "      <td>-0.055913</td>\n",
       "      <td>0.040476</td>\n",
       "      <td>-0.021099</td>\n",
       "      <td>-0.014108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058136</td>\n",
       "      <td>-0.079923</td>\n",
       "      <td>0.145454</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.138765</td>\n",
       "      <td>0.083551</td>\n",
       "      <td>-0.092092</td>\n",
       "      <td>0.056602</td>\n",
       "      <td>-0.012857</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>492 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          V14        V4       V10       V17      Time       V12       V26  \\\n",
       "0   -0.364140  0.247360 -0.284719 -0.306976 -0.208503 -0.227107  0.045838   \n",
       "1   -0.166205  0.152887 -0.077801  0.071059 -0.208294 -0.067495 -0.022004   \n",
       "2   -0.147407  0.155290 -0.148970 -0.495555 -0.195564 -0.387155 -0.119771   \n",
       "3   -0.530328  0.175246 -0.518688 -1.186117 -0.187380 -0.529034 -0.150364   \n",
       "4   -0.485576  0.285807 -0.248664  0.978364 -0.185638 -0.308949  0.115880   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "487 -0.523543  0.121158 -0.612683 -0.475608  0.164417 -0.342881  0.152349   \n",
       "488 -0.427730  0.100689 -0.336493 -0.350142  0.165773 -0.237416  0.144464   \n",
       "489 -0.347682  0.040168 -0.362882 -0.519358  0.165799 -0.220426  0.099463   \n",
       "490 -0.507529  0.125210 -0.571613 -0.479788  0.169878 -0.326933  0.122500   \n",
       "491 -0.187473  0.036138 -0.082890  0.110715  0.172419  0.077097 -0.055913   \n",
       "\n",
       "       Amount       V21        V8  ...       V25       V24       V18  \\\n",
       "0   -0.199245  0.071872  0.148524  ...  0.004413  0.047312 -0.004068   \n",
       "1    0.181204  0.090903 -0.014051  ...  0.051311 -0.057084  0.212486   \n",
       "2    0.137843 -0.039545 -0.044662  ... -0.033824 -0.024992 -0.298661   \n",
       "3    0.059165  0.079316 -0.031046  ...  0.045752 -0.019474 -0.569714   \n",
       "4   -0.153064 -0.051722 -0.053250  ...  0.314021 -0.228582  0.383094   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "487  0.164564  0.106174  0.066094  ...  0.104685 -0.057246 -0.154118   \n",
       "488 -0.161527  0.052358  0.017518  ...  0.101398 -0.023974 -0.082700   \n",
       "489  0.074928  0.102687  0.126219  ... -0.138295 -0.005116 -0.061707   \n",
       "490  0.138998  0.080595  0.108006  ... -0.065488 -0.040280 -0.172473   \n",
       "491  0.040476 -0.021099 -0.014108  ...  0.058136 -0.079923  0.145454   \n",
       "\n",
       "           V2        V1        V5       V15        V9       V23  Class_1  \n",
       "0    0.136173 -0.127201 -0.038863  0.038433 -0.282337 -0.075195        0  \n",
       "1   -0.181909 -0.150359  0.099108  0.240807 -0.019959  0.230717        0  \n",
       "2    0.120709 -0.126902 -0.060265 -0.078791 -0.016853  0.026953        0  \n",
       "3    0.089320 -0.187225 -0.082049 -0.006559 -0.017755 -0.070646        0  \n",
       "4    0.225762  0.066542  0.270500 -0.042050 -0.121053 -0.105072        0  \n",
       "..        ...       ...       ...       ...       ...       ...      ...  \n",
       "487  0.071608 -0.113806 -0.113019  0.039830 -0.204329  0.104693        0  \n",
       "488  0.084029  0.079401  0.031137 -0.070237 -0.104976 -0.024638        0  \n",
       "489  0.071662 -0.061751 -0.081511 -0.080171 -0.056997  0.029946        0  \n",
       "490  0.032122 -0.152446 -0.061618  0.009960 -0.157797 -0.073768        0  \n",
       "491  0.002641  0.138765  0.083551 -0.092092  0.056602 -0.012857        0  \n",
       "\n",
       "[492 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.713639, acc.: 21.48%] [G loss: 0.681439]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "1 [D loss: 0.685176, acc.: 50.39%] [G loss: 0.670198]\n",
      "2 [D loss: 0.684121, acc.: 50.00%] [G loss: 0.657245]\n",
      "3 [D loss: 0.674379, acc.: 50.00%] [G loss: 0.685362]\n",
      "4 [D loss: 0.651105, acc.: 49.61%] [G loss: 0.757057]\n",
      "5 [D loss: 0.672869, acc.: 49.22%] [G loss: 0.755655]\n",
      "6 [D loss: 0.771544, acc.: 47.27%] [G loss: 0.646324]\n",
      "7 [D loss: 0.817958, acc.: 43.75%] [G loss: 0.631879]\n",
      "8 [D loss: 0.771024, acc.: 39.06%] [G loss: 0.761065]\n",
      "9 [D loss: 0.688058, acc.: 58.20%] [G loss: 0.963590]\n",
      "10 [D loss: 0.620855, acc.: 66.80%] [G loss: 1.123103]\n",
      "11 [D loss: 0.610712, acc.: 64.84%] [G loss: 1.165010]\n",
      "12 [D loss: 0.650455, acc.: 63.67%] [G loss: 1.021778]\n",
      "13 [D loss: 0.681055, acc.: 63.28%] [G loss: 0.921371]\n",
      "14 [D loss: 0.673429, acc.: 66.80%] [G loss: 0.919946]\n",
      "15 [D loss: 0.646743, acc.: 71.88%] [G loss: 0.942330]\n",
      "16 [D loss: 0.649137, acc.: 73.44%] [G loss: 0.911851]\n",
      "17 [D loss: 0.652963, acc.: 73.44%] [G loss: 0.868513]\n",
      "18 [D loss: 0.656502, acc.: 76.56%] [G loss: 0.828774]\n",
      "19 [D loss: 0.657330, acc.: 78.12%] [G loss: 0.810277]\n",
      "20 [D loss: 0.651735, acc.: 76.95%] [G loss: 0.791607]\n",
      "21 [D loss: 0.655094, acc.: 69.92%] [G loss: 0.761897]\n",
      "22 [D loss: 0.656587, acc.: 58.59%] [G loss: 0.739673]\n",
      "23 [D loss: 0.652183, acc.: 53.12%] [G loss: 0.720941]\n",
      "24 [D loss: 0.649283, acc.: 52.73%] [G loss: 0.715732]\n",
      "25 [D loss: 0.644100, acc.: 51.95%] [G loss: 0.723760]\n",
      "26 [D loss: 0.642363, acc.: 51.17%] [G loss: 0.720850]\n",
      "27 [D loss: 0.636691, acc.: 51.95%] [G loss: 0.740059]\n",
      "28 [D loss: 0.629318, acc.: 54.69%] [G loss: 0.771859]\n",
      "29 [D loss: 0.610882, acc.: 72.27%] [G loss: 0.800345]\n",
      "30 [D loss: 0.602106, acc.: 76.56%] [G loss: 0.829055]\n",
      "31 [D loss: 0.603010, acc.: 75.78%] [G loss: 0.813254]\n",
      "32 [D loss: 0.622953, acc.: 51.95%] [G loss: 0.761953]\n",
      "33 [D loss: 0.647566, acc.: 45.70%] [G loss: 0.739171]\n",
      "34 [D loss: 0.644524, acc.: 42.97%] [G loss: 0.766448]\n",
      "35 [D loss: 0.627148, acc.: 51.17%] [G loss: 0.809614]\n",
      "36 [D loss: 0.619714, acc.: 58.20%] [G loss: 0.850175]\n",
      "37 [D loss: 0.612665, acc.: 64.06%] [G loss: 0.863046]\n",
      "38 [D loss: 0.630124, acc.: 56.64%] [G loss: 0.844600]\n",
      "39 [D loss: 0.615825, acc.: 63.67%] [G loss: 0.859117]\n",
      "40 [D loss: 0.666012, acc.: 55.86%] [G loss: 0.854217]\n",
      "41 [D loss: 0.677029, acc.: 52.34%] [G loss: 0.859610]\n",
      "42 [D loss: 0.688107, acc.: 51.56%] [G loss: 0.935300]\n",
      "43 [D loss: 0.613249, acc.: 62.11%] [G loss: 1.190202]\n",
      "44 [D loss: 0.497611, acc.: 81.64%] [G loss: 1.391277]\n",
      "45 [D loss: 0.459319, acc.: 82.81%] [G loss: 1.370399]\n",
      "46 [D loss: 0.476230, acc.: 83.59%] [G loss: 1.170797]\n",
      "47 [D loss: 0.516663, acc.: 75.39%] [G loss: 0.925570]\n",
      "48 [D loss: 0.581379, acc.: 57.42%] [G loss: 0.764904]\n",
      "49 [D loss: 0.644821, acc.: 45.31%] [G loss: 0.661346]\n",
      "50 [D loss: 0.704431, acc.: 39.84%] [G loss: 0.591724]\n",
      "51 [D loss: 0.715687, acc.: 39.84%] [G loss: 0.624232]\n",
      "52 [D loss: 0.709866, acc.: 39.84%] [G loss: 0.695251]\n",
      "53 [D loss: 0.724588, acc.: 40.62%] [G loss: 0.825564]\n",
      "54 [D loss: 0.754621, acc.: 37.11%] [G loss: 0.895155]\n",
      "55 [D loss: 0.790502, acc.: 21.88%] [G loss: 0.874391]\n",
      "56 [D loss: 0.823770, acc.: 12.50%] [G loss: 0.852189]\n",
      "57 [D loss: 0.779589, acc.: 19.53%] [G loss: 0.920570]\n",
      "58 [D loss: 0.715464, acc.: 53.12%] [G loss: 1.074883]\n",
      "59 [D loss: 0.659864, acc.: 61.33%] [G loss: 1.203761]\n",
      "60 [D loss: 0.635864, acc.: 66.02%] [G loss: 1.123055]\n",
      "61 [D loss: 0.625378, acc.: 70.31%] [G loss: 1.082932]\n",
      "62 [D loss: 0.633674, acc.: 73.44%] [G loss: 0.975959]\n",
      "63 [D loss: 0.650551, acc.: 69.14%] [G loss: 0.897262]\n",
      "64 [D loss: 0.654316, acc.: 64.06%] [G loss: 0.861353]\n",
      "65 [D loss: 0.644689, acc.: 65.62%] [G loss: 0.855828]\n",
      "66 [D loss: 0.630377, acc.: 70.31%] [G loss: 0.880104]\n",
      "67 [D loss: 0.612764, acc.: 80.08%] [G loss: 0.902382]\n",
      "68 [D loss: 0.603064, acc.: 78.52%] [G loss: 0.926617]\n",
      "69 [D loss: 0.577563, acc.: 87.50%] [G loss: 0.936028]\n",
      "70 [D loss: 0.555820, acc.: 91.80%] [G loss: 0.950924]\n",
      "71 [D loss: 0.546706, acc.: 92.97%] [G loss: 0.937680]\n",
      "72 [D loss: 0.542053, acc.: 90.23%] [G loss: 0.921337]\n",
      "73 [D loss: 0.530212, acc.: 91.02%] [G loss: 0.910993]\n",
      "74 [D loss: 0.540280, acc.: 78.91%] [G loss: 0.903208]\n",
      "75 [D loss: 0.537443, acc.: 78.52%] [G loss: 0.934393]\n",
      "76 [D loss: 0.562186, acc.: 73.44%] [G loss: 0.921038]\n",
      "77 [D loss: 0.607244, acc.: 60.16%] [G loss: 0.908430]\n",
      "78 [D loss: 0.625208, acc.: 53.91%] [G loss: 0.924681]\n",
      "79 [D loss: 0.627085, acc.: 49.22%] [G loss: 1.017090]\n",
      "80 [D loss: 0.594894, acc.: 66.80%] [G loss: 1.154527]\n",
      "81 [D loss: 0.580711, acc.: 72.66%] [G loss: 1.206107]\n",
      "82 [D loss: 0.604011, acc.: 65.62%] [G loss: 1.251566]\n",
      "83 [D loss: 0.563635, acc.: 75.39%] [G loss: 1.220940]\n",
      "84 [D loss: 0.512944, acc.: 80.86%] [G loss: 1.301699]\n",
      "85 [D loss: 0.496303, acc.: 83.59%] [G loss: 1.244979]\n",
      "86 [D loss: 0.464537, acc.: 88.28%] [G loss: 1.199183]\n",
      "87 [D loss: 0.464870, acc.: 86.72%] [G loss: 1.151258]\n",
      "88 [D loss: 0.454797, acc.: 85.55%] [G loss: 1.118886]\n",
      "89 [D loss: 0.468987, acc.: 81.25%] [G loss: 1.116261]\n",
      "90 [D loss: 0.491105, acc.: 78.52%] [G loss: 1.155546]\n",
      "91 [D loss: 0.479335, acc.: 81.64%] [G loss: 1.255723]\n",
      "92 [D loss: 0.461772, acc.: 84.38%] [G loss: 1.316533]\n",
      "93 [D loss: 0.462372, acc.: 82.42%] [G loss: 1.359059]\n",
      "94 [D loss: 0.473991, acc.: 80.47%] [G loss: 1.332102]\n",
      "95 [D loss: 0.579656, acc.: 68.36%] [G loss: 1.301761]\n",
      "96 [D loss: 0.654889, acc.: 60.55%] [G loss: 1.486406]\n",
      "97 [D loss: 0.708581, acc.: 53.91%] [G loss: 1.528746]\n",
      "98 [D loss: 1.054364, acc.: 26.17%] [G loss: 1.238662]\n",
      "99 [D loss: 1.216960, acc.: 14.45%] [G loss: 1.108067]\n",
      "100 [D loss: 0.973231, acc.: 23.83%] [G loss: 1.470727]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "101 [D loss: 0.779503, acc.: 54.69%] [G loss: 1.806997]\n",
      "102 [D loss: 0.685918, acc.: 65.23%] [G loss: 1.808605]\n",
      "103 [D loss: 0.629579, acc.: 71.09%] [G loss: 1.674589]\n",
      "104 [D loss: 0.622875, acc.: 71.48%] [G loss: 1.482680]\n",
      "105 [D loss: 0.614390, acc.: 71.48%] [G loss: 1.442827]\n",
      "106 [D loss: 0.604953, acc.: 73.44%] [G loss: 1.395822]\n",
      "107 [D loss: 0.605175, acc.: 72.66%] [G loss: 1.340189]\n",
      "108 [D loss: 0.586914, acc.: 74.61%] [G loss: 1.264764]\n",
      "109 [D loss: 0.572167, acc.: 75.39%] [G loss: 1.154935]\n",
      "110 [D loss: 0.570698, acc.: 76.17%] [G loss: 1.145699]\n",
      "111 [D loss: 0.572578, acc.: 73.83%] [G loss: 1.149436]\n",
      "112 [D loss: 0.571465, acc.: 69.14%] [G loss: 1.150237]\n",
      "113 [D loss: 0.625572, acc.: 57.42%] [G loss: 1.070700]\n",
      "114 [D loss: 0.740440, acc.: 41.41%] [G loss: 1.076935]\n",
      "115 [D loss: 0.753004, acc.: 39.45%] [G loss: 1.209766]\n",
      "116 [D loss: 0.664022, acc.: 55.86%] [G loss: 1.417516]\n",
      "117 [D loss: 0.598630, acc.: 71.09%] [G loss: 1.539675]\n",
      "118 [D loss: 0.573772, acc.: 72.27%] [G loss: 1.591202]\n",
      "119 [D loss: 0.536454, acc.: 73.44%] [G loss: 1.624113]\n",
      "120 [D loss: 0.519650, acc.: 75.00%] [G loss: 1.513744]\n",
      "121 [D loss: 0.508907, acc.: 78.91%] [G loss: 1.406802]\n",
      "122 [D loss: 0.503658, acc.: 80.08%] [G loss: 1.228927]\n",
      "123 [D loss: 0.495334, acc.: 82.81%] [G loss: 1.121265]\n",
      "124 [D loss: 0.499801, acc.: 80.47%] [G loss: 1.045013]\n",
      "125 [D loss: 0.523092, acc.: 76.17%] [G loss: 0.937803]\n",
      "126 [D loss: 0.547576, acc.: 71.48%] [G loss: 0.844104]\n",
      "127 [D loss: 0.585305, acc.: 70.31%] [G loss: 0.830861]\n",
      "128 [D loss: 0.625940, acc.: 58.98%] [G loss: 0.758548]\n",
      "129 [D loss: 0.731295, acc.: 48.83%] [G loss: 0.744181]\n",
      "130 [D loss: 0.776238, acc.: 53.12%] [G loss: 0.808792]\n",
      "131 [D loss: 0.769107, acc.: 53.12%] [G loss: 0.970733]\n",
      "132 [D loss: 0.725154, acc.: 47.27%] [G loss: 1.121174]\n",
      "133 [D loss: 0.805821, acc.: 38.28%] [G loss: 1.214238]\n",
      "134 [D loss: 0.889933, acc.: 29.69%] [G loss: 1.258021]\n",
      "135 [D loss: 0.809652, acc.: 57.81%] [G loss: 1.242313]\n",
      "136 [D loss: 0.787395, acc.: 58.20%] [G loss: 1.204102]\n",
      "137 [D loss: 0.731243, acc.: 60.16%] [G loss: 1.192965]\n",
      "138 [D loss: 0.669955, acc.: 63.67%] [G loss: 1.199658]\n",
      "139 [D loss: 0.626454, acc.: 66.41%] [G loss: 1.155042]\n",
      "140 [D loss: 0.635736, acc.: 64.84%] [G loss: 1.118618]\n",
      "141 [D loss: 0.608337, acc.: 66.41%] [G loss: 0.997772]\n",
      "142 [D loss: 0.610713, acc.: 67.97%] [G loss: 0.963486]\n",
      "143 [D loss: 0.624771, acc.: 69.14%] [G loss: 0.926236]\n",
      "144 [D loss: 0.697576, acc.: 64.06%] [G loss: 0.850018]\n",
      "145 [D loss: 0.676945, acc.: 65.62%] [G loss: 0.799801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 [D loss: 0.681905, acc.: 60.16%] [G loss: 0.736231]\n",
      "147 [D loss: 0.720413, acc.: 55.86%] [G loss: 0.774145]\n",
      "148 [D loss: 0.645396, acc.: 57.03%] [G loss: 0.787421]\n",
      "149 [D loss: 0.650365, acc.: 53.52%] [G loss: 0.869805]\n",
      "150 [D loss: 0.640472, acc.: 57.81%] [G loss: 0.915170]\n",
      "151 [D loss: 0.662183, acc.: 48.83%] [G loss: 0.907752]\n",
      "152 [D loss: 0.661717, acc.: 51.95%] [G loss: 0.905770]\n",
      "153 [D loss: 0.693026, acc.: 44.53%] [G loss: 0.856874]\n",
      "154 [D loss: 0.728621, acc.: 37.50%] [G loss: 0.919105]\n",
      "155 [D loss: 0.689714, acc.: 46.48%] [G loss: 0.968899]\n",
      "156 [D loss: 0.656231, acc.: 61.72%] [G loss: 1.052211]\n",
      "157 [D loss: 0.643109, acc.: 60.94%] [G loss: 1.159500]\n",
      "158 [D loss: 0.636391, acc.: 60.55%] [G loss: 1.185298]\n",
      "159 [D loss: 0.629624, acc.: 60.16%] [G loss: 1.147224]\n",
      "160 [D loss: 0.633161, acc.: 62.50%] [G loss: 1.126819]\n",
      "161 [D loss: 0.641448, acc.: 60.94%] [G loss: 1.087767]\n",
      "162 [D loss: 0.654049, acc.: 57.03%] [G loss: 1.064663]\n",
      "163 [D loss: 0.652273, acc.: 57.42%] [G loss: 1.018046]\n",
      "164 [D loss: 0.658157, acc.: 57.42%] [G loss: 1.008184]\n",
      "165 [D loss: 0.658163, acc.: 56.64%] [G loss: 0.996423]\n",
      "166 [D loss: 0.664626, acc.: 55.86%] [G loss: 0.948016]\n",
      "167 [D loss: 0.671297, acc.: 52.73%] [G loss: 0.973796]\n",
      "168 [D loss: 0.659556, acc.: 59.38%] [G loss: 0.963078]\n",
      "169 [D loss: 0.658786, acc.: 59.38%] [G loss: 0.986859]\n",
      "170 [D loss: 0.647548, acc.: 62.11%] [G loss: 0.948917]\n",
      "171 [D loss: 0.633719, acc.: 65.62%] [G loss: 0.945191]\n",
      "172 [D loss: 0.623680, acc.: 68.75%] [G loss: 0.936148]\n",
      "173 [D loss: 0.618442, acc.: 67.58%] [G loss: 0.923570]\n",
      "174 [D loss: 0.604512, acc.: 70.70%] [G loss: 0.915954]\n",
      "175 [D loss: 0.596004, acc.: 69.92%] [G loss: 0.953718]\n",
      "176 [D loss: 0.579504, acc.: 73.05%] [G loss: 0.988568]\n",
      "177 [D loss: 0.576843, acc.: 72.66%] [G loss: 1.018415]\n",
      "178 [D loss: 0.600730, acc.: 68.36%] [G loss: 0.990913]\n",
      "179 [D loss: 0.625179, acc.: 66.80%] [G loss: 0.966503]\n",
      "180 [D loss: 0.632325, acc.: 62.11%] [G loss: 0.848342]\n",
      "181 [D loss: 0.708908, acc.: 54.30%] [G loss: 0.821405]\n",
      "182 [D loss: 0.734999, acc.: 49.61%] [G loss: 0.756811]\n",
      "183 [D loss: 0.745208, acc.: 42.97%] [G loss: 0.724605]\n",
      "184 [D loss: 0.748782, acc.: 39.45%] [G loss: 0.740282]\n",
      "185 [D loss: 0.735795, acc.: 41.02%] [G loss: 0.732521]\n",
      "186 [D loss: 0.736060, acc.: 42.58%] [G loss: 0.822058]\n",
      "187 [D loss: 0.679437, acc.: 58.98%] [G loss: 0.899029]\n",
      "188 [D loss: 0.664986, acc.: 57.42%] [G loss: 0.957591]\n",
      "189 [D loss: 0.684183, acc.: 50.78%] [G loss: 1.005615]\n",
      "190 [D loss: 0.659030, acc.: 53.52%] [G loss: 1.053390]\n",
      "191 [D loss: 0.675980, acc.: 56.25%] [G loss: 1.006431]\n",
      "192 [D loss: 0.688587, acc.: 56.25%] [G loss: 0.923260]\n",
      "193 [D loss: 0.682021, acc.: 60.94%] [G loss: 0.873089]\n",
      "194 [D loss: 0.684941, acc.: 56.25%] [G loss: 0.842483]\n",
      "195 [D loss: 0.677457, acc.: 65.62%] [G loss: 0.825173]\n",
      "196 [D loss: 0.677141, acc.: 65.23%] [G loss: 0.812473]\n",
      "197 [D loss: 0.675676, acc.: 61.72%] [G loss: 0.801007]\n",
      "198 [D loss: 0.671926, acc.: 60.55%] [G loss: 0.805410]\n",
      "199 [D loss: 0.667181, acc.: 67.19%] [G loss: 0.817583]\n",
      "200 [D loss: 0.662350, acc.: 60.55%] [G loss: 0.804121]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "201 [D loss: 0.673200, acc.: 54.69%] [G loss: 0.833990]\n",
      "202 [D loss: 0.699779, acc.: 55.86%] [G loss: 0.855731]\n",
      "203 [D loss: 0.660964, acc.: 58.98%] [G loss: 0.884861]\n",
      "204 [D loss: 0.655182, acc.: 64.84%] [G loss: 0.892320]\n",
      "205 [D loss: 0.655355, acc.: 64.06%] [G loss: 0.886105]\n",
      "206 [D loss: 0.643292, acc.: 66.02%] [G loss: 0.911257]\n",
      "207 [D loss: 0.630632, acc.: 70.31%] [G loss: 0.945801]\n",
      "208 [D loss: 0.628820, acc.: 72.27%] [G loss: 0.930549]\n",
      "209 [D loss: 0.626633, acc.: 72.66%] [G loss: 0.927626]\n",
      "210 [D loss: 0.639801, acc.: 68.36%] [G loss: 0.918995]\n",
      "211 [D loss: 0.650299, acc.: 67.19%] [G loss: 0.896598]\n",
      "212 [D loss: 0.658950, acc.: 64.06%] [G loss: 0.887641]\n",
      "213 [D loss: 0.676449, acc.: 58.20%] [G loss: 0.896289]\n",
      "214 [D loss: 0.672545, acc.: 57.42%] [G loss: 0.864417]\n",
      "215 [D loss: 0.685790, acc.: 57.42%] [G loss: 0.893683]\n",
      "216 [D loss: 0.676710, acc.: 57.81%] [G loss: 0.850692]\n",
      "217 [D loss: 0.681828, acc.: 59.38%] [G loss: 0.874957]\n",
      "218 [D loss: 0.662854, acc.: 62.50%] [G loss: 0.889946]\n",
      "219 [D loss: 0.613706, acc.: 69.53%] [G loss: 0.969912]\n",
      "220 [D loss: 0.583349, acc.: 69.14%] [G loss: 1.027918]\n",
      "221 [D loss: 0.565181, acc.: 71.48%] [G loss: 1.073200]\n",
      "222 [D loss: 0.591693, acc.: 66.02%] [G loss: 1.011385]\n",
      "223 [D loss: 0.608027, acc.: 66.80%] [G loss: 0.917893]\n",
      "224 [D loss: 0.627021, acc.: 64.45%] [G loss: 0.807630]\n",
      "225 [D loss: 0.665431, acc.: 57.42%] [G loss: 0.801048]\n",
      "226 [D loss: 0.664433, acc.: 52.34%] [G loss: 0.808916]\n",
      "227 [D loss: 0.662854, acc.: 48.44%] [G loss: 0.806558]\n",
      "228 [D loss: 0.663055, acc.: 54.30%] [G loss: 0.826354]\n",
      "229 [D loss: 0.688957, acc.: 53.91%] [G loss: 0.817929]\n",
      "230 [D loss: 0.690765, acc.: 53.91%] [G loss: 0.782589]\n",
      "231 [D loss: 0.696898, acc.: 50.00%] [G loss: 0.765508]\n",
      "232 [D loss: 0.734517, acc.: 51.56%] [G loss: 0.711030]\n",
      "233 [D loss: 0.732585, acc.: 45.31%] [G loss: 0.750785]\n",
      "234 [D loss: 0.699062, acc.: 50.39%] [G loss: 0.754883]\n",
      "235 [D loss: 0.709941, acc.: 45.31%] [G loss: 0.766592]\n",
      "236 [D loss: 0.703824, acc.: 48.44%] [G loss: 0.799341]\n",
      "237 [D loss: 0.679855, acc.: 57.03%] [G loss: 0.855848]\n",
      "238 [D loss: 0.680466, acc.: 57.03%] [G loss: 0.848866]\n",
      "239 [D loss: 0.678139, acc.: 60.55%] [G loss: 0.832111]\n",
      "240 [D loss: 0.667817, acc.: 60.16%] [G loss: 0.858261]\n",
      "241 [D loss: 0.657667, acc.: 62.11%] [G loss: 0.861101]\n",
      "242 [D loss: 0.663342, acc.: 59.77%] [G loss: 0.833474]\n",
      "243 [D loss: 0.659105, acc.: 62.50%] [G loss: 0.833473]\n",
      "244 [D loss: 0.655285, acc.: 63.28%] [G loss: 0.824481]\n",
      "245 [D loss: 0.651214, acc.: 62.89%] [G loss: 0.813344]\n",
      "246 [D loss: 0.639622, acc.: 62.50%] [G loss: 0.834488]\n",
      "247 [D loss: 0.637112, acc.: 63.67%] [G loss: 0.800916]\n",
      "248 [D loss: 0.637423, acc.: 67.97%] [G loss: 0.812696]\n",
      "249 [D loss: 0.651535, acc.: 63.67%] [G loss: 0.809059]\n",
      "250 [D loss: 0.647800, acc.: 65.62%] [G loss: 0.834603]\n",
      "251 [D loss: 0.628665, acc.: 67.97%] [G loss: 0.822620]\n",
      "252 [D loss: 0.640388, acc.: 61.33%] [G loss: 0.780356]\n",
      "253 [D loss: 0.681966, acc.: 53.12%] [G loss: 0.763165]\n",
      "254 [D loss: 0.687623, acc.: 52.73%] [G loss: 0.797583]\n",
      "255 [D loss: 0.709911, acc.: 53.91%] [G loss: 0.892651]\n",
      "256 [D loss: 0.663371, acc.: 57.42%] [G loss: 1.048466]\n",
      "257 [D loss: 0.623570, acc.: 65.23%] [G loss: 1.117561]\n",
      "258 [D loss: 0.623421, acc.: 66.02%] [G loss: 1.076334]\n",
      "259 [D loss: 0.645592, acc.: 61.72%] [G loss: 0.957982]\n",
      "260 [D loss: 0.656165, acc.: 60.16%] [G loss: 0.926309]\n",
      "261 [D loss: 0.663738, acc.: 55.47%] [G loss: 0.863455]\n",
      "262 [D loss: 0.673701, acc.: 57.42%] [G loss: 0.851324]\n",
      "263 [D loss: 0.692011, acc.: 53.91%] [G loss: 0.889585]\n",
      "264 [D loss: 0.659755, acc.: 66.02%] [G loss: 0.929828]\n",
      "265 [D loss: 0.668552, acc.: 61.33%] [G loss: 0.896630]\n",
      "266 [D loss: 0.665073, acc.: 59.38%] [G loss: 0.850010]\n",
      "267 [D loss: 0.691062, acc.: 47.66%] [G loss: 0.860910]\n",
      "268 [D loss: 0.690210, acc.: 48.05%] [G loss: 0.858693]\n",
      "269 [D loss: 0.689175, acc.: 49.22%] [G loss: 0.860832]\n",
      "270 [D loss: 0.689371, acc.: 49.22%] [G loss: 0.844633]\n",
      "271 [D loss: 0.680833, acc.: 51.56%] [G loss: 0.901432]\n",
      "272 [D loss: 0.664741, acc.: 59.77%] [G loss: 0.931018]\n",
      "273 [D loss: 0.633362, acc.: 62.50%] [G loss: 0.993853]\n",
      "274 [D loss: 0.623226, acc.: 61.72%] [G loss: 1.002495]\n",
      "275 [D loss: 0.611448, acc.: 67.19%] [G loss: 1.007043]\n",
      "276 [D loss: 0.616198, acc.: 67.58%] [G loss: 0.926737]\n",
      "277 [D loss: 0.635120, acc.: 64.45%] [G loss: 0.886379]\n",
      "278 [D loss: 0.663391, acc.: 65.62%] [G loss: 0.859334]\n",
      "279 [D loss: 0.674558, acc.: 63.28%] [G loss: 0.838024]\n",
      "280 [D loss: 0.667162, acc.: 62.89%] [G loss: 0.851079]\n",
      "281 [D loss: 0.654074, acc.: 65.62%] [G loss: 0.845224]\n",
      "282 [D loss: 0.686272, acc.: 58.98%] [G loss: 0.845479]\n",
      "283 [D loss: 0.669827, acc.: 57.81%] [G loss: 0.820854]\n",
      "284 [D loss: 0.716546, acc.: 52.34%] [G loss: 0.817718]\n",
      "285 [D loss: 0.672617, acc.: 57.81%] [G loss: 0.796526]\n",
      "286 [D loss: 0.670396, acc.: 55.86%] [G loss: 0.791122]\n",
      "287 [D loss: 0.705076, acc.: 49.61%] [G loss: 0.798948]\n",
      "288 [D loss: 0.695488, acc.: 48.83%] [G loss: 0.817519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289 [D loss: 0.667190, acc.: 53.52%] [G loss: 0.802709]\n",
      "290 [D loss: 0.697678, acc.: 43.75%] [G loss: 0.840475]\n",
      "291 [D loss: 0.671932, acc.: 54.30%] [G loss: 0.851512]\n",
      "292 [D loss: 0.675280, acc.: 51.56%] [G loss: 0.883612]\n",
      "293 [D loss: 0.662816, acc.: 57.03%] [G loss: 0.867588]\n",
      "294 [D loss: 0.663556, acc.: 56.64%] [G loss: 0.881581]\n",
      "295 [D loss: 0.664815, acc.: 57.42%] [G loss: 0.875925]\n",
      "296 [D loss: 0.660642, acc.: 58.98%] [G loss: 0.872280]\n",
      "297 [D loss: 0.663362, acc.: 55.86%] [G loss: 0.838673]\n",
      "298 [D loss: 0.689747, acc.: 52.73%] [G loss: 0.832205]\n",
      "299 [D loss: 0.678087, acc.: 52.73%] [G loss: 0.819881]\n",
      "300 [D loss: 0.714838, acc.: 44.92%] [G loss: 0.811127]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "301 [D loss: 0.708443, acc.: 46.09%] [G loss: 0.821831]\n",
      "302 [D loss: 0.696810, acc.: 54.30%] [G loss: 0.839392]\n",
      "303 [D loss: 0.685734, acc.: 60.16%] [G loss: 0.846957]\n",
      "304 [D loss: 0.684352, acc.: 58.98%] [G loss: 0.851352]\n",
      "305 [D loss: 0.680719, acc.: 58.59%] [G loss: 0.849613]\n",
      "306 [D loss: 0.671693, acc.: 60.55%] [G loss: 0.860185]\n",
      "307 [D loss: 0.655152, acc.: 61.33%] [G loss: 0.877923]\n",
      "308 [D loss: 0.651856, acc.: 58.59%] [G loss: 0.854278]\n",
      "309 [D loss: 0.642028, acc.: 59.38%] [G loss: 0.882965]\n",
      "310 [D loss: 0.633874, acc.: 60.94%] [G loss: 0.929175]\n",
      "311 [D loss: 0.624008, acc.: 61.33%] [G loss: 0.899726]\n",
      "312 [D loss: 0.627168, acc.: 61.33%] [G loss: 0.884424]\n",
      "313 [D loss: 0.623447, acc.: 61.33%] [G loss: 0.877927]\n",
      "314 [D loss: 0.625038, acc.: 61.72%] [G loss: 0.871404]\n",
      "315 [D loss: 0.636241, acc.: 61.72%] [G loss: 0.858247]\n",
      "316 [D loss: 0.642671, acc.: 58.59%] [G loss: 0.843733]\n",
      "317 [D loss: 0.664523, acc.: 61.33%] [G loss: 0.841135]\n",
      "318 [D loss: 0.650413, acc.: 59.38%] [G loss: 0.833800]\n",
      "319 [D loss: 0.694293, acc.: 55.47%] [G loss: 0.841639]\n",
      "320 [D loss: 0.668092, acc.: 55.08%] [G loss: 0.844841]\n",
      "321 [D loss: 0.669578, acc.: 56.64%] [G loss: 0.862284]\n",
      "322 [D loss: 0.669965, acc.: 55.86%] [G loss: 0.902695]\n",
      "323 [D loss: 0.649441, acc.: 60.55%] [G loss: 0.934804]\n",
      "324 [D loss: 0.630969, acc.: 58.59%] [G loss: 0.947623]\n",
      "325 [D loss: 0.630347, acc.: 60.94%] [G loss: 0.953814]\n",
      "326 [D loss: 0.624166, acc.: 62.50%] [G loss: 0.947167]\n",
      "327 [D loss: 0.632716, acc.: 63.28%] [G loss: 0.916644]\n",
      "328 [D loss: 0.629044, acc.: 63.28%] [G loss: 0.897490]\n",
      "329 [D loss: 0.627330, acc.: 64.06%] [G loss: 0.892214]\n",
      "330 [D loss: 0.648656, acc.: 61.72%] [G loss: 0.858749]\n",
      "331 [D loss: 0.642676, acc.: 64.06%] [G loss: 0.821968]\n",
      "332 [D loss: 0.642900, acc.: 63.28%] [G loss: 0.822885]\n",
      "333 [D loss: 0.650143, acc.: 59.38%] [G loss: 0.800070]\n",
      "334 [D loss: 0.658457, acc.: 58.20%] [G loss: 0.795368]\n",
      "335 [D loss: 0.649822, acc.: 63.28%] [G loss: 0.797551]\n",
      "336 [D loss: 0.655738, acc.: 58.59%] [G loss: 0.788790]\n",
      "337 [D loss: 0.655215, acc.: 58.59%] [G loss: 0.765476]\n",
      "338 [D loss: 0.654330, acc.: 56.64%] [G loss: 0.767579]\n",
      "339 [D loss: 0.658812, acc.: 55.08%] [G loss: 0.771216]\n",
      "340 [D loss: 0.647842, acc.: 59.77%] [G loss: 0.782494]\n",
      "341 [D loss: 0.626660, acc.: 69.14%] [G loss: 0.811932]\n",
      "342 [D loss: 0.627930, acc.: 68.75%] [G loss: 0.825783]\n",
      "343 [D loss: 0.604351, acc.: 73.05%] [G loss: 0.851767]\n",
      "344 [D loss: 0.603076, acc.: 73.44%] [G loss: 0.906197]\n",
      "345 [D loss: 0.589936, acc.: 76.17%] [G loss: 0.910762]\n",
      "346 [D loss: 0.582872, acc.: 73.83%] [G loss: 0.912611]\n",
      "347 [D loss: 0.588428, acc.: 76.17%] [G loss: 0.911226]\n",
      "348 [D loss: 0.588196, acc.: 71.88%] [G loss: 0.888825]\n",
      "349 [D loss: 0.598712, acc.: 68.75%] [G loss: 0.858467]\n",
      "350 [D loss: 0.609421, acc.: 66.02%] [G loss: 0.842050]\n",
      "351 [D loss: 0.626508, acc.: 58.98%] [G loss: 0.839988]\n",
      "352 [D loss: 0.638500, acc.: 55.86%] [G loss: 0.794847]\n",
      "353 [D loss: 0.658176, acc.: 48.83%] [G loss: 0.807522]\n",
      "354 [D loss: 0.674842, acc.: 48.83%] [G loss: 0.820408]\n",
      "355 [D loss: 0.680792, acc.: 51.17%] [G loss: 0.882057]\n",
      "356 [D loss: 0.657428, acc.: 57.81%] [G loss: 0.898597]\n",
      "357 [D loss: 0.635665, acc.: 62.89%] [G loss: 0.977482]\n",
      "358 [D loss: 0.635812, acc.: 63.67%] [G loss: 1.025306]\n",
      "359 [D loss: 0.615032, acc.: 66.41%] [G loss: 1.093566]\n",
      "360 [D loss: 0.590129, acc.: 67.58%] [G loss: 1.154499]\n",
      "361 [D loss: 0.589113, acc.: 67.19%] [G loss: 1.176079]\n",
      "362 [D loss: 0.579396, acc.: 69.92%] [G loss: 1.173825]\n",
      "363 [D loss: 0.589585, acc.: 70.31%] [G loss: 1.117184]\n",
      "364 [D loss: 0.584722, acc.: 76.56%] [G loss: 1.020175]\n",
      "365 [D loss: 0.621444, acc.: 67.58%] [G loss: 0.988769]\n",
      "366 [D loss: 0.637413, acc.: 63.28%] [G loss: 0.949967]\n",
      "367 [D loss: 0.655957, acc.: 57.42%] [G loss: 0.913873]\n",
      "368 [D loss: 0.674100, acc.: 52.73%] [G loss: 0.890540]\n",
      "369 [D loss: 0.695329, acc.: 48.44%] [G loss: 0.950834]\n",
      "370 [D loss: 0.668570, acc.: 55.08%] [G loss: 1.090626]\n",
      "371 [D loss: 0.632157, acc.: 64.06%] [G loss: 1.309500]\n",
      "372 [D loss: 0.587653, acc.: 67.19%] [G loss: 1.401119]\n",
      "373 [D loss: 0.587809, acc.: 69.53%] [G loss: 1.336111]\n",
      "374 [D loss: 0.624226, acc.: 69.53%] [G loss: 1.205741]\n",
      "375 [D loss: 0.619424, acc.: 70.31%] [G loss: 1.093901]\n",
      "376 [D loss: 0.644462, acc.: 69.53%] [G loss: 1.004330]\n",
      "377 [D loss: 0.662233, acc.: 63.28%] [G loss: 0.924256]\n",
      "378 [D loss: 0.678545, acc.: 58.20%] [G loss: 0.887827]\n",
      "379 [D loss: 0.670168, acc.: 57.03%] [G loss: 0.860651]\n",
      "380 [D loss: 0.684558, acc.: 55.86%] [G loss: 0.846750]\n",
      "381 [D loss: 0.687626, acc.: 54.69%] [G loss: 0.835715]\n",
      "382 [D loss: 0.679289, acc.: 53.52%] [G loss: 0.813305]\n",
      "383 [D loss: 0.684661, acc.: 51.17%] [G loss: 0.830521]\n",
      "384 [D loss: 0.691525, acc.: 50.00%] [G loss: 0.861678]\n",
      "385 [D loss: 0.676018, acc.: 51.95%] [G loss: 0.892320]\n",
      "386 [D loss: 0.663834, acc.: 52.34%] [G loss: 0.934034]\n",
      "387 [D loss: 0.642748, acc.: 61.72%] [G loss: 0.995407]\n",
      "388 [D loss: 0.624238, acc.: 65.23%] [G loss: 1.045001]\n",
      "389 [D loss: 0.628866, acc.: 66.02%] [G loss: 1.049186]\n",
      "390 [D loss: 0.644794, acc.: 64.84%] [G loss: 1.023210]\n",
      "391 [D loss: 0.634898, acc.: 68.36%] [G loss: 0.973930]\n",
      "392 [D loss: 0.655863, acc.: 65.62%] [G loss: 0.957349]\n",
      "393 [D loss: 0.646850, acc.: 66.80%] [G loss: 0.970591]\n",
      "394 [D loss: 0.640729, acc.: 69.53%] [G loss: 0.992070]\n",
      "395 [D loss: 0.610186, acc.: 68.75%] [G loss: 0.985526]\n",
      "396 [D loss: 0.592452, acc.: 68.36%] [G loss: 1.002258]\n",
      "397 [D loss: 0.605615, acc.: 64.84%] [G loss: 0.970050]\n",
      "398 [D loss: 0.600953, acc.: 66.41%] [G loss: 0.918801]\n",
      "399 [D loss: 0.641286, acc.: 63.28%] [G loss: 0.839654]\n",
      "400 [D loss: 0.650088, acc.: 60.94%] [G loss: 0.835567]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "401 [D loss: 0.705875, acc.: 47.66%] [G loss: 0.821104]\n",
      "402 [D loss: 0.723337, acc.: 40.23%] [G loss: 0.901463]\n",
      "403 [D loss: 0.723417, acc.: 43.75%] [G loss: 0.932982]\n",
      "404 [D loss: 0.715736, acc.: 47.27%] [G loss: 1.068782]\n",
      "405 [D loss: 0.708361, acc.: 55.08%] [G loss: 1.131356]\n",
      "406 [D loss: 0.662574, acc.: 58.59%] [G loss: 1.215398]\n",
      "407 [D loss: 0.657605, acc.: 58.59%] [G loss: 1.258556]\n",
      "408 [D loss: 0.646639, acc.: 59.38%] [G loss: 1.187047]\n",
      "409 [D loss: 0.647034, acc.: 58.98%] [G loss: 1.093605]\n",
      "410 [D loss: 0.648588, acc.: 61.72%] [G loss: 0.990182]\n",
      "411 [D loss: 0.659990, acc.: 60.55%] [G loss: 0.924335]\n",
      "412 [D loss: 0.670230, acc.: 63.67%] [G loss: 0.921215]\n",
      "413 [D loss: 0.646611, acc.: 65.23%] [G loss: 0.921537]\n",
      "414 [D loss: 0.639854, acc.: 65.23%] [G loss: 0.932379]\n",
      "415 [D loss: 0.643160, acc.: 64.84%] [G loss: 0.946754]\n",
      "416 [D loss: 0.636254, acc.: 64.06%] [G loss: 0.902313]\n",
      "417 [D loss: 0.627701, acc.: 66.02%] [G loss: 0.880251]\n",
      "418 [D loss: 0.630097, acc.: 62.11%] [G loss: 0.882956]\n",
      "419 [D loss: 0.641890, acc.: 58.59%] [G loss: 0.850914]\n",
      "420 [D loss: 0.637391, acc.: 61.72%] [G loss: 0.835774]\n",
      "421 [D loss: 0.612294, acc.: 66.80%] [G loss: 0.833245]\n",
      "422 [D loss: 0.622521, acc.: 67.19%] [G loss: 0.795878]\n",
      "423 [D loss: 0.602116, acc.: 68.36%] [G loss: 0.806549]\n",
      "424 [D loss: 0.618567, acc.: 65.23%] [G loss: 0.809096]\n",
      "425 [D loss: 0.611879, acc.: 64.84%] [G loss: 0.830977]\n",
      "426 [D loss: 0.605559, acc.: 66.02%] [G loss: 0.827447]\n",
      "427 [D loss: 0.615161, acc.: 65.62%] [G loss: 0.841427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428 [D loss: 0.613487, acc.: 63.67%] [G loss: 0.831850]\n",
      "429 [D loss: 0.620706, acc.: 60.55%] [G loss: 0.819133]\n",
      "430 [D loss: 0.630581, acc.: 59.77%] [G loss: 0.860148]\n",
      "431 [D loss: 0.622144, acc.: 64.06%] [G loss: 0.845826]\n",
      "432 [D loss: 0.641443, acc.: 60.94%] [G loss: 0.844628]\n",
      "433 [D loss: 0.634652, acc.: 59.77%] [G loss: 0.921144]\n",
      "434 [D loss: 0.615511, acc.: 61.33%] [G loss: 0.984679]\n",
      "435 [D loss: 0.618142, acc.: 60.94%] [G loss: 0.988632]\n",
      "436 [D loss: 0.647191, acc.: 52.34%] [G loss: 0.985229]\n",
      "437 [D loss: 0.614622, acc.: 61.72%] [G loss: 1.033191]\n",
      "438 [D loss: 0.600019, acc.: 69.14%] [G loss: 1.033963]\n",
      "439 [D loss: 0.599150, acc.: 69.14%] [G loss: 1.044661]\n",
      "440 [D loss: 0.601306, acc.: 67.58%] [G loss: 1.016139]\n",
      "441 [D loss: 0.596196, acc.: 71.09%] [G loss: 0.980932]\n",
      "442 [D loss: 0.599102, acc.: 71.88%] [G loss: 0.932000]\n",
      "443 [D loss: 0.607009, acc.: 72.27%] [G loss: 0.905092]\n",
      "444 [D loss: 0.627672, acc.: 65.23%] [G loss: 0.887055]\n",
      "445 [D loss: 0.627024, acc.: 66.80%] [G loss: 0.882583]\n",
      "446 [D loss: 0.638917, acc.: 66.02%] [G loss: 0.898348]\n",
      "447 [D loss: 0.622818, acc.: 65.62%] [G loss: 0.941927]\n",
      "448 [D loss: 0.646332, acc.: 62.50%] [G loss: 0.963015]\n",
      "449 [D loss: 0.626654, acc.: 64.06%] [G loss: 0.955414]\n",
      "450 [D loss: 0.657777, acc.: 58.20%] [G loss: 0.969150]\n",
      "451 [D loss: 0.645781, acc.: 58.59%] [G loss: 0.982192]\n",
      "452 [D loss: 0.639235, acc.: 58.98%] [G loss: 0.989515]\n",
      "453 [D loss: 0.626684, acc.: 64.84%] [G loss: 1.061925]\n",
      "454 [D loss: 0.621335, acc.: 67.19%] [G loss: 0.989179]\n",
      "455 [D loss: 0.615384, acc.: 67.97%] [G loss: 1.038355]\n",
      "456 [D loss: 0.604195, acc.: 68.75%] [G loss: 0.997797]\n",
      "457 [D loss: 0.621220, acc.: 66.41%] [G loss: 1.017130]\n",
      "458 [D loss: 0.619287, acc.: 67.58%] [G loss: 1.016583]\n",
      "459 [D loss: 0.603824, acc.: 73.05%] [G loss: 1.044424]\n",
      "460 [D loss: 0.606459, acc.: 73.44%] [G loss: 0.985393]\n",
      "461 [D loss: 0.598863, acc.: 76.17%] [G loss: 1.025875]\n",
      "462 [D loss: 0.573150, acc.: 79.69%] [G loss: 1.050499]\n",
      "463 [D loss: 0.556926, acc.: 78.91%] [G loss: 1.116615]\n",
      "464 [D loss: 0.531918, acc.: 80.86%] [G loss: 1.061870]\n",
      "465 [D loss: 0.524643, acc.: 84.38%] [G loss: 1.018678]\n",
      "466 [D loss: 0.532088, acc.: 83.98%] [G loss: 1.039329]\n",
      "467 [D loss: 0.532668, acc.: 84.77%] [G loss: 0.987955]\n",
      "468 [D loss: 0.526710, acc.: 84.38%] [G loss: 0.975994]\n",
      "469 [D loss: 0.543324, acc.: 82.03%] [G loss: 0.982474]\n",
      "470 [D loss: 0.559854, acc.: 78.52%] [G loss: 0.987073]\n",
      "471 [D loss: 0.555546, acc.: 75.39%] [G loss: 0.992616]\n",
      "472 [D loss: 0.577375, acc.: 71.88%] [G loss: 0.968231]\n",
      "473 [D loss: 0.610508, acc.: 67.19%] [G loss: 0.970499]\n",
      "474 [D loss: 0.656836, acc.: 56.25%] [G loss: 0.951212]\n",
      "475 [D loss: 0.672719, acc.: 58.98%] [G loss: 0.856184]\n",
      "476 [D loss: 0.804633, acc.: 44.53%] [G loss: 0.848947]\n",
      "477 [D loss: 0.840759, acc.: 37.89%] [G loss: 0.900693]\n",
      "478 [D loss: 0.756654, acc.: 40.23%] [G loss: 1.158826]\n",
      "479 [D loss: 0.656259, acc.: 61.72%] [G loss: 1.320460]\n",
      "480 [D loss: 0.639843, acc.: 58.20%] [G loss: 1.294099]\n",
      "481 [D loss: 0.624755, acc.: 60.94%] [G loss: 1.198670]\n",
      "482 [D loss: 0.613112, acc.: 63.67%] [G loss: 1.106223]\n",
      "483 [D loss: 0.617955, acc.: 64.06%] [G loss: 1.124104]\n",
      "484 [D loss: 0.620451, acc.: 63.28%] [G loss: 1.079696]\n",
      "485 [D loss: 0.615680, acc.: 63.28%] [G loss: 1.084600]\n",
      "486 [D loss: 0.601544, acc.: 66.41%] [G loss: 1.094753]\n",
      "487 [D loss: 0.618856, acc.: 65.62%] [G loss: 1.027720]\n",
      "488 [D loss: 0.618342, acc.: 64.84%] [G loss: 0.970388]\n",
      "489 [D loss: 0.626575, acc.: 62.89%] [G loss: 0.942781]\n",
      "490 [D loss: 0.629819, acc.: 64.06%] [G loss: 0.908423]\n",
      "491 [D loss: 0.645879, acc.: 62.89%] [G loss: 0.944779]\n",
      "492 [D loss: 0.638297, acc.: 62.50%] [G loss: 0.938293]\n",
      "493 [D loss: 0.637848, acc.: 61.72%] [G loss: 0.897682]\n",
      "494 [D loss: 0.674743, acc.: 62.50%] [G loss: 0.907064]\n",
      "495 [D loss: 0.617450, acc.: 66.02%] [G loss: 0.922679]\n",
      "496 [D loss: 0.645683, acc.: 60.16%] [G loss: 0.930259]\n",
      "497 [D loss: 0.629099, acc.: 65.62%] [G loss: 0.947054]\n",
      "498 [D loss: 0.639845, acc.: 64.45%] [G loss: 0.939482]\n",
      "499 [D loss: 0.617220, acc.: 66.02%] [G loss: 0.979087]\n",
      "500 [D loss: 0.611508, acc.: 67.97%] [G loss: 1.058270]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "501 [D loss: 0.586445, acc.: 71.48%] [G loss: 1.080525]\n",
      "502 [D loss: 0.575461, acc.: 73.05%] [G loss: 1.097798]\n",
      "503 [D loss: 0.566918, acc.: 73.05%] [G loss: 1.115249]\n",
      "504 [D loss: 0.567097, acc.: 76.17%] [G loss: 1.077848]\n",
      "505 [D loss: 0.575988, acc.: 73.83%] [G loss: 1.049348]\n",
      "506 [D loss: 0.595157, acc.: 72.27%] [G loss: 0.945061]\n",
      "507 [D loss: 0.632503, acc.: 71.09%] [G loss: 0.952201]\n",
      "508 [D loss: 0.625328, acc.: 68.36%] [G loss: 0.960126]\n",
      "509 [D loss: 0.616041, acc.: 71.48%] [G loss: 0.982484]\n",
      "510 [D loss: 0.599136, acc.: 71.48%] [G loss: 1.020472]\n",
      "511 [D loss: 0.595709, acc.: 70.70%] [G loss: 1.010425]\n",
      "512 [D loss: 0.590676, acc.: 70.31%] [G loss: 0.997943]\n",
      "513 [D loss: 0.572689, acc.: 69.92%] [G loss: 1.019204]\n",
      "514 [D loss: 0.568392, acc.: 69.53%] [G loss: 1.021362]\n",
      "515 [D loss: 0.548785, acc.: 73.44%] [G loss: 1.004070]\n",
      "516 [D loss: 0.560280, acc.: 73.05%] [G loss: 0.943070]\n",
      "517 [D loss: 0.581156, acc.: 68.36%] [G loss: 0.897589]\n",
      "518 [D loss: 0.586131, acc.: 68.75%] [G loss: 0.879745]\n",
      "519 [D loss: 0.584936, acc.: 69.14%] [G loss: 0.874377]\n",
      "520 [D loss: 0.592324, acc.: 68.36%] [G loss: 0.861388]\n",
      "521 [D loss: 0.600979, acc.: 67.58%] [G loss: 0.836293]\n",
      "522 [D loss: 0.625383, acc.: 63.28%] [G loss: 0.827881]\n",
      "523 [D loss: 0.631983, acc.: 61.33%] [G loss: 0.830390]\n",
      "524 [D loss: 0.626466, acc.: 59.38%] [G loss: 0.898549]\n",
      "525 [D loss: 0.611728, acc.: 64.45%] [G loss: 1.027193]\n",
      "526 [D loss: 0.578278, acc.: 68.36%] [G loss: 1.032901]\n",
      "527 [D loss: 0.613399, acc.: 64.06%] [G loss: 1.025968]\n",
      "528 [D loss: 0.608404, acc.: 64.84%] [G loss: 0.969409]\n",
      "529 [D loss: 0.648980, acc.: 60.94%] [G loss: 1.036565]\n",
      "530 [D loss: 0.627719, acc.: 65.23%] [G loss: 0.975294]\n",
      "531 [D loss: 0.631592, acc.: 64.84%] [G loss: 0.945267]\n",
      "532 [D loss: 0.639424, acc.: 61.72%] [G loss: 0.872261]\n",
      "533 [D loss: 0.636739, acc.: 64.45%] [G loss: 0.902564]\n",
      "534 [D loss: 0.656158, acc.: 59.77%] [G loss: 0.905390]\n",
      "535 [D loss: 0.621139, acc.: 66.02%] [G loss: 0.906803]\n",
      "536 [D loss: 0.633398, acc.: 63.28%] [G loss: 0.917729]\n",
      "537 [D loss: 0.618454, acc.: 67.58%] [G loss: 0.959488]\n",
      "538 [D loss: 0.591628, acc.: 72.27%] [G loss: 0.951023]\n",
      "539 [D loss: 0.597714, acc.: 71.09%] [G loss: 0.954369]\n",
      "540 [D loss: 0.600463, acc.: 69.53%] [G loss: 0.959780]\n",
      "541 [D loss: 0.571843, acc.: 76.17%] [G loss: 0.987497]\n",
      "542 [D loss: 0.579350, acc.: 73.05%] [G loss: 1.011611]\n",
      "543 [D loss: 0.561033, acc.: 80.08%] [G loss: 0.972136]\n",
      "544 [D loss: 0.541235, acc.: 78.91%] [G loss: 0.942369]\n",
      "545 [D loss: 0.548258, acc.: 82.81%] [G loss: 0.936748]\n",
      "546 [D loss: 0.535650, acc.: 78.52%] [G loss: 0.965983]\n",
      "547 [D loss: 0.561171, acc.: 71.09%] [G loss: 0.965832]\n",
      "548 [D loss: 0.563096, acc.: 73.83%] [G loss: 0.978339]\n",
      "549 [D loss: 0.592376, acc.: 66.80%] [G loss: 0.966021]\n",
      "550 [D loss: 0.664896, acc.: 51.17%] [G loss: 1.011909]\n",
      "551 [D loss: 0.692086, acc.: 50.00%] [G loss: 1.050677]\n",
      "552 [D loss: 0.674684, acc.: 56.25%] [G loss: 1.195378]\n",
      "553 [D loss: 0.601698, acc.: 68.36%] [G loss: 1.329609]\n",
      "554 [D loss: 0.579025, acc.: 71.48%] [G loss: 1.381580]\n",
      "555 [D loss: 0.578768, acc.: 70.31%] [G loss: 1.336161]\n",
      "556 [D loss: 0.591402, acc.: 71.09%] [G loss: 1.125845]\n",
      "557 [D loss: 0.584297, acc.: 72.27%] [G loss: 1.090839]\n",
      "558 [D loss: 0.589090, acc.: 73.83%] [G loss: 1.041878]\n",
      "559 [D loss: 0.587934, acc.: 69.53%] [G loss: 1.018706]\n",
      "560 [D loss: 0.593798, acc.: 70.31%] [G loss: 0.986180]\n",
      "561 [D loss: 0.599376, acc.: 68.75%] [G loss: 0.938021]\n",
      "562 [D loss: 0.637347, acc.: 62.11%] [G loss: 0.884847]\n",
      "563 [D loss: 0.646566, acc.: 62.89%] [G loss: 0.899397]\n",
      "564 [D loss: 0.670354, acc.: 57.42%] [G loss: 0.906349]\n",
      "565 [D loss: 0.658690, acc.: 58.20%] [G loss: 0.920737]\n",
      "566 [D loss: 0.653506, acc.: 60.94%] [G loss: 0.955086]\n",
      "567 [D loss: 0.629401, acc.: 66.80%] [G loss: 0.991912]\n",
      "568 [D loss: 0.627157, acc.: 64.45%] [G loss: 1.015595]\n",
      "569 [D loss: 0.617182, acc.: 64.45%] [G loss: 1.016577]\n",
      "570 [D loss: 0.615720, acc.: 64.84%] [G loss: 1.003448]\n",
      "571 [D loss: 0.598998, acc.: 67.19%] [G loss: 0.960322]\n",
      "572 [D loss: 0.609349, acc.: 65.23%] [G loss: 0.977840]\n",
      "573 [D loss: 0.603680, acc.: 65.62%] [G loss: 0.963038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574 [D loss: 0.596894, acc.: 67.97%] [G loss: 0.952581]\n",
      "575 [D loss: 0.582806, acc.: 69.14%] [G loss: 0.910140]\n",
      "576 [D loss: 0.596624, acc.: 67.58%] [G loss: 0.965074]\n",
      "577 [D loss: 0.604439, acc.: 63.67%] [G loss: 0.924071]\n",
      "578 [D loss: 0.599586, acc.: 66.02%] [G loss: 0.956706]\n",
      "579 [D loss: 0.597957, acc.: 66.02%] [G loss: 0.960554]\n",
      "580 [D loss: 0.596942, acc.: 67.97%] [G loss: 0.939427]\n",
      "581 [D loss: 0.583465, acc.: 68.36%] [G loss: 0.945986]\n",
      "582 [D loss: 0.569204, acc.: 73.05%] [G loss: 0.931690]\n",
      "583 [D loss: 0.564999, acc.: 71.88%] [G loss: 0.928771]\n",
      "584 [D loss: 0.590318, acc.: 67.19%] [G loss: 0.933531]\n",
      "585 [D loss: 0.566235, acc.: 71.88%] [G loss: 0.866518]\n",
      "586 [D loss: 0.587486, acc.: 66.02%] [G loss: 0.873195]\n",
      "587 [D loss: 0.584312, acc.: 68.36%] [G loss: 0.841492]\n",
      "588 [D loss: 0.600143, acc.: 65.23%] [G loss: 0.847597]\n",
      "589 [D loss: 0.608262, acc.: 62.89%] [G loss: 0.832336]\n",
      "590 [D loss: 0.619824, acc.: 62.11%] [G loss: 0.874543]\n",
      "591 [D loss: 0.588963, acc.: 64.84%] [G loss: 0.900492]\n",
      "592 [D loss: 0.596983, acc.: 68.75%] [G loss: 0.958913]\n",
      "593 [D loss: 0.580382, acc.: 71.48%] [G loss: 0.950968]\n",
      "594 [D loss: 0.593228, acc.: 67.19%] [G loss: 0.952464]\n",
      "595 [D loss: 0.597167, acc.: 67.19%] [G loss: 0.971799]\n",
      "596 [D loss: 0.604897, acc.: 66.80%] [G loss: 0.970314]\n",
      "597 [D loss: 0.604407, acc.: 64.84%] [G loss: 0.924344]\n",
      "598 [D loss: 0.618047, acc.: 60.55%] [G loss: 0.956933]\n",
      "599 [D loss: 0.631005, acc.: 60.16%] [G loss: 1.009283]\n",
      "600 [D loss: 0.615996, acc.: 61.33%] [G loss: 1.049340]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "601 [D loss: 0.616830, acc.: 63.28%] [G loss: 1.079829]\n",
      "602 [D loss: 0.624160, acc.: 59.38%] [G loss: 1.112191]\n",
      "603 [D loss: 0.602248, acc.: 63.28%] [G loss: 1.088101]\n",
      "604 [D loss: 0.602435, acc.: 63.67%] [G loss: 1.048769]\n",
      "605 [D loss: 0.612740, acc.: 64.45%] [G loss: 0.981858]\n",
      "606 [D loss: 0.594862, acc.: 67.97%] [G loss: 0.978436]\n",
      "607 [D loss: 0.596380, acc.: 65.23%] [G loss: 1.000053]\n",
      "608 [D loss: 0.574915, acc.: 71.88%] [G loss: 0.992548]\n",
      "609 [D loss: 0.579363, acc.: 70.70%] [G loss: 0.996080]\n",
      "610 [D loss: 0.581335, acc.: 69.14%] [G loss: 0.954715]\n",
      "611 [D loss: 0.592600, acc.: 68.36%] [G loss: 0.996417]\n",
      "612 [D loss: 0.580158, acc.: 67.97%] [G loss: 0.948149]\n",
      "613 [D loss: 0.578860, acc.: 69.14%] [G loss: 0.949210]\n",
      "614 [D loss: 0.578124, acc.: 67.97%] [G loss: 0.905577]\n",
      "615 [D loss: 0.589412, acc.: 64.84%] [G loss: 0.908241]\n",
      "616 [D loss: 0.599107, acc.: 65.23%] [G loss: 0.909827]\n",
      "617 [D loss: 0.588510, acc.: 67.58%] [G loss: 0.896548]\n",
      "618 [D loss: 0.609834, acc.: 62.11%] [G loss: 0.859431]\n",
      "619 [D loss: 0.610790, acc.: 62.50%] [G loss: 0.846313]\n",
      "620 [D loss: 0.629650, acc.: 58.20%] [G loss: 0.863101]\n",
      "621 [D loss: 0.594641, acc.: 67.19%] [G loss: 0.857042]\n",
      "622 [D loss: 0.606932, acc.: 62.50%] [G loss: 0.863890]\n",
      "623 [D loss: 0.596734, acc.: 65.62%] [G loss: 0.875933]\n",
      "624 [D loss: 0.608553, acc.: 64.45%] [G loss: 0.883316]\n",
      "625 [D loss: 0.607604, acc.: 62.11%] [G loss: 0.892944]\n",
      "626 [D loss: 0.607098, acc.: 63.67%] [G loss: 0.860619]\n",
      "627 [D loss: 0.621864, acc.: 59.77%] [G loss: 0.892165]\n",
      "628 [D loss: 0.603077, acc.: 62.89%] [G loss: 0.905603]\n",
      "629 [D loss: 0.595829, acc.: 64.45%] [G loss: 0.934840]\n",
      "630 [D loss: 0.607465, acc.: 64.06%] [G loss: 0.920649]\n",
      "631 [D loss: 0.602831, acc.: 65.62%] [G loss: 0.959723]\n",
      "632 [D loss: 0.589746, acc.: 66.41%] [G loss: 0.941816]\n",
      "633 [D loss: 0.597659, acc.: 65.23%] [G loss: 0.949065]\n",
      "634 [D loss: 0.604499, acc.: 67.58%] [G loss: 0.928988]\n",
      "635 [D loss: 0.619470, acc.: 61.33%] [G loss: 0.950290]\n",
      "636 [D loss: 0.605189, acc.: 63.67%] [G loss: 0.987833]\n",
      "637 [D loss: 0.577674, acc.: 68.75%] [G loss: 1.061697]\n",
      "638 [D loss: 0.599320, acc.: 68.75%] [G loss: 1.028141]\n",
      "639 [D loss: 0.586101, acc.: 68.75%] [G loss: 1.060463]\n",
      "640 [D loss: 0.602451, acc.: 65.62%] [G loss: 1.014480]\n",
      "641 [D loss: 0.604714, acc.: 65.23%] [G loss: 0.987397]\n",
      "642 [D loss: 0.615654, acc.: 65.23%] [G loss: 0.979672]\n",
      "643 [D loss: 0.602795, acc.: 66.41%] [G loss: 0.990911]\n",
      "644 [D loss: 0.562572, acc.: 72.27%] [G loss: 1.003487]\n",
      "645 [D loss: 0.576269, acc.: 67.58%] [G loss: 1.029976]\n",
      "646 [D loss: 0.557456, acc.: 71.88%] [G loss: 1.006365]\n",
      "647 [D loss: 0.553546, acc.: 75.78%] [G loss: 0.988763]\n",
      "648 [D loss: 0.581875, acc.: 70.31%] [G loss: 0.963801]\n",
      "649 [D loss: 0.632282, acc.: 58.98%] [G loss: 0.994514]\n",
      "650 [D loss: 0.681231, acc.: 50.39%] [G loss: 0.983873]\n",
      "651 [D loss: 0.728793, acc.: 46.09%] [G loss: 1.013688]\n",
      "652 [D loss: 0.690234, acc.: 56.25%] [G loss: 1.145351]\n",
      "653 [D loss: 0.654735, acc.: 58.98%] [G loss: 1.217585]\n",
      "654 [D loss: 0.648227, acc.: 57.81%] [G loss: 1.113460]\n",
      "655 [D loss: 0.642022, acc.: 60.16%] [G loss: 1.064030]\n",
      "656 [D loss: 0.635332, acc.: 58.20%] [G loss: 1.051398]\n",
      "657 [D loss: 0.624232, acc.: 59.77%] [G loss: 0.982238]\n",
      "658 [D loss: 0.612583, acc.: 64.45%] [G loss: 0.978617]\n",
      "659 [D loss: 0.610651, acc.: 64.06%] [G loss: 0.964597]\n",
      "660 [D loss: 0.598645, acc.: 65.62%] [G loss: 0.893085]\n",
      "661 [D loss: 0.605295, acc.: 64.45%] [G loss: 0.915419]\n",
      "662 [D loss: 0.593897, acc.: 68.36%] [G loss: 0.908225]\n",
      "663 [D loss: 0.586323, acc.: 71.48%] [G loss: 0.935725]\n",
      "664 [D loss: 0.593549, acc.: 73.44%] [G loss: 0.924050]\n",
      "665 [D loss: 0.575256, acc.: 73.44%] [G loss: 0.889045]\n",
      "666 [D loss: 0.585187, acc.: 73.83%] [G loss: 0.875937]\n",
      "667 [D loss: 0.567376, acc.: 75.78%] [G loss: 0.859951]\n",
      "668 [D loss: 0.578493, acc.: 72.27%] [G loss: 0.840888]\n",
      "669 [D loss: 0.583741, acc.: 70.31%] [G loss: 0.862081]\n",
      "670 [D loss: 0.581449, acc.: 71.48%] [G loss: 0.894853]\n",
      "671 [D loss: 0.598756, acc.: 69.92%] [G loss: 0.876700]\n",
      "672 [D loss: 0.582626, acc.: 70.31%] [G loss: 0.917717]\n",
      "673 [D loss: 0.585967, acc.: 68.36%] [G loss: 0.892824]\n",
      "674 [D loss: 0.602016, acc.: 66.41%] [G loss: 0.866526]\n",
      "675 [D loss: 0.600797, acc.: 69.92%] [G loss: 0.862955]\n",
      "676 [D loss: 0.573455, acc.: 73.44%] [G loss: 0.872671]\n",
      "677 [D loss: 0.589210, acc.: 69.53%] [G loss: 0.905349]\n",
      "678 [D loss: 0.581451, acc.: 69.92%] [G loss: 0.925946]\n",
      "679 [D loss: 0.567997, acc.: 71.48%] [G loss: 0.963592]\n",
      "680 [D loss: 0.577743, acc.: 69.53%] [G loss: 0.977264]\n",
      "681 [D loss: 0.557248, acc.: 72.66%] [G loss: 0.978086]\n",
      "682 [D loss: 0.558698, acc.: 73.05%] [G loss: 1.002700]\n",
      "683 [D loss: 0.562207, acc.: 70.70%] [G loss: 1.000086]\n",
      "684 [D loss: 0.558186, acc.: 73.44%] [G loss: 1.019933]\n",
      "685 [D loss: 0.576762, acc.: 70.70%] [G loss: 0.954298]\n",
      "686 [D loss: 0.600425, acc.: 71.09%] [G loss: 0.950595]\n",
      "687 [D loss: 0.611050, acc.: 66.02%] [G loss: 0.958355]\n",
      "688 [D loss: 0.592968, acc.: 67.97%] [G loss: 0.991047]\n",
      "689 [D loss: 0.573485, acc.: 69.92%] [G loss: 1.029442]\n",
      "690 [D loss: 0.575972, acc.: 66.80%] [G loss: 1.061652]\n",
      "691 [D loss: 0.537169, acc.: 70.31%] [G loss: 1.184712]\n",
      "692 [D loss: 0.520084, acc.: 71.88%] [G loss: 1.247874]\n",
      "693 [D loss: 0.521277, acc.: 72.27%] [G loss: 1.206226]\n",
      "694 [D loss: 0.505449, acc.: 75.00%] [G loss: 1.142646]\n",
      "695 [D loss: 0.569406, acc.: 67.19%] [G loss: 0.993423]\n",
      "696 [D loss: 0.587367, acc.: 69.53%] [G loss: 0.907007]\n",
      "697 [D loss: 0.628083, acc.: 66.02%] [G loss: 0.960409]\n",
      "698 [D loss: 0.575232, acc.: 70.70%] [G loss: 0.978442]\n",
      "699 [D loss: 0.568801, acc.: 72.27%] [G loss: 1.032633]\n",
      "700 [D loss: 0.551909, acc.: 73.44%] [G loss: 1.086283]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "701 [D loss: 0.538057, acc.: 73.44%] [G loss: 1.150117]\n",
      "702 [D loss: 0.545857, acc.: 71.88%] [G loss: 1.157051]\n",
      "703 [D loss: 0.552258, acc.: 72.66%] [G loss: 1.154791]\n",
      "704 [D loss: 0.555739, acc.: 70.70%] [G loss: 1.114777]\n",
      "705 [D loss: 0.573442, acc.: 69.14%] [G loss: 1.043870]\n",
      "706 [D loss: 0.612360, acc.: 67.19%] [G loss: 0.964347]\n",
      "707 [D loss: 0.635644, acc.: 64.06%] [G loss: 0.983454]\n",
      "708 [D loss: 0.639860, acc.: 63.67%] [G loss: 1.003820]\n",
      "709 [D loss: 0.623854, acc.: 67.58%] [G loss: 1.035013]\n",
      "710 [D loss: 0.611787, acc.: 66.41%] [G loss: 1.009434]\n",
      "711 [D loss: 0.609845, acc.: 64.45%] [G loss: 1.015176]\n",
      "712 [D loss: 0.582882, acc.: 70.70%] [G loss: 1.054565]\n",
      "713 [D loss: 0.551474, acc.: 71.88%] [G loss: 1.139755]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714 [D loss: 0.529812, acc.: 75.00%] [G loss: 1.135465]\n",
      "715 [D loss: 0.549690, acc.: 71.48%] [G loss: 1.114131]\n",
      "716 [D loss: 0.550406, acc.: 70.70%] [G loss: 1.059482]\n",
      "717 [D loss: 0.569239, acc.: 69.92%] [G loss: 1.000421]\n",
      "718 [D loss: 0.563761, acc.: 70.70%] [G loss: 1.024512]\n",
      "719 [D loss: 0.539402, acc.: 73.05%] [G loss: 0.982400]\n",
      "720 [D loss: 0.558133, acc.: 67.97%] [G loss: 0.973030]\n",
      "721 [D loss: 0.551074, acc.: 72.27%] [G loss: 0.956675]\n",
      "722 [D loss: 0.584487, acc.: 69.14%] [G loss: 0.901361]\n",
      "723 [D loss: 0.571214, acc.: 69.14%] [G loss: 0.894146]\n",
      "724 [D loss: 0.577086, acc.: 68.75%] [G loss: 0.897281]\n",
      "725 [D loss: 0.603148, acc.: 64.06%] [G loss: 0.893730]\n",
      "726 [D loss: 0.599543, acc.: 67.19%] [G loss: 0.941667]\n",
      "727 [D loss: 0.607289, acc.: 65.23%] [G loss: 1.013586]\n",
      "728 [D loss: 0.605344, acc.: 69.92%] [G loss: 1.033275]\n",
      "729 [D loss: 0.562944, acc.: 73.44%] [G loss: 1.122651]\n",
      "730 [D loss: 0.558360, acc.: 71.88%] [G loss: 1.179467]\n",
      "731 [D loss: 0.524509, acc.: 75.78%] [G loss: 1.089509]\n",
      "732 [D loss: 0.622090, acc.: 74.22%] [G loss: 1.075533]\n",
      "733 [D loss: 0.605723, acc.: 73.44%] [G loss: 1.022655]\n",
      "734 [D loss: 0.625843, acc.: 69.92%] [G loss: 1.130788]\n",
      "735 [D loss: 0.593293, acc.: 68.36%] [G loss: 1.203021]\n",
      "736 [D loss: 0.569146, acc.: 72.66%] [G loss: 1.136300]\n",
      "737 [D loss: 0.561003, acc.: 71.88%] [G loss: 1.163843]\n",
      "738 [D loss: 0.555148, acc.: 75.00%] [G loss: 1.213165]\n",
      "739 [D loss: 0.537507, acc.: 74.22%] [G loss: 1.135909]\n",
      "740 [D loss: 0.546119, acc.: 76.17%] [G loss: 1.121503]\n",
      "741 [D loss: 0.535906, acc.: 74.61%] [G loss: 1.076831]\n",
      "742 [D loss: 0.541357, acc.: 75.39%] [G loss: 1.037288]\n",
      "743 [D loss: 0.548631, acc.: 72.27%] [G loss: 1.052218]\n",
      "744 [D loss: 0.552531, acc.: 72.27%] [G loss: 1.057438]\n",
      "745 [D loss: 0.531099, acc.: 74.22%] [G loss: 1.063842]\n",
      "746 [D loss: 0.548105, acc.: 72.27%] [G loss: 1.014555]\n",
      "747 [D loss: 0.526407, acc.: 76.17%] [G loss: 1.036198]\n",
      "748 [D loss: 0.537450, acc.: 73.83%] [G loss: 1.120771]\n",
      "749 [D loss: 0.511818, acc.: 76.95%] [G loss: 1.136223]\n",
      "750 [D loss: 0.509276, acc.: 77.34%] [G loss: 1.097539]\n",
      "751 [D loss: 0.503735, acc.: 78.52%] [G loss: 1.122565]\n",
      "752 [D loss: 0.504382, acc.: 77.34%] [G loss: 1.161786]\n",
      "753 [D loss: 0.535048, acc.: 75.78%] [G loss: 1.136427]\n",
      "754 [D loss: 0.561877, acc.: 71.88%] [G loss: 1.235901]\n",
      "755 [D loss: 0.533884, acc.: 74.22%] [G loss: 1.186550]\n",
      "756 [D loss: 0.551398, acc.: 73.83%] [G loss: 1.148333]\n",
      "757 [D loss: 0.555048, acc.: 71.88%] [G loss: 1.068661]\n",
      "758 [D loss: 0.552953, acc.: 72.27%] [G loss: 1.056705]\n",
      "759 [D loss: 0.573389, acc.: 71.88%] [G loss: 1.071413]\n",
      "760 [D loss: 0.621940, acc.: 66.41%] [G loss: 1.116672]\n",
      "761 [D loss: 0.587908, acc.: 68.36%] [G loss: 1.120555]\n",
      "762 [D loss: 0.547060, acc.: 73.44%] [G loss: 1.051441]\n",
      "763 [D loss: 0.576957, acc.: 69.14%] [G loss: 1.023194]\n",
      "764 [D loss: 0.552758, acc.: 70.70%] [G loss: 0.995055]\n",
      "765 [D loss: 0.548969, acc.: 71.88%] [G loss: 0.976929]\n",
      "766 [D loss: 0.567382, acc.: 68.36%] [G loss: 0.965087]\n",
      "767 [D loss: 0.564966, acc.: 69.92%] [G loss: 1.003084]\n",
      "768 [D loss: 0.575742, acc.: 70.31%] [G loss: 1.001819]\n",
      "769 [D loss: 0.591264, acc.: 66.02%] [G loss: 0.938044]\n",
      "770 [D loss: 0.600734, acc.: 66.02%] [G loss: 0.988521]\n",
      "771 [D loss: 0.595097, acc.: 67.58%] [G loss: 0.999867]\n",
      "772 [D loss: 0.588430, acc.: 67.97%] [G loss: 0.969303]\n",
      "773 [D loss: 0.590929, acc.: 67.97%] [G loss: 0.999798]\n",
      "774 [D loss: 0.582550, acc.: 66.80%] [G loss: 0.999805]\n",
      "775 [D loss: 0.595448, acc.: 67.58%] [G loss: 1.016381]\n",
      "776 [D loss: 0.610158, acc.: 66.02%] [G loss: 0.991655]\n",
      "777 [D loss: 0.603832, acc.: 67.97%] [G loss: 0.995734]\n",
      "778 [D loss: 0.571880, acc.: 69.14%] [G loss: 1.034435]\n",
      "779 [D loss: 0.573113, acc.: 69.92%] [G loss: 1.003471]\n",
      "780 [D loss: 0.575423, acc.: 71.09%] [G loss: 1.008773]\n",
      "781 [D loss: 0.568603, acc.: 71.48%] [G loss: 1.006100]\n",
      "782 [D loss: 0.579155, acc.: 69.14%] [G loss: 1.074859]\n",
      "783 [D loss: 0.569688, acc.: 66.80%] [G loss: 1.018668]\n",
      "784 [D loss: 0.590955, acc.: 69.92%] [G loss: 1.024854]\n",
      "785 [D loss: 0.640650, acc.: 60.94%] [G loss: 1.034927]\n",
      "786 [D loss: 0.629182, acc.: 62.89%] [G loss: 0.999467]\n",
      "787 [D loss: 0.597983, acc.: 67.58%] [G loss: 0.999568]\n",
      "788 [D loss: 0.646200, acc.: 61.33%] [G loss: 1.048039]\n",
      "789 [D loss: 0.611702, acc.: 64.84%] [G loss: 1.178609]\n",
      "790 [D loss: 0.620802, acc.: 63.28%] [G loss: 1.090098]\n",
      "791 [D loss: 0.628466, acc.: 62.89%] [G loss: 1.035924]\n",
      "792 [D loss: 0.604440, acc.: 70.31%] [G loss: 1.057461]\n",
      "793 [D loss: 0.618598, acc.: 64.84%] [G loss: 1.044565]\n",
      "794 [D loss: 0.594941, acc.: 71.09%] [G loss: 1.045146]\n",
      "795 [D loss: 0.560985, acc.: 75.00%] [G loss: 1.072406]\n",
      "796 [D loss: 0.573092, acc.: 75.39%] [G loss: 1.104150]\n",
      "797 [D loss: 0.532964, acc.: 75.78%] [G loss: 1.095641]\n",
      "798 [D loss: 0.545541, acc.: 75.00%] [G loss: 1.036286]\n",
      "799 [D loss: 0.565861, acc.: 71.09%] [G loss: 1.018189]\n",
      "800 [D loss: 0.573506, acc.: 71.88%] [G loss: 0.963124]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "801 [D loss: 0.593573, acc.: 66.02%] [G loss: 0.974214]\n",
      "802 [D loss: 0.597725, acc.: 69.92%] [G loss: 1.035815]\n",
      "803 [D loss: 0.607126, acc.: 67.58%] [G loss: 1.013690]\n",
      "804 [D loss: 0.611543, acc.: 66.02%] [G loss: 1.006797]\n",
      "805 [D loss: 0.591822, acc.: 71.09%] [G loss: 1.003243]\n",
      "806 [D loss: 0.591199, acc.: 69.53%] [G loss: 1.064094]\n",
      "807 [D loss: 0.598090, acc.: 69.53%] [G loss: 1.082455]\n",
      "808 [D loss: 0.546685, acc.: 72.27%] [G loss: 1.197285]\n",
      "809 [D loss: 0.529286, acc.: 71.88%] [G loss: 1.293137]\n",
      "810 [D loss: 0.529259, acc.: 71.88%] [G loss: 1.279400]\n",
      "811 [D loss: 0.537858, acc.: 71.48%] [G loss: 1.213769]\n",
      "812 [D loss: 0.543110, acc.: 73.83%] [G loss: 1.121310]\n",
      "813 [D loss: 0.555336, acc.: 71.88%] [G loss: 1.147025]\n",
      "814 [D loss: 0.565829, acc.: 71.09%] [G loss: 1.211390]\n",
      "815 [D loss: 0.513116, acc.: 75.78%] [G loss: 1.129108]\n",
      "816 [D loss: 0.531542, acc.: 73.05%] [G loss: 1.061962]\n",
      "817 [D loss: 0.575774, acc.: 71.09%] [G loss: 1.044358]\n",
      "818 [D loss: 0.529387, acc.: 77.73%] [G loss: 1.093829]\n",
      "819 [D loss: 0.582930, acc.: 68.75%] [G loss: 1.006756]\n",
      "820 [D loss: 0.631927, acc.: 65.23%] [G loss: 0.897766]\n",
      "821 [D loss: 0.600526, acc.: 67.19%] [G loss: 0.912107]\n",
      "822 [D loss: 0.614702, acc.: 64.45%] [G loss: 0.916747]\n",
      "823 [D loss: 0.617630, acc.: 69.14%] [G loss: 0.957693]\n",
      "824 [D loss: 0.628407, acc.: 62.89%] [G loss: 1.022475]\n",
      "825 [D loss: 0.603250, acc.: 67.19%] [G loss: 1.053033]\n",
      "826 [D loss: 0.588269, acc.: 71.48%] [G loss: 1.038109]\n",
      "827 [D loss: 0.562746, acc.: 71.48%] [G loss: 1.094332]\n",
      "828 [D loss: 0.563278, acc.: 72.27%] [G loss: 1.080509]\n",
      "829 [D loss: 0.559821, acc.: 71.09%] [G loss: 1.046377]\n",
      "830 [D loss: 0.554831, acc.: 74.22%] [G loss: 1.022040]\n",
      "831 [D loss: 0.548526, acc.: 74.61%] [G loss: 1.015363]\n",
      "832 [D loss: 0.575210, acc.: 71.09%] [G loss: 1.035121]\n",
      "833 [D loss: 0.610033, acc.: 68.36%] [G loss: 1.029112]\n",
      "834 [D loss: 0.561588, acc.: 71.88%] [G loss: 1.035224]\n",
      "835 [D loss: 0.546838, acc.: 73.05%] [G loss: 1.002458]\n",
      "836 [D loss: 0.574920, acc.: 69.92%] [G loss: 1.015672]\n",
      "837 [D loss: 0.560494, acc.: 72.66%] [G loss: 1.066508]\n",
      "838 [D loss: 0.538791, acc.: 74.61%] [G loss: 1.127307]\n",
      "839 [D loss: 0.554137, acc.: 71.09%] [G loss: 1.100230]\n",
      "840 [D loss: 0.554384, acc.: 70.70%] [G loss: 1.087854]\n",
      "841 [D loss: 0.532420, acc.: 77.73%] [G loss: 1.032502]\n",
      "842 [D loss: 0.566391, acc.: 71.88%] [G loss: 1.071656]\n",
      "843 [D loss: 0.565021, acc.: 71.48%] [G loss: 1.019818]\n",
      "844 [D loss: 0.565891, acc.: 72.66%] [G loss: 1.003132]\n",
      "845 [D loss: 0.554596, acc.: 73.05%] [G loss: 0.946069]\n",
      "846 [D loss: 0.582694, acc.: 67.58%] [G loss: 1.001567]\n",
      "847 [D loss: 0.564405, acc.: 71.48%] [G loss: 0.977867]\n",
      "848 [D loss: 0.567023, acc.: 72.27%] [G loss: 0.977580]\n",
      "849 [D loss: 0.584442, acc.: 67.19%] [G loss: 0.987447]\n",
      "850 [D loss: 0.557377, acc.: 73.83%] [G loss: 1.035662]\n",
      "851 [D loss: 0.551639, acc.: 75.00%] [G loss: 1.044423]\n",
      "852 [D loss: 0.537920, acc.: 76.17%] [G loss: 1.087635]\n",
      "853 [D loss: 0.532172, acc.: 75.78%] [G loss: 1.052061]\n",
      "854 [D loss: 0.539433, acc.: 74.22%] [G loss: 1.062095]\n",
      "855 [D loss: 0.516276, acc.: 76.95%] [G loss: 1.090516]\n",
      "856 [D loss: 0.542605, acc.: 73.44%] [G loss: 1.069954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "857 [D loss: 0.541484, acc.: 73.83%] [G loss: 1.033122]\n",
      "858 [D loss: 0.533450, acc.: 73.05%] [G loss: 1.045555]\n",
      "859 [D loss: 0.551627, acc.: 70.31%] [G loss: 1.051266]\n",
      "860 [D loss: 0.556791, acc.: 70.70%] [G loss: 0.999440]\n",
      "861 [D loss: 0.598274, acc.: 68.75%] [G loss: 1.050696]\n",
      "862 [D loss: 0.598174, acc.: 66.41%] [G loss: 1.056866]\n",
      "863 [D loss: 0.593463, acc.: 66.41%] [G loss: 1.087111]\n",
      "864 [D loss: 0.615763, acc.: 65.23%] [G loss: 1.152236]\n",
      "865 [D loss: 0.570517, acc.: 71.09%] [G loss: 1.245868]\n",
      "866 [D loss: 0.548360, acc.: 71.09%] [G loss: 1.230679]\n",
      "867 [D loss: 0.536145, acc.: 72.27%] [G loss: 1.244323]\n",
      "868 [D loss: 0.556352, acc.: 69.92%] [G loss: 1.257440]\n",
      "869 [D loss: 0.548313, acc.: 72.27%] [G loss: 1.186143]\n",
      "870 [D loss: 0.557210, acc.: 72.66%] [G loss: 1.077580]\n",
      "871 [D loss: 0.556709, acc.: 69.53%] [G loss: 1.070722]\n",
      "872 [D loss: 0.596523, acc.: 68.75%] [G loss: 1.048264]\n",
      "873 [D loss: 0.566195, acc.: 69.92%] [G loss: 1.073147]\n",
      "874 [D loss: 0.596812, acc.: 67.58%] [G loss: 1.113386]\n",
      "875 [D loss: 0.557488, acc.: 71.88%] [G loss: 1.074785]\n",
      "876 [D loss: 0.568247, acc.: 68.75%] [G loss: 1.049693]\n",
      "877 [D loss: 0.562724, acc.: 71.48%] [G loss: 1.014360]\n",
      "878 [D loss: 0.558198, acc.: 72.27%] [G loss: 1.024684]\n",
      "879 [D loss: 0.550586, acc.: 71.88%] [G loss: 1.021536]\n",
      "880 [D loss: 0.560434, acc.: 71.88%] [G loss: 1.042024]\n",
      "881 [D loss: 0.534661, acc.: 76.56%] [G loss: 1.074213]\n",
      "882 [D loss: 0.554616, acc.: 71.48%] [G loss: 1.084285]\n",
      "883 [D loss: 0.542020, acc.: 70.31%] [G loss: 1.101461]\n",
      "884 [D loss: 0.569039, acc.: 70.70%] [G loss: 1.150010]\n",
      "885 [D loss: 0.569786, acc.: 69.53%] [G loss: 1.145337]\n",
      "886 [D loss: 0.568578, acc.: 69.92%] [G loss: 1.157595]\n",
      "887 [D loss: 0.562575, acc.: 69.53%] [G loss: 1.151205]\n",
      "888 [D loss: 0.558884, acc.: 71.48%] [G loss: 1.134105]\n",
      "889 [D loss: 0.563652, acc.: 69.53%] [G loss: 1.167894]\n",
      "890 [D loss: 0.548506, acc.: 72.66%] [G loss: 1.139299]\n",
      "891 [D loss: 0.556278, acc.: 71.48%] [G loss: 1.193992]\n",
      "892 [D loss: 0.544996, acc.: 74.61%] [G loss: 1.131625]\n",
      "893 [D loss: 0.569770, acc.: 70.31%] [G loss: 1.139503]\n",
      "894 [D loss: 0.544124, acc.: 73.05%] [G loss: 1.188018]\n",
      "895 [D loss: 0.571206, acc.: 70.70%] [G loss: 1.188122]\n",
      "896 [D loss: 0.541489, acc.: 72.27%] [G loss: 1.205078]\n",
      "897 [D loss: 0.540357, acc.: 72.66%] [G loss: 1.211073]\n",
      "898 [D loss: 0.524049, acc.: 73.83%] [G loss: 1.186887]\n",
      "899 [D loss: 0.516750, acc.: 75.78%] [G loss: 1.211338]\n",
      "900 [D loss: 0.512709, acc.: 79.30%] [G loss: 1.118451]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "901 [D loss: 0.515198, acc.: 80.47%] [G loss: 1.046601]\n",
      "902 [D loss: 0.527658, acc.: 76.56%] [G loss: 1.013390]\n",
      "903 [D loss: 0.562337, acc.: 73.05%] [G loss: 1.065624]\n",
      "904 [D loss: 0.542489, acc.: 75.39%] [G loss: 1.050197]\n",
      "905 [D loss: 0.589603, acc.: 68.36%] [G loss: 1.062380]\n",
      "906 [D loss: 0.586794, acc.: 70.70%] [G loss: 1.032058]\n",
      "907 [D loss: 0.590169, acc.: 67.58%] [G loss: 1.075235]\n",
      "908 [D loss: 0.575268, acc.: 69.14%] [G loss: 1.196188]\n",
      "909 [D loss: 0.557661, acc.: 69.92%] [G loss: 1.193721]\n",
      "910 [D loss: 0.565145, acc.: 71.48%] [G loss: 1.189266]\n",
      "911 [D loss: 0.556853, acc.: 73.44%] [G loss: 1.147674]\n",
      "912 [D loss: 0.566733, acc.: 70.70%] [G loss: 1.144302]\n",
      "913 [D loss: 0.525688, acc.: 75.00%] [G loss: 1.140777]\n",
      "914 [D loss: 0.554958, acc.: 70.31%] [G loss: 1.146364]\n",
      "915 [D loss: 0.535929, acc.: 75.78%] [G loss: 1.086184]\n",
      "916 [D loss: 0.581565, acc.: 67.19%] [G loss: 1.122025]\n",
      "917 [D loss: 0.560307, acc.: 70.70%] [G loss: 1.163191]\n",
      "918 [D loss: 0.551112, acc.: 71.48%] [G loss: 1.161080]\n",
      "919 [D loss: 0.527601, acc.: 74.22%] [G loss: 1.193523]\n",
      "920 [D loss: 0.539830, acc.: 73.44%] [G loss: 1.234389]\n",
      "921 [D loss: 0.525695, acc.: 74.61%] [G loss: 1.218490]\n",
      "922 [D loss: 0.524832, acc.: 75.39%] [G loss: 1.155052]\n",
      "923 [D loss: 0.547771, acc.: 73.83%] [G loss: 1.071441]\n",
      "924 [D loss: 0.547738, acc.: 73.05%] [G loss: 1.139893]\n",
      "925 [D loss: 0.526099, acc.: 75.39%] [G loss: 1.152324]\n",
      "926 [D loss: 0.525712, acc.: 75.39%] [G loss: 1.120469]\n",
      "927 [D loss: 0.556431, acc.: 76.17%] [G loss: 1.144818]\n",
      "928 [D loss: 0.518029, acc.: 78.52%] [G loss: 1.113152]\n",
      "929 [D loss: 0.530348, acc.: 76.95%] [G loss: 1.071682]\n",
      "930 [D loss: 0.511160, acc.: 77.34%] [G loss: 1.067011]\n",
      "931 [D loss: 0.513959, acc.: 79.30%] [G loss: 1.127905]\n",
      "932 [D loss: 0.495631, acc.: 80.47%] [G loss: 1.114454]\n",
      "933 [D loss: 0.511284, acc.: 79.30%] [G loss: 1.177215]\n",
      "934 [D loss: 0.514406, acc.: 75.00%] [G loss: 1.157182]\n",
      "935 [D loss: 0.501995, acc.: 77.73%] [G loss: 1.232228]\n",
      "936 [D loss: 0.526087, acc.: 75.00%] [G loss: 1.160512]\n",
      "937 [D loss: 0.532797, acc.: 73.83%] [G loss: 1.214279]\n",
      "938 [D loss: 0.527445, acc.: 72.66%] [G loss: 1.229457]\n",
      "939 [D loss: 0.523343, acc.: 73.44%] [G loss: 1.192115]\n",
      "940 [D loss: 0.518331, acc.: 75.00%] [G loss: 1.218382]\n",
      "941 [D loss: 0.531396, acc.: 74.22%] [G loss: 1.227046]\n",
      "942 [D loss: 0.505494, acc.: 76.17%] [G loss: 1.266531]\n",
      "943 [D loss: 0.531721, acc.: 74.61%] [G loss: 1.238044]\n",
      "944 [D loss: 0.519569, acc.: 74.61%] [G loss: 1.222534]\n",
      "945 [D loss: 0.551218, acc.: 67.97%] [G loss: 1.193960]\n",
      "946 [D loss: 0.570851, acc.: 71.48%] [G loss: 1.108489]\n",
      "947 [D loss: 0.567359, acc.: 67.97%] [G loss: 1.159504]\n",
      "948 [D loss: 0.530286, acc.: 72.66%] [G loss: 1.177648]\n",
      "949 [D loss: 0.532565, acc.: 75.39%] [G loss: 1.217067]\n",
      "950 [D loss: 0.503063, acc.: 77.34%] [G loss: 1.288983]\n",
      "951 [D loss: 0.486492, acc.: 81.25%] [G loss: 1.219487]\n",
      "952 [D loss: 0.479195, acc.: 81.64%] [G loss: 1.149882]\n",
      "953 [D loss: 0.496686, acc.: 78.91%] [G loss: 1.174515]\n",
      "954 [D loss: 0.509764, acc.: 75.78%] [G loss: 1.172285]\n",
      "955 [D loss: 0.547475, acc.: 74.22%] [G loss: 1.158831]\n",
      "956 [D loss: 0.540316, acc.: 72.66%] [G loss: 1.165064]\n",
      "957 [D loss: 0.547630, acc.: 69.14%] [G loss: 1.188916]\n",
      "958 [D loss: 0.551125, acc.: 71.48%] [G loss: 1.383782]\n",
      "959 [D loss: 0.495029, acc.: 75.78%] [G loss: 1.440585]\n",
      "960 [D loss: 0.503102, acc.: 75.78%] [G loss: 1.396542]\n",
      "961 [D loss: 0.527835, acc.: 75.78%] [G loss: 1.365858]\n",
      "962 [D loss: 0.535962, acc.: 73.83%] [G loss: 1.384263]\n",
      "963 [D loss: 0.512095, acc.: 76.95%] [G loss: 1.392373]\n",
      "964 [D loss: 0.547982, acc.: 73.44%] [G loss: 1.472572]\n",
      "965 [D loss: 0.496878, acc.: 75.00%] [G loss: 1.435402]\n",
      "966 [D loss: 0.524134, acc.: 75.00%] [G loss: 1.401506]\n",
      "967 [D loss: 0.541945, acc.: 71.09%] [G loss: 1.271709]\n",
      "968 [D loss: 0.536574, acc.: 73.44%] [G loss: 1.263095]\n",
      "969 [D loss: 0.529347, acc.: 74.22%] [G loss: 1.246974]\n",
      "970 [D loss: 0.535444, acc.: 70.31%] [G loss: 1.239263]\n",
      "971 [D loss: 0.529523, acc.: 74.61%] [G loss: 1.290239]\n",
      "972 [D loss: 0.533993, acc.: 73.83%] [G loss: 1.390758]\n",
      "973 [D loss: 0.521246, acc.: 75.39%] [G loss: 1.276078]\n",
      "974 [D loss: 0.539697, acc.: 75.39%] [G loss: 1.205764]\n",
      "975 [D loss: 0.516348, acc.: 76.56%] [G loss: 1.227916]\n",
      "976 [D loss: 0.508179, acc.: 78.12%] [G loss: 1.215624]\n",
      "977 [D loss: 0.506257, acc.: 78.12%] [G loss: 1.253148]\n",
      "978 [D loss: 0.517174, acc.: 77.34%] [G loss: 1.222734]\n",
      "979 [D loss: 0.528967, acc.: 73.83%] [G loss: 1.252572]\n",
      "980 [D loss: 0.520121, acc.: 74.61%] [G loss: 1.166860]\n",
      "981 [D loss: 0.528639, acc.: 73.44%] [G loss: 1.158246]\n",
      "982 [D loss: 0.550018, acc.: 73.44%] [G loss: 1.165953]\n",
      "983 [D loss: 0.516395, acc.: 75.00%] [G loss: 1.206141]\n",
      "984 [D loss: 0.514400, acc.: 76.95%] [G loss: 1.185183]\n",
      "985 [D loss: 0.526336, acc.: 74.22%] [G loss: 1.300756]\n",
      "986 [D loss: 0.518029, acc.: 73.83%] [G loss: 1.383907]\n",
      "987 [D loss: 0.489843, acc.: 78.91%] [G loss: 1.250526]\n",
      "988 [D loss: 0.518445, acc.: 75.39%] [G loss: 1.281408]\n",
      "989 [D loss: 0.475888, acc.: 78.52%] [G loss: 1.188627]\n",
      "990 [D loss: 0.478570, acc.: 77.73%] [G loss: 1.355469]\n",
      "991 [D loss: 0.462650, acc.: 79.69%] [G loss: 1.313094]\n",
      "992 [D loss: 0.460097, acc.: 79.30%] [G loss: 1.305933]\n",
      "993 [D loss: 0.464479, acc.: 79.30%] [G loss: 1.341177]\n",
      "994 [D loss: 0.473938, acc.: 77.73%] [G loss: 1.318356]\n",
      "995 [D loss: 0.515576, acc.: 74.61%] [G loss: 1.302516]\n",
      "996 [D loss: 0.513150, acc.: 75.00%] [G loss: 1.392412]\n",
      "997 [D loss: 0.491911, acc.: 76.17%] [G loss: 1.353127]\n",
      "998 [D loss: 0.495834, acc.: 78.12%] [G loss: 1.321244]\n",
      "999 [D loss: 0.516779, acc.: 75.39%] [G loss: 1.424414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 [D loss: 0.482805, acc.: 77.34%] [G loss: 1.338090]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "1001 [D loss: 0.490969, acc.: 76.56%] [G loss: 1.334483]\n",
      "1002 [D loss: 0.470641, acc.: 79.30%] [G loss: 1.235641]\n",
      "1003 [D loss: 0.499583, acc.: 78.52%] [G loss: 1.263925]\n",
      "1004 [D loss: 0.484280, acc.: 79.30%] [G loss: 1.241538]\n",
      "1005 [D loss: 0.477598, acc.: 82.42%] [G loss: 1.197287]\n",
      "1006 [D loss: 0.478500, acc.: 80.47%] [G loss: 1.188207]\n",
      "1007 [D loss: 0.503286, acc.: 76.17%] [G loss: 1.148946]\n",
      "1008 [D loss: 0.484323, acc.: 78.12%] [G loss: 1.182968]\n",
      "1009 [D loss: 0.519202, acc.: 76.95%] [G loss: 1.255980]\n",
      "1010 [D loss: 0.462563, acc.: 80.86%] [G loss: 1.157442]\n",
      "1011 [D loss: 0.514276, acc.: 75.00%] [G loss: 1.211310]\n",
      "1012 [D loss: 0.497738, acc.: 77.73%] [G loss: 1.268565]\n",
      "1013 [D loss: 0.456740, acc.: 81.64%] [G loss: 1.228434]\n",
      "1014 [D loss: 0.430437, acc.: 83.98%] [G loss: 1.204036]\n",
      "1015 [D loss: 0.459203, acc.: 79.69%] [G loss: 1.175235]\n",
      "1016 [D loss: 0.461892, acc.: 80.47%] [G loss: 1.217363]\n",
      "1017 [D loss: 0.449915, acc.: 80.47%] [G loss: 1.164881]\n",
      "1018 [D loss: 0.466031, acc.: 77.34%] [G loss: 1.138032]\n",
      "1019 [D loss: 0.471886, acc.: 79.30%] [G loss: 1.270656]\n",
      "1020 [D loss: 0.477423, acc.: 78.52%] [G loss: 1.284746]\n",
      "1021 [D loss: 0.519600, acc.: 76.17%] [G loss: 1.379662]\n",
      "1022 [D loss: 0.495039, acc.: 76.17%] [G loss: 1.329099]\n",
      "1023 [D loss: 0.498612, acc.: 76.95%] [G loss: 1.326738]\n",
      "1024 [D loss: 0.482837, acc.: 76.95%] [G loss: 1.369868]\n",
      "1025 [D loss: 0.479365, acc.: 77.73%] [G loss: 1.392803]\n",
      "1026 [D loss: 0.465050, acc.: 79.30%] [G loss: 1.393340]\n",
      "1027 [D loss: 0.458246, acc.: 79.69%] [G loss: 1.315457]\n",
      "1028 [D loss: 0.477756, acc.: 77.73%] [G loss: 1.390722]\n",
      "1029 [D loss: 0.448767, acc.: 80.86%] [G loss: 1.389532]\n",
      "1030 [D loss: 0.469121, acc.: 78.52%] [G loss: 1.342681]\n",
      "1031 [D loss: 0.448517, acc.: 81.64%] [G loss: 1.373018]\n",
      "1032 [D loss: 0.446929, acc.: 84.77%] [G loss: 1.423475]\n",
      "1033 [D loss: 0.431806, acc.: 85.16%] [G loss: 1.416588]\n",
      "1034 [D loss: 0.476850, acc.: 77.34%] [G loss: 1.346000]\n",
      "1035 [D loss: 0.439257, acc.: 81.64%] [G loss: 1.386614]\n",
      "1036 [D loss: 0.484347, acc.: 76.95%] [G loss: 1.397654]\n",
      "1037 [D loss: 0.449749, acc.: 80.47%] [G loss: 1.412330]\n",
      "1038 [D loss: 0.458080, acc.: 79.30%] [G loss: 1.380593]\n",
      "1039 [D loss: 0.486937, acc.: 76.95%] [G loss: 1.427491]\n",
      "1040 [D loss: 0.494937, acc.: 76.17%] [G loss: 1.388429]\n",
      "1041 [D loss: 0.511991, acc.: 76.17%] [G loss: 1.411019]\n",
      "1042 [D loss: 0.481246, acc.: 76.17%] [G loss: 1.420847]\n",
      "1043 [D loss: 0.461099, acc.: 77.73%] [G loss: 1.555859]\n",
      "1044 [D loss: 0.458497, acc.: 80.86%] [G loss: 1.600225]\n",
      "1045 [D loss: 0.447337, acc.: 79.30%] [G loss: 1.405221]\n",
      "1046 [D loss: 0.469331, acc.: 79.30%] [G loss: 1.425359]\n",
      "1047 [D loss: 0.489742, acc.: 77.73%] [G loss: 1.346514]\n",
      "1048 [D loss: 0.474232, acc.: 77.73%] [G loss: 1.437612]\n",
      "1049 [D loss: 0.479505, acc.: 75.78%] [G loss: 1.472073]\n",
      "1050 [D loss: 0.482609, acc.: 77.34%] [G loss: 1.566601]\n",
      "1051 [D loss: 0.468064, acc.: 78.52%] [G loss: 1.484896]\n",
      "1052 [D loss: 0.486264, acc.: 75.00%] [G loss: 1.572978]\n",
      "1053 [D loss: 0.416831, acc.: 81.64%] [G loss: 1.655997]\n",
      "1054 [D loss: 0.408240, acc.: 82.81%] [G loss: 1.648066]\n",
      "1055 [D loss: 0.407266, acc.: 82.81%] [G loss: 1.794057]\n",
      "1056 [D loss: 0.473767, acc.: 77.73%] [G loss: 1.693609]\n",
      "1057 [D loss: 0.457836, acc.: 77.73%] [G loss: 1.556370]\n",
      "1058 [D loss: 0.485418, acc.: 75.78%] [G loss: 1.422286]\n",
      "1059 [D loss: 0.495273, acc.: 76.95%] [G loss: 1.384951]\n",
      "1060 [D loss: 0.482597, acc.: 78.91%] [G loss: 1.394076]\n",
      "1061 [D loss: 0.468564, acc.: 80.86%] [G loss: 1.483540]\n",
      "1062 [D loss: 0.491476, acc.: 78.12%] [G loss: 1.444912]\n",
      "1063 [D loss: 0.490437, acc.: 78.12%] [G loss: 1.342364]\n",
      "1064 [D loss: 0.479677, acc.: 81.25%] [G loss: 1.325536]\n",
      "1065 [D loss: 0.529188, acc.: 75.00%] [G loss: 1.380470]\n",
      "1066 [D loss: 0.481623, acc.: 76.56%] [G loss: 1.560575]\n",
      "1067 [D loss: 0.452980, acc.: 79.69%] [G loss: 1.599625]\n",
      "1068 [D loss: 0.440275, acc.: 81.25%] [G loss: 1.565377]\n",
      "1069 [D loss: 0.394484, acc.: 85.16%] [G loss: 1.521217]\n",
      "1070 [D loss: 0.419207, acc.: 83.98%] [G loss: 1.412287]\n",
      "1071 [D loss: 0.443003, acc.: 82.81%] [G loss: 1.427923]\n",
      "1072 [D loss: 0.453058, acc.: 78.12%] [G loss: 1.413352]\n",
      "1073 [D loss: 0.496640, acc.: 78.12%] [G loss: 1.402373]\n",
      "1074 [D loss: 0.513440, acc.: 74.22%] [G loss: 1.442531]\n",
      "1075 [D loss: 0.463181, acc.: 80.08%] [G loss: 1.483160]\n",
      "1076 [D loss: 0.457208, acc.: 78.12%] [G loss: 1.411970]\n",
      "1077 [D loss: 0.497201, acc.: 75.00%] [G loss: 1.449964]\n",
      "1078 [D loss: 0.484964, acc.: 78.12%] [G loss: 1.569850]\n",
      "1079 [D loss: 0.436679, acc.: 79.69%] [G loss: 1.467168]\n",
      "1080 [D loss: 0.448522, acc.: 81.64%] [G loss: 1.482496]\n",
      "1081 [D loss: 0.408490, acc.: 84.38%] [G loss: 1.586716]\n",
      "1082 [D loss: 0.441694, acc.: 82.03%] [G loss: 1.506123]\n",
      "1083 [D loss: 0.426501, acc.: 82.42%] [G loss: 1.486798]\n",
      "1084 [D loss: 0.427099, acc.: 81.25%] [G loss: 1.471291]\n",
      "1085 [D loss: 0.464547, acc.: 77.73%] [G loss: 1.470996]\n",
      "1086 [D loss: 0.448367, acc.: 79.69%] [G loss: 1.451604]\n",
      "1087 [D loss: 0.492653, acc.: 75.78%] [G loss: 1.532462]\n",
      "1088 [D loss: 0.483707, acc.: 75.78%] [G loss: 1.464485]\n",
      "1089 [D loss: 0.417009, acc.: 82.42%] [G loss: 1.423920]\n",
      "1090 [D loss: 0.470354, acc.: 75.39%] [G loss: 1.481606]\n",
      "1091 [D loss: 0.432230, acc.: 82.03%] [G loss: 1.540244]\n",
      "1092 [D loss: 0.458824, acc.: 75.39%] [G loss: 1.655940]\n",
      "1093 [D loss: 0.409224, acc.: 82.42%] [G loss: 1.556402]\n",
      "1094 [D loss: 0.426860, acc.: 82.81%] [G loss: 1.551049]\n",
      "1095 [D loss: 0.447681, acc.: 80.47%] [G loss: 1.584384]\n",
      "1096 [D loss: 0.416621, acc.: 82.42%] [G loss: 1.568431]\n",
      "1097 [D loss: 0.461201, acc.: 78.52%] [G loss: 1.552989]\n",
      "1098 [D loss: 0.451898, acc.: 80.47%] [G loss: 1.571674]\n",
      "1099 [D loss: 0.431843, acc.: 82.81%] [G loss: 1.673303]\n",
      "1100 [D loss: 0.455534, acc.: 79.30%] [G loss: 1.634554]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "1101 [D loss: 0.486928, acc.: 78.91%] [G loss: 1.508624]\n",
      "1102 [D loss: 0.494605, acc.: 78.52%] [G loss: 1.524654]\n",
      "1103 [D loss: 0.461597, acc.: 82.03%] [G loss: 1.569415]\n",
      "1104 [D loss: 0.456360, acc.: 82.81%] [G loss: 1.449007]\n",
      "1105 [D loss: 0.412503, acc.: 82.81%] [G loss: 1.605841]\n",
      "1106 [D loss: 0.390217, acc.: 85.16%] [G loss: 1.558338]\n",
      "1107 [D loss: 0.386353, acc.: 85.94%] [G loss: 1.551027]\n",
      "1108 [D loss: 0.414340, acc.: 83.20%] [G loss: 1.499594]\n",
      "1109 [D loss: 0.430693, acc.: 82.81%] [G loss: 1.434256]\n",
      "1110 [D loss: 0.406068, acc.: 82.03%] [G loss: 1.513302]\n",
      "1111 [D loss: 0.402303, acc.: 84.38%] [G loss: 1.531899]\n",
      "1112 [D loss: 0.387933, acc.: 86.33%] [G loss: 1.388132]\n",
      "1113 [D loss: 0.427064, acc.: 83.20%] [G loss: 1.541012]\n",
      "1114 [D loss: 0.436311, acc.: 78.52%] [G loss: 1.534595]\n",
      "1115 [D loss: 0.442141, acc.: 78.52%] [G loss: 1.638612]\n",
      "1116 [D loss: 0.471862, acc.: 76.95%] [G loss: 1.553838]\n",
      "1117 [D loss: 0.424611, acc.: 79.30%] [G loss: 1.449804]\n",
      "1118 [D loss: 0.450696, acc.: 82.03%] [G loss: 1.586530]\n",
      "1119 [D loss: 0.457985, acc.: 78.12%] [G loss: 1.662743]\n",
      "1120 [D loss: 0.418891, acc.: 82.42%] [G loss: 1.772225]\n",
      "1121 [D loss: 0.394073, acc.: 83.59%] [G loss: 1.763463]\n",
      "1122 [D loss: 0.396533, acc.: 83.98%] [G loss: 1.703133]\n",
      "1123 [D loss: 0.384993, acc.: 86.33%] [G loss: 1.668886]\n",
      "1124 [D loss: 0.398856, acc.: 84.77%] [G loss: 1.688757]\n",
      "1125 [D loss: 0.416430, acc.: 85.16%] [G loss: 1.712868]\n",
      "1126 [D loss: 0.409032, acc.: 84.38%] [G loss: 1.733960]\n",
      "1127 [D loss: 0.397285, acc.: 83.20%] [G loss: 1.605793]\n",
      "1128 [D loss: 0.404853, acc.: 83.20%] [G loss: 1.650690]\n",
      "1129 [D loss: 0.435028, acc.: 80.08%] [G loss: 1.569417]\n",
      "1130 [D loss: 0.419864, acc.: 82.81%] [G loss: 1.563159]\n",
      "1131 [D loss: 0.439985, acc.: 82.03%] [G loss: 1.569153]\n",
      "1132 [D loss: 0.409107, acc.: 85.16%] [G loss: 1.625542]\n",
      "1133 [D loss: 0.376707, acc.: 85.94%] [G loss: 1.679755]\n",
      "1134 [D loss: 0.362616, acc.: 85.94%] [G loss: 1.692594]\n",
      "1135 [D loss: 0.357373, acc.: 88.28%] [G loss: 1.628640]\n",
      "1136 [D loss: 0.401737, acc.: 82.42%] [G loss: 1.667252]\n",
      "1137 [D loss: 0.376651, acc.: 85.94%] [G loss: 1.603878]\n",
      "1138 [D loss: 0.473187, acc.: 76.95%] [G loss: 1.508502]\n",
      "1139 [D loss: 0.390577, acc.: 83.20%] [G loss: 1.557338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140 [D loss: 0.420391, acc.: 82.81%] [G loss: 1.457427]\n",
      "1141 [D loss: 0.449380, acc.: 81.25%] [G loss: 1.584301]\n",
      "1142 [D loss: 0.474640, acc.: 77.73%] [G loss: 1.712664]\n",
      "1143 [D loss: 0.507560, acc.: 74.61%] [G loss: 1.780980]\n",
      "1144 [D loss: 0.449919, acc.: 80.86%] [G loss: 1.804760]\n",
      "1145 [D loss: 0.487237, acc.: 76.56%] [G loss: 1.722534]\n",
      "1146 [D loss: 0.436677, acc.: 80.47%] [G loss: 1.632796]\n",
      "1147 [D loss: 0.398616, acc.: 82.81%] [G loss: 1.671404]\n",
      "1148 [D loss: 0.403872, acc.: 82.03%] [G loss: 1.649684]\n",
      "1149 [D loss: 0.399701, acc.: 82.42%] [G loss: 1.643800]\n",
      "1150 [D loss: 0.403419, acc.: 84.77%] [G loss: 1.564126]\n",
      "1151 [D loss: 0.412175, acc.: 82.81%] [G loss: 1.532639]\n",
      "1152 [D loss: 0.406675, acc.: 84.38%] [G loss: 1.687715]\n",
      "1153 [D loss: 0.388524, acc.: 83.59%] [G loss: 1.660149]\n",
      "1154 [D loss: 0.381158, acc.: 82.42%] [G loss: 1.727215]\n",
      "1155 [D loss: 0.375127, acc.: 85.16%] [G loss: 1.897412]\n",
      "1156 [D loss: 0.388188, acc.: 82.81%] [G loss: 1.818895]\n",
      "1157 [D loss: 0.440545, acc.: 79.30%] [G loss: 1.732328]\n",
      "1158 [D loss: 0.401777, acc.: 80.47%] [G loss: 1.692168]\n",
      "1159 [D loss: 0.453983, acc.: 80.08%] [G loss: 1.498678]\n",
      "1160 [D loss: 0.447442, acc.: 79.69%] [G loss: 1.572106]\n",
      "1161 [D loss: 0.397595, acc.: 82.03%] [G loss: 1.628739]\n",
      "1162 [D loss: 0.402755, acc.: 84.38%] [G loss: 1.664654]\n",
      "1163 [D loss: 0.396966, acc.: 83.20%] [G loss: 1.671423]\n",
      "1164 [D loss: 0.422607, acc.: 79.69%] [G loss: 1.667871]\n",
      "1165 [D loss: 0.417011, acc.: 80.47%] [G loss: 1.564560]\n",
      "1166 [D loss: 0.429603, acc.: 79.69%] [G loss: 1.466796]\n",
      "1167 [D loss: 0.413535, acc.: 80.47%] [G loss: 1.457454]\n",
      "1168 [D loss: 0.402822, acc.: 81.25%] [G loss: 1.611422]\n",
      "1169 [D loss: 0.439550, acc.: 78.91%] [G loss: 1.598738]\n",
      "1170 [D loss: 0.446110, acc.: 79.30%] [G loss: 1.710226]\n",
      "1171 [D loss: 0.431688, acc.: 78.52%] [G loss: 1.598309]\n",
      "1172 [D loss: 0.414404, acc.: 83.20%] [G loss: 1.589306]\n",
      "1173 [D loss: 0.421305, acc.: 78.52%] [G loss: 1.549229]\n",
      "1174 [D loss: 0.400403, acc.: 82.03%] [G loss: 1.610667]\n",
      "1175 [D loss: 0.430350, acc.: 82.42%] [G loss: 1.660407]\n",
      "1176 [D loss: 0.441811, acc.: 78.91%] [G loss: 1.635069]\n",
      "1177 [D loss: 0.416120, acc.: 80.47%] [G loss: 1.603869]\n",
      "1178 [D loss: 0.391264, acc.: 83.20%] [G loss: 1.741322]\n",
      "1179 [D loss: 0.368697, acc.: 84.77%] [G loss: 1.765334]\n",
      "1180 [D loss: 0.391976, acc.: 80.86%] [G loss: 1.653292]\n",
      "1181 [D loss: 0.402900, acc.: 80.86%] [G loss: 1.619169]\n",
      "1182 [D loss: 0.402850, acc.: 81.25%] [G loss: 1.540109]\n",
      "1183 [D loss: 0.419981, acc.: 82.03%] [G loss: 1.647923]\n",
      "1184 [D loss: 0.409566, acc.: 80.86%] [G loss: 1.656714]\n",
      "1185 [D loss: 0.423277, acc.: 81.25%] [G loss: 1.518273]\n",
      "1186 [D loss: 0.384230, acc.: 83.20%] [G loss: 1.624649]\n",
      "1187 [D loss: 0.375947, acc.: 83.59%] [G loss: 1.614068]\n",
      "1188 [D loss: 0.379081, acc.: 82.42%] [G loss: 1.670067]\n",
      "1189 [D loss: 0.420882, acc.: 80.47%] [G loss: 1.750826]\n",
      "1190 [D loss: 0.380510, acc.: 83.20%] [G loss: 1.683481]\n",
      "1191 [D loss: 0.404304, acc.: 83.20%] [G loss: 1.507810]\n",
      "1192 [D loss: 0.404104, acc.: 81.64%] [G loss: 1.559002]\n",
      "1193 [D loss: 0.396996, acc.: 84.38%] [G loss: 1.552282]\n",
      "1194 [D loss: 0.405488, acc.: 82.81%] [G loss: 1.540413]\n",
      "1195 [D loss: 0.422146, acc.: 81.25%] [G loss: 1.679125]\n",
      "1196 [D loss: 0.384129, acc.: 83.20%] [G loss: 1.634166]\n",
      "1197 [D loss: 0.395122, acc.: 80.47%] [G loss: 1.658664]\n",
      "1198 [D loss: 0.405470, acc.: 82.81%] [G loss: 1.648715]\n",
      "1199 [D loss: 0.416422, acc.: 80.47%] [G loss: 1.694316]\n",
      "1200 [D loss: 0.445184, acc.: 77.73%] [G loss: 1.698838]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "1201 [D loss: 0.364209, acc.: 85.55%] [G loss: 1.645419]\n",
      "1202 [D loss: 0.420223, acc.: 77.73%] [G loss: 1.619543]\n",
      "1203 [D loss: 0.443429, acc.: 77.34%] [G loss: 1.723463]\n",
      "1204 [D loss: 0.436835, acc.: 77.34%] [G loss: 1.805868]\n",
      "1205 [D loss: 0.391442, acc.: 81.25%] [G loss: 1.747674]\n",
      "1206 [D loss: 0.429192, acc.: 79.30%] [G loss: 1.574004]\n",
      "1207 [D loss: 0.387282, acc.: 83.20%] [G loss: 1.612429]\n",
      "1208 [D loss: 0.395017, acc.: 81.64%] [G loss: 1.706573]\n",
      "1209 [D loss: 0.420878, acc.: 80.47%] [G loss: 1.792908]\n",
      "1210 [D loss: 0.434027, acc.: 78.52%] [G loss: 1.703293]\n",
      "1211 [D loss: 0.444408, acc.: 78.12%] [G loss: 1.583153]\n",
      "1212 [D loss: 0.408898, acc.: 81.64%] [G loss: 1.638846]\n",
      "1213 [D loss: 0.434957, acc.: 78.91%] [G loss: 1.806785]\n",
      "1214 [D loss: 0.400100, acc.: 82.03%] [G loss: 1.666391]\n",
      "1215 [D loss: 0.409331, acc.: 83.20%] [G loss: 1.590604]\n",
      "1216 [D loss: 0.386312, acc.: 82.03%] [G loss: 1.649109]\n",
      "1217 [D loss: 0.370686, acc.: 84.38%] [G loss: 1.599318]\n",
      "1218 [D loss: 0.381604, acc.: 82.81%] [G loss: 1.623088]\n",
      "1219 [D loss: 0.397934, acc.: 82.81%] [G loss: 1.759716]\n",
      "1220 [D loss: 0.380503, acc.: 85.16%] [G loss: 1.602901]\n",
      "1221 [D loss: 0.417477, acc.: 82.81%] [G loss: 1.575541]\n",
      "1222 [D loss: 0.423729, acc.: 79.69%] [G loss: 1.699023]\n",
      "1223 [D loss: 0.408230, acc.: 81.25%] [G loss: 1.752462]\n",
      "1224 [D loss: 0.407789, acc.: 84.38%] [G loss: 1.689519]\n",
      "1225 [D loss: 0.430629, acc.: 78.52%] [G loss: 1.642980]\n",
      "1226 [D loss: 0.402290, acc.: 82.03%] [G loss: 1.657952]\n",
      "1227 [D loss: 0.386737, acc.: 85.55%] [G loss: 1.652041]\n",
      "1228 [D loss: 0.413617, acc.: 83.59%] [G loss: 1.739244]\n",
      "1229 [D loss: 0.429079, acc.: 78.52%] [G loss: 1.637750]\n",
      "1230 [D loss: 0.424423, acc.: 82.81%] [G loss: 1.756160]\n",
      "1231 [D loss: 0.410353, acc.: 80.86%] [G loss: 1.810002]\n",
      "1232 [D loss: 0.413131, acc.: 82.42%] [G loss: 1.714779]\n",
      "1233 [D loss: 0.407729, acc.: 83.20%] [G loss: 1.774900]\n",
      "1234 [D loss: 0.425196, acc.: 81.25%] [G loss: 1.700063]\n",
      "1235 [D loss: 0.371258, acc.: 84.77%] [G loss: 1.753445]\n",
      "1236 [D loss: 0.382979, acc.: 82.03%] [G loss: 1.648241]\n",
      "1237 [D loss: 0.407045, acc.: 82.42%] [G loss: 1.588457]\n",
      "1238 [D loss: 0.405755, acc.: 80.47%] [G loss: 1.696669]\n",
      "1239 [D loss: 0.399182, acc.: 82.03%] [G loss: 1.709679]\n",
      "1240 [D loss: 0.357927, acc.: 86.72%] [G loss: 1.710479]\n",
      "1241 [D loss: 0.364214, acc.: 84.77%] [G loss: 1.630244]\n",
      "1242 [D loss: 0.372621, acc.: 82.03%] [G loss: 1.704149]\n",
      "1243 [D loss: 0.364243, acc.: 84.38%] [G loss: 1.712327]\n",
      "1244 [D loss: 0.391385, acc.: 82.03%] [G loss: 1.782291]\n",
      "1245 [D loss: 0.378051, acc.: 79.69%] [G loss: 1.603645]\n",
      "1246 [D loss: 0.423252, acc.: 81.25%] [G loss: 1.681564]\n",
      "1247 [D loss: 0.384496, acc.: 82.81%] [G loss: 1.647226]\n",
      "1248 [D loss: 0.393296, acc.: 83.20%] [G loss: 1.803784]\n",
      "1249 [D loss: 0.384370, acc.: 83.59%] [G loss: 1.763970]\n",
      "1250 [D loss: 0.416519, acc.: 83.98%] [G loss: 1.788261]\n",
      "1251 [D loss: 0.387741, acc.: 85.16%] [G loss: 1.654798]\n",
      "1252 [D loss: 0.371214, acc.: 83.98%] [G loss: 1.834170]\n",
      "1253 [D loss: 0.387388, acc.: 84.38%] [G loss: 1.759502]\n",
      "1254 [D loss: 0.384214, acc.: 81.64%] [G loss: 1.801614]\n",
      "1255 [D loss: 0.369418, acc.: 83.20%] [G loss: 1.707347]\n",
      "1256 [D loss: 0.366702, acc.: 83.98%] [G loss: 1.553048]\n",
      "1257 [D loss: 0.413455, acc.: 82.42%] [G loss: 1.606317]\n",
      "1258 [D loss: 0.378599, acc.: 84.38%] [G loss: 1.675642]\n",
      "1259 [D loss: 0.387388, acc.: 84.77%] [G loss: 1.706748]\n",
      "1260 [D loss: 0.406839, acc.: 82.81%] [G loss: 1.770072]\n",
      "1261 [D loss: 0.372469, acc.: 82.42%] [G loss: 1.740684]\n",
      "1262 [D loss: 0.372439, acc.: 85.16%] [G loss: 1.522574]\n",
      "1263 [D loss: 0.407063, acc.: 83.59%] [G loss: 1.686036]\n",
      "1264 [D loss: 0.401043, acc.: 82.42%] [G loss: 1.681829]\n",
      "1265 [D loss: 0.350357, acc.: 85.16%] [G loss: 1.789329]\n",
      "1266 [D loss: 0.369843, acc.: 84.77%] [G loss: 1.558095]\n",
      "1267 [D loss: 0.377147, acc.: 83.59%] [G loss: 1.724318]\n",
      "1268 [D loss: 0.385101, acc.: 84.38%] [G loss: 1.789501]\n",
      "1269 [D loss: 0.399886, acc.: 83.20%] [G loss: 1.748919]\n",
      "1270 [D loss: 0.389456, acc.: 81.64%] [G loss: 1.706439]\n",
      "1271 [D loss: 0.349100, acc.: 85.94%] [G loss: 1.708987]\n",
      "1272 [D loss: 0.385190, acc.: 85.55%] [G loss: 1.753868]\n",
      "1273 [D loss: 0.355125, acc.: 86.33%] [G loss: 1.751364]\n",
      "1274 [D loss: 0.363331, acc.: 85.55%] [G loss: 1.760306]\n",
      "1275 [D loss: 0.358409, acc.: 85.16%] [G loss: 1.791574]\n",
      "1276 [D loss: 0.351808, acc.: 83.59%] [G loss: 1.776958]\n",
      "1277 [D loss: 0.356606, acc.: 84.77%] [G loss: 1.669456]\n",
      "1278 [D loss: 0.389302, acc.: 81.64%] [G loss: 1.718748]\n",
      "1279 [D loss: 0.363230, acc.: 85.55%] [G loss: 1.706919]\n",
      "1280 [D loss: 0.355940, acc.: 84.38%] [G loss: 1.641001]\n",
      "1281 [D loss: 0.355810, acc.: 85.94%] [G loss: 1.688647]\n",
      "1282 [D loss: 0.373046, acc.: 82.03%] [G loss: 1.774679]\n",
      "1283 [D loss: 0.357936, acc.: 84.77%] [G loss: 1.740869]\n",
      "1284 [D loss: 0.350506, acc.: 87.50%] [G loss: 1.781557]\n",
      "1285 [D loss: 0.388753, acc.: 82.42%] [G loss: 1.816511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286 [D loss: 0.350116, acc.: 86.33%] [G loss: 1.759917]\n",
      "1287 [D loss: 0.385578, acc.: 84.77%] [G loss: 1.831321]\n",
      "1288 [D loss: 0.360801, acc.: 82.42%] [G loss: 1.781470]\n",
      "1289 [D loss: 0.341086, acc.: 86.72%] [G loss: 1.854708]\n",
      "1290 [D loss: 0.361226, acc.: 85.55%] [G loss: 1.731730]\n",
      "1291 [D loss: 0.385267, acc.: 85.16%] [G loss: 1.832307]\n",
      "1292 [D loss: 0.358728, acc.: 85.16%] [G loss: 1.837168]\n",
      "1293 [D loss: 0.348604, acc.: 86.72%] [G loss: 1.805035]\n",
      "1294 [D loss: 0.345827, acc.: 85.55%] [G loss: 1.752907]\n",
      "1295 [D loss: 0.375718, acc.: 84.77%] [G loss: 1.769172]\n",
      "1296 [D loss: 0.363130, acc.: 84.38%] [G loss: 1.842273]\n",
      "1297 [D loss: 0.375017, acc.: 83.98%] [G loss: 1.791397]\n",
      "1298 [D loss: 0.339195, acc.: 85.16%] [G loss: 1.764147]\n",
      "1299 [D loss: 0.358924, acc.: 85.16%] [G loss: 1.794078]\n",
      "1300 [D loss: 0.350797, acc.: 84.38%] [G loss: 1.842415]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "1301 [D loss: 0.407942, acc.: 80.08%] [G loss: 1.909672]\n",
      "1302 [D loss: 0.352306, acc.: 86.72%] [G loss: 1.808493]\n",
      "1303 [D loss: 0.392202, acc.: 80.08%] [G loss: 1.720666]\n",
      "1304 [D loss: 0.414833, acc.: 78.91%] [G loss: 1.857703]\n",
      "1305 [D loss: 0.375134, acc.: 80.47%] [G loss: 2.034187]\n",
      "1306 [D loss: 0.382567, acc.: 82.42%] [G loss: 1.803479]\n",
      "1307 [D loss: 0.354946, acc.: 84.38%] [G loss: 1.750143]\n",
      "1308 [D loss: 0.384336, acc.: 83.20%] [G loss: 1.637226]\n",
      "1309 [D loss: 0.368480, acc.: 82.81%] [G loss: 1.814945]\n",
      "1310 [D loss: 0.383998, acc.: 81.25%] [G loss: 1.887359]\n",
      "1311 [D loss: 0.344060, acc.: 83.59%] [G loss: 1.678430]\n",
      "1312 [D loss: 0.397499, acc.: 80.47%] [G loss: 1.689111]\n",
      "1313 [D loss: 0.362059, acc.: 85.16%] [G loss: 1.833251]\n",
      "1314 [D loss: 0.410956, acc.: 81.25%] [G loss: 1.810579]\n",
      "1315 [D loss: 0.389977, acc.: 83.59%] [G loss: 1.848600]\n",
      "1316 [D loss: 0.396872, acc.: 79.69%] [G loss: 1.843678]\n",
      "1317 [D loss: 0.390217, acc.: 81.25%] [G loss: 1.639513]\n",
      "1318 [D loss: 0.369342, acc.: 85.55%] [G loss: 1.773448]\n",
      "1319 [D loss: 0.344003, acc.: 85.94%] [G loss: 1.852494]\n",
      "1320 [D loss: 0.369961, acc.: 82.81%] [G loss: 1.825992]\n",
      "1321 [D loss: 0.362082, acc.: 83.98%] [G loss: 1.883634]\n",
      "1322 [D loss: 0.356356, acc.: 82.81%] [G loss: 1.816285]\n",
      "1323 [D loss: 0.385950, acc.: 80.47%] [G loss: 1.791547]\n",
      "1324 [D loss: 0.376959, acc.: 79.30%] [G loss: 1.827606]\n",
      "1325 [D loss: 0.384255, acc.: 82.81%] [G loss: 1.869322]\n",
      "1326 [D loss: 0.417743, acc.: 80.08%] [G loss: 1.818529]\n",
      "1327 [D loss: 0.356702, acc.: 84.38%] [G loss: 1.966313]\n",
      "1328 [D loss: 0.395593, acc.: 80.47%] [G loss: 1.922457]\n",
      "1329 [D loss: 0.359935, acc.: 83.98%] [G loss: 1.908867]\n",
      "1330 [D loss: 0.356667, acc.: 86.33%] [G loss: 1.867522]\n",
      "1331 [D loss: 0.355682, acc.: 85.55%] [G loss: 1.857391]\n",
      "1332 [D loss: 0.376387, acc.: 82.03%] [G loss: 1.762228]\n",
      "1333 [D loss: 0.356857, acc.: 83.20%] [G loss: 1.786857]\n",
      "1334 [D loss: 0.380329, acc.: 79.30%] [G loss: 1.913077]\n",
      "1335 [D loss: 0.389175, acc.: 83.59%] [G loss: 1.884666]\n",
      "1336 [D loss: 0.419706, acc.: 79.69%] [G loss: 1.809529]\n",
      "1337 [D loss: 0.358482, acc.: 83.20%] [G loss: 1.925425]\n",
      "1338 [D loss: 0.355879, acc.: 85.55%] [G loss: 1.744488]\n",
      "1339 [D loss: 0.339651, acc.: 85.16%] [G loss: 1.864193]\n",
      "1340 [D loss: 0.374741, acc.: 82.42%] [G loss: 1.823928]\n",
      "1341 [D loss: 0.392861, acc.: 82.42%] [G loss: 1.879969]\n",
      "1342 [D loss: 0.383379, acc.: 83.20%] [G loss: 1.943140]\n",
      "1343 [D loss: 0.382813, acc.: 80.86%] [G loss: 1.689779]\n",
      "1344 [D loss: 0.356602, acc.: 84.77%] [G loss: 1.766483]\n",
      "1345 [D loss: 0.353769, acc.: 84.38%] [G loss: 1.989798]\n",
      "1346 [D loss: 0.408440, acc.: 79.30%] [G loss: 2.143287]\n",
      "1347 [D loss: 0.369020, acc.: 83.59%] [G loss: 1.909962]\n",
      "1348 [D loss: 0.375780, acc.: 81.25%] [G loss: 1.682423]\n",
      "1349 [D loss: 0.374925, acc.: 81.64%] [G loss: 1.747864]\n",
      "1350 [D loss: 0.357839, acc.: 83.59%] [G loss: 1.746466]\n",
      "1351 [D loss: 0.362499, acc.: 81.64%] [G loss: 1.864777]\n",
      "1352 [D loss: 0.360914, acc.: 85.94%] [G loss: 1.863273]\n",
      "1353 [D loss: 0.346091, acc.: 86.33%] [G loss: 1.865083]\n",
      "1354 [D loss: 0.354592, acc.: 82.81%] [G loss: 1.727438]\n",
      "1355 [D loss: 0.372341, acc.: 80.86%] [G loss: 1.873334]\n",
      "1356 [D loss: 0.341611, acc.: 84.38%] [G loss: 1.966580]\n",
      "1357 [D loss: 0.368020, acc.: 82.42%] [G loss: 1.810066]\n",
      "1358 [D loss: 0.371818, acc.: 82.03%] [G loss: 1.845705]\n",
      "1359 [D loss: 0.347893, acc.: 85.55%] [G loss: 1.866263]\n",
      "1360 [D loss: 0.355865, acc.: 82.42%] [G loss: 1.862853]\n",
      "1361 [D loss: 0.341233, acc.: 86.72%] [G loss: 1.773538]\n",
      "1362 [D loss: 0.369682, acc.: 83.59%] [G loss: 1.766713]\n",
      "1363 [D loss: 0.344894, acc.: 85.94%] [G loss: 1.958879]\n",
      "1364 [D loss: 0.344880, acc.: 86.72%] [G loss: 1.895148]\n",
      "1365 [D loss: 0.356286, acc.: 83.59%] [G loss: 1.863055]\n",
      "1366 [D loss: 0.352105, acc.: 84.38%] [G loss: 1.820705]\n",
      "1367 [D loss: 0.418707, acc.: 78.52%] [G loss: 1.814538]\n",
      "1368 [D loss: 0.356923, acc.: 83.20%] [G loss: 1.820682]\n",
      "1369 [D loss: 0.369435, acc.: 81.64%] [G loss: 1.837739]\n",
      "1370 [D loss: 0.363109, acc.: 84.38%] [G loss: 1.832699]\n",
      "1371 [D loss: 0.366646, acc.: 85.16%] [G loss: 1.832131]\n",
      "1372 [D loss: 0.355016, acc.: 84.38%] [G loss: 1.939762]\n",
      "1373 [D loss: 0.358203, acc.: 85.16%] [G loss: 1.716331]\n",
      "1374 [D loss: 0.351591, acc.: 84.77%] [G loss: 1.853219]\n",
      "1375 [D loss: 0.328920, acc.: 84.77%] [G loss: 1.827920]\n",
      "1376 [D loss: 0.363063, acc.: 84.38%] [G loss: 1.977931]\n",
      "1377 [D loss: 0.356104, acc.: 83.98%] [G loss: 2.010927]\n",
      "1378 [D loss: 0.398912, acc.: 78.91%] [G loss: 2.064043]\n",
      "1379 [D loss: 0.365551, acc.: 82.03%] [G loss: 1.980773]\n",
      "1380 [D loss: 0.341460, acc.: 83.98%] [G loss: 1.798745]\n",
      "1381 [D loss: 0.326652, acc.: 86.33%] [G loss: 1.982367]\n",
      "1382 [D loss: 0.353256, acc.: 83.98%] [G loss: 1.872418]\n",
      "1383 [D loss: 0.315775, acc.: 87.50%] [G loss: 1.767795]\n",
      "1384 [D loss: 0.323444, acc.: 86.33%] [G loss: 1.838220]\n",
      "1385 [D loss: 0.403648, acc.: 82.81%] [G loss: 1.946880]\n",
      "1386 [D loss: 0.341536, acc.: 86.72%] [G loss: 1.878221]\n",
      "1387 [D loss: 0.379354, acc.: 81.25%] [G loss: 1.858946]\n",
      "1388 [D loss: 0.344828, acc.: 84.38%] [G loss: 1.944412]\n",
      "1389 [D loss: 0.355291, acc.: 82.81%] [G loss: 1.800031]\n",
      "1390 [D loss: 0.364404, acc.: 85.16%] [G loss: 1.824284]\n",
      "1391 [D loss: 0.363977, acc.: 84.77%] [G loss: 1.914226]\n",
      "1392 [D loss: 0.376661, acc.: 85.55%] [G loss: 1.898860]\n",
      "1393 [D loss: 0.381299, acc.: 81.25%] [G loss: 1.845940]\n",
      "1394 [D loss: 0.381500, acc.: 83.59%] [G loss: 1.804635]\n",
      "1395 [D loss: 0.331487, acc.: 86.72%] [G loss: 1.881333]\n",
      "1396 [D loss: 0.343400, acc.: 84.77%] [G loss: 1.971343]\n",
      "1397 [D loss: 0.359614, acc.: 83.98%] [G loss: 1.876446]\n",
      "1398 [D loss: 0.344730, acc.: 83.20%] [G loss: 1.968119]\n",
      "1399 [D loss: 0.359964, acc.: 85.55%] [G loss: 1.854382]\n",
      "1400 [D loss: 0.369529, acc.: 82.03%] [G loss: 1.856446]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "1401 [D loss: 0.330458, acc.: 87.50%] [G loss: 1.829835]\n",
      "1402 [D loss: 0.379744, acc.: 82.42%] [G loss: 1.845212]\n",
      "1403 [D loss: 0.355059, acc.: 83.59%] [G loss: 1.911475]\n",
      "1404 [D loss: 0.334375, acc.: 86.33%] [G loss: 1.901373]\n",
      "1405 [D loss: 0.348304, acc.: 86.33%] [G loss: 1.860679]\n",
      "1406 [D loss: 0.304351, acc.: 88.67%] [G loss: 1.940711]\n",
      "1407 [D loss: 0.369546, acc.: 85.94%] [G loss: 1.915895]\n",
      "1408 [D loss: 0.358463, acc.: 83.59%] [G loss: 1.998562]\n",
      "1409 [D loss: 0.341292, acc.: 85.55%] [G loss: 1.898930]\n",
      "1410 [D loss: 0.325904, acc.: 87.11%] [G loss: 1.930033]\n",
      "1411 [D loss: 0.316423, acc.: 87.11%] [G loss: 1.881196]\n",
      "1412 [D loss: 0.309688, acc.: 87.89%] [G loss: 1.898970]\n",
      "1413 [D loss: 0.332364, acc.: 87.11%] [G loss: 1.941738]\n",
      "1414 [D loss: 0.329786, acc.: 84.38%] [G loss: 1.914145]\n",
      "1415 [D loss: 0.362228, acc.: 84.77%] [G loss: 1.994088]\n",
      "1416 [D loss: 0.320154, acc.: 86.33%] [G loss: 1.891434]\n",
      "1417 [D loss: 0.362046, acc.: 85.16%] [G loss: 1.957731]\n",
      "1418 [D loss: 0.334521, acc.: 85.94%] [G loss: 1.978029]\n",
      "1419 [D loss: 0.352971, acc.: 84.38%] [G loss: 1.900250]\n",
      "1420 [D loss: 0.336441, acc.: 86.72%] [G loss: 2.023109]\n",
      "1421 [D loss: 0.346276, acc.: 84.38%] [G loss: 2.081102]\n",
      "1422 [D loss: 0.335368, acc.: 83.59%] [G loss: 1.932437]\n",
      "1423 [D loss: 0.332011, acc.: 86.72%] [G loss: 1.887185]\n",
      "1424 [D loss: 0.322711, acc.: 85.94%] [G loss: 1.817566]\n",
      "1425 [D loss: 0.372365, acc.: 84.77%] [G loss: 2.063863]\n",
      "1426 [D loss: 0.366992, acc.: 83.20%] [G loss: 2.128406]\n",
      "1427 [D loss: 0.395709, acc.: 82.81%] [G loss: 1.861778]\n",
      "1428 [D loss: 0.335677, acc.: 87.11%] [G loss: 1.949320]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1429 [D loss: 0.309608, acc.: 87.11%] [G loss: 1.856014]\n",
      "1430 [D loss: 0.329347, acc.: 87.89%] [G loss: 1.997763]\n",
      "1431 [D loss: 0.359651, acc.: 85.55%] [G loss: 2.206105]\n",
      "1432 [D loss: 0.330535, acc.: 87.11%] [G loss: 1.869420]\n",
      "1433 [D loss: 0.312414, acc.: 86.72%] [G loss: 2.034369]\n",
      "1434 [D loss: 0.381458, acc.: 84.38%] [G loss: 2.031734]\n",
      "1435 [D loss: 0.344592, acc.: 85.94%] [G loss: 1.999457]\n",
      "1436 [D loss: 0.380991, acc.: 82.42%] [G loss: 2.002673]\n",
      "1437 [D loss: 0.314931, acc.: 87.11%] [G loss: 1.937025]\n",
      "1438 [D loss: 0.357989, acc.: 83.98%] [G loss: 2.016913]\n",
      "1439 [D loss: 0.350998, acc.: 83.59%] [G loss: 1.815248]\n",
      "1440 [D loss: 0.314762, acc.: 87.50%] [G loss: 2.062279]\n",
      "1441 [D loss: 0.343479, acc.: 86.33%] [G loss: 1.918502]\n",
      "1442 [D loss: 0.322074, acc.: 85.94%] [G loss: 2.009403]\n",
      "1443 [D loss: 0.321210, acc.: 87.89%] [G loss: 1.942057]\n",
      "1444 [D loss: 0.332262, acc.: 87.89%] [G loss: 2.085245]\n",
      "1445 [D loss: 0.386224, acc.: 85.16%] [G loss: 2.028607]\n",
      "1446 [D loss: 0.319157, acc.: 86.33%] [G loss: 2.097799]\n",
      "1447 [D loss: 0.354127, acc.: 83.20%] [G loss: 2.048101]\n",
      "1448 [D loss: 0.333315, acc.: 85.16%] [G loss: 2.134195]\n",
      "1449 [D loss: 0.314429, acc.: 86.72%] [G loss: 2.081655]\n",
      "1450 [D loss: 0.327278, acc.: 84.77%] [G loss: 1.888201]\n",
      "1451 [D loss: 0.335179, acc.: 85.94%] [G loss: 2.114428]\n",
      "1452 [D loss: 0.319288, acc.: 86.33%] [G loss: 2.030616]\n",
      "1453 [D loss: 0.324154, acc.: 87.50%] [G loss: 2.044731]\n",
      "1454 [D loss: 0.315308, acc.: 86.33%] [G loss: 1.959035]\n",
      "1455 [D loss: 0.343484, acc.: 83.98%] [G loss: 1.905604]\n",
      "1456 [D loss: 0.325119, acc.: 86.72%] [G loss: 2.087548]\n",
      "1457 [D loss: 0.296080, acc.: 88.28%] [G loss: 2.037359]\n",
      "1458 [D loss: 0.331199, acc.: 85.55%] [G loss: 1.965403]\n",
      "1459 [D loss: 0.318775, acc.: 87.11%] [G loss: 2.113832]\n",
      "1460 [D loss: 0.311274, acc.: 85.94%] [G loss: 1.996661]\n",
      "1461 [D loss: 0.340067, acc.: 85.55%] [G loss: 2.060466]\n",
      "1462 [D loss: 0.312392, acc.: 87.50%] [G loss: 2.086216]\n",
      "1463 [D loss: 0.294251, acc.: 89.06%] [G loss: 1.971391]\n",
      "1464 [D loss: 0.325037, acc.: 85.55%] [G loss: 1.928590]\n",
      "1465 [D loss: 0.338865, acc.: 85.16%] [G loss: 1.936887]\n",
      "1466 [D loss: 0.299770, acc.: 85.55%] [G loss: 2.043371]\n",
      "1467 [D loss: 0.315908, acc.: 88.67%] [G loss: 2.114287]\n",
      "1468 [D loss: 0.298099, acc.: 87.89%] [G loss: 2.014462]\n",
      "1469 [D loss: 0.295911, acc.: 87.89%] [G loss: 1.963542]\n",
      "1470 [D loss: 0.275533, acc.: 89.84%] [G loss: 2.031156]\n",
      "1471 [D loss: 0.285423, acc.: 87.50%] [G loss: 2.011486]\n",
      "1472 [D loss: 0.316734, acc.: 86.72%] [G loss: 2.036711]\n",
      "1473 [D loss: 0.270038, acc.: 89.06%] [G loss: 2.080085]\n",
      "1474 [D loss: 0.328963, acc.: 86.33%] [G loss: 2.096905]\n",
      "1475 [D loss: 0.322597, acc.: 84.77%] [G loss: 2.212556]\n",
      "1476 [D loss: 0.295528, acc.: 88.28%] [G loss: 2.074659]\n",
      "1477 [D loss: 0.308408, acc.: 87.50%] [G loss: 2.040018]\n",
      "1478 [D loss: 0.293096, acc.: 87.50%] [G loss: 2.045234]\n",
      "1479 [D loss: 0.306730, acc.: 85.94%] [G loss: 2.217762]\n",
      "1480 [D loss: 0.325822, acc.: 87.89%] [G loss: 2.001780]\n",
      "1481 [D loss: 0.350093, acc.: 86.33%] [G loss: 2.059838]\n",
      "1482 [D loss: 0.282311, acc.: 89.45%] [G loss: 2.203862]\n",
      "1483 [D loss: 0.289645, acc.: 87.50%] [G loss: 1.894276]\n",
      "1484 [D loss: 0.307625, acc.: 86.72%] [G loss: 2.066023]\n",
      "1485 [D loss: 0.330710, acc.: 84.77%] [G loss: 2.092908]\n",
      "1486 [D loss: 0.305177, acc.: 84.38%] [G loss: 2.035515]\n",
      "1487 [D loss: 0.299965, acc.: 87.11%] [G loss: 2.132035]\n",
      "1488 [D loss: 0.260845, acc.: 89.06%] [G loss: 2.130148]\n",
      "1489 [D loss: 0.294252, acc.: 86.33%] [G loss: 2.279873]\n",
      "1490 [D loss: 0.289747, acc.: 87.89%] [G loss: 2.092479]\n",
      "1491 [D loss: 0.323587, acc.: 86.72%] [G loss: 1.989380]\n",
      "1492 [D loss: 0.298094, acc.: 87.50%] [G loss: 2.201950]\n",
      "1493 [D loss: 0.293103, acc.: 87.50%] [G loss: 2.136248]\n",
      "1494 [D loss: 0.302090, acc.: 87.11%] [G loss: 2.147930]\n",
      "1495 [D loss: 0.296836, acc.: 89.06%] [G loss: 2.159779]\n",
      "1496 [D loss: 0.284782, acc.: 87.89%] [G loss: 1.978258]\n",
      "1497 [D loss: 0.303475, acc.: 88.28%] [G loss: 2.109318]\n",
      "1498 [D loss: 0.310928, acc.: 86.33%] [G loss: 2.160762]\n",
      "1499 [D loss: 0.316996, acc.: 84.38%] [G loss: 2.089303]\n",
      "1500 [D loss: 0.294503, acc.: 87.11%] [G loss: 2.026460]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "1501 [D loss: 0.311119, acc.: 87.50%] [G loss: 2.138347]\n",
      "1502 [D loss: 0.333759, acc.: 86.33%] [G loss: 2.213050]\n",
      "1503 [D loss: 0.325401, acc.: 84.77%] [G loss: 2.247949]\n",
      "1504 [D loss: 0.309138, acc.: 87.50%] [G loss: 2.112124]\n",
      "1505 [D loss: 0.300044, acc.: 87.11%] [G loss: 2.129486]\n",
      "1506 [D loss: 0.310947, acc.: 86.72%] [G loss: 2.044573]\n",
      "1507 [D loss: 0.314974, acc.: 87.11%] [G loss: 2.156493]\n",
      "1508 [D loss: 0.301614, acc.: 86.72%] [G loss: 2.086010]\n",
      "1509 [D loss: 0.314061, acc.: 85.94%] [G loss: 2.076345]\n",
      "1510 [D loss: 0.302213, acc.: 86.72%] [G loss: 2.128810]\n",
      "1511 [D loss: 0.324451, acc.: 87.11%] [G loss: 2.099943]\n",
      "1512 [D loss: 0.293507, acc.: 86.72%] [G loss: 2.195815]\n",
      "1513 [D loss: 0.324546, acc.: 87.11%] [G loss: 2.170385]\n",
      "1514 [D loss: 0.295827, acc.: 90.23%] [G loss: 2.080774]\n",
      "1515 [D loss: 0.300638, acc.: 86.33%] [G loss: 2.077229]\n",
      "1516 [D loss: 0.303274, acc.: 85.16%] [G loss: 2.263279]\n",
      "1517 [D loss: 0.284880, acc.: 88.28%] [G loss: 2.318043]\n",
      "1518 [D loss: 0.344036, acc.: 84.77%] [G loss: 2.103070]\n",
      "1519 [D loss: 0.322844, acc.: 84.77%] [G loss: 2.219923]\n",
      "1520 [D loss: 0.311240, acc.: 85.94%] [G loss: 2.117747]\n",
      "1521 [D loss: 0.299591, acc.: 89.06%] [G loss: 2.047976]\n",
      "1522 [D loss: 0.271201, acc.: 91.41%] [G loss: 2.170253]\n",
      "1523 [D loss: 0.298168, acc.: 88.28%] [G loss: 2.207264]\n",
      "1524 [D loss: 0.276407, acc.: 90.62%] [G loss: 2.173910]\n",
      "1525 [D loss: 0.309919, acc.: 87.89%] [G loss: 2.247009]\n",
      "1526 [D loss: 0.277732, acc.: 89.45%] [G loss: 1.992381]\n",
      "1527 [D loss: 0.275790, acc.: 88.28%] [G loss: 2.094121]\n",
      "1528 [D loss: 0.318340, acc.: 85.55%] [G loss: 2.296851]\n",
      "1529 [D loss: 0.296921, acc.: 88.67%] [G loss: 2.186898]\n",
      "1530 [D loss: 0.274327, acc.: 88.67%] [G loss: 2.135032]\n",
      "1531 [D loss: 0.294415, acc.: 87.89%] [G loss: 2.168881]\n",
      "1532 [D loss: 0.303181, acc.: 88.28%] [G loss: 2.238591]\n",
      "1533 [D loss: 0.265057, acc.: 89.06%] [G loss: 2.270226]\n",
      "1534 [D loss: 0.305251, acc.: 86.72%] [G loss: 2.077054]\n",
      "1535 [D loss: 0.273746, acc.: 89.45%] [G loss: 2.243750]\n",
      "1536 [D loss: 0.299120, acc.: 86.33%] [G loss: 2.149033]\n",
      "1537 [D loss: 0.275746, acc.: 89.84%] [G loss: 2.112113]\n",
      "1538 [D loss: 0.307304, acc.: 85.94%] [G loss: 2.188769]\n",
      "1539 [D loss: 0.314623, acc.: 87.11%] [G loss: 2.151076]\n",
      "1540 [D loss: 0.263351, acc.: 90.23%] [G loss: 2.129337]\n",
      "1541 [D loss: 0.227350, acc.: 91.80%] [G loss: 2.183063]\n",
      "1542 [D loss: 0.281750, acc.: 87.89%] [G loss: 2.278320]\n",
      "1543 [D loss: 0.318862, acc.: 86.72%] [G loss: 2.239741]\n",
      "1544 [D loss: 0.279528, acc.: 90.62%] [G loss: 2.059228]\n",
      "1545 [D loss: 0.330038, acc.: 85.94%] [G loss: 2.367308]\n",
      "1546 [D loss: 0.289028, acc.: 86.72%] [G loss: 2.190498]\n",
      "1547 [D loss: 0.281271, acc.: 88.28%] [G loss: 2.172404]\n",
      "1548 [D loss: 0.306625, acc.: 87.50%] [G loss: 2.016406]\n",
      "1549 [D loss: 0.268763, acc.: 89.45%] [G loss: 2.313589]\n",
      "1550 [D loss: 0.279278, acc.: 87.89%] [G loss: 2.178605]\n",
      "1551 [D loss: 0.316546, acc.: 86.33%] [G loss: 2.200162]\n",
      "1552 [D loss: 0.278278, acc.: 89.84%] [G loss: 2.034786]\n",
      "1553 [D loss: 0.293294, acc.: 87.50%] [G loss: 2.376920]\n",
      "1554 [D loss: 0.329037, acc.: 84.77%] [G loss: 2.308196]\n",
      "1555 [D loss: 0.304805, acc.: 83.98%] [G loss: 2.235064]\n",
      "1556 [D loss: 0.278412, acc.: 87.89%] [G loss: 2.232067]\n",
      "1557 [D loss: 0.284254, acc.: 87.89%] [G loss: 2.189558]\n",
      "1558 [D loss: 0.264601, acc.: 90.23%] [G loss: 2.440177]\n",
      "1559 [D loss: 0.273835, acc.: 89.84%] [G loss: 2.298328]\n",
      "1560 [D loss: 0.311395, acc.: 89.84%] [G loss: 2.418866]\n",
      "1561 [D loss: 0.286746, acc.: 87.11%] [G loss: 2.320987]\n",
      "1562 [D loss: 0.322422, acc.: 84.38%] [G loss: 2.298614]\n",
      "1563 [D loss: 0.297262, acc.: 86.72%] [G loss: 2.365083]\n",
      "1564 [D loss: 0.301716, acc.: 87.50%] [G loss: 2.318088]\n",
      "1565 [D loss: 0.286745, acc.: 89.06%] [G loss: 2.253875]\n",
      "1566 [D loss: 0.323747, acc.: 87.50%] [G loss: 2.202621]\n",
      "1567 [D loss: 0.285680, acc.: 88.67%] [G loss: 2.247201]\n",
      "1568 [D loss: 0.296699, acc.: 87.50%] [G loss: 2.404960]\n",
      "1569 [D loss: 0.273913, acc.: 87.11%] [G loss: 2.206234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570 [D loss: 0.314522, acc.: 87.11%] [G loss: 2.299756]\n",
      "1571 [D loss: 0.276217, acc.: 89.84%] [G loss: 2.203218]\n",
      "1572 [D loss: 0.282414, acc.: 87.11%] [G loss: 2.305270]\n",
      "1573 [D loss: 0.286809, acc.: 89.06%] [G loss: 2.075467]\n",
      "1574 [D loss: 0.276776, acc.: 88.28%] [G loss: 2.201333]\n",
      "1575 [D loss: 0.302468, acc.: 85.55%] [G loss: 2.355569]\n",
      "1576 [D loss: 0.288876, acc.: 89.45%] [G loss: 2.277505]\n",
      "1577 [D loss: 0.297406, acc.: 87.89%] [G loss: 2.203768]\n",
      "1578 [D loss: 0.305245, acc.: 87.89%] [G loss: 2.338322]\n",
      "1579 [D loss: 0.284120, acc.: 87.89%] [G loss: 2.343747]\n",
      "1580 [D loss: 0.294292, acc.: 88.67%] [G loss: 2.207284]\n",
      "1581 [D loss: 0.282515, acc.: 88.28%] [G loss: 2.205789]\n",
      "1582 [D loss: 0.286475, acc.: 88.67%] [G loss: 2.368837]\n",
      "1583 [D loss: 0.269492, acc.: 90.23%] [G loss: 2.271060]\n",
      "1584 [D loss: 0.312279, acc.: 87.50%] [G loss: 2.242857]\n",
      "1585 [D loss: 0.261338, acc.: 90.62%] [G loss: 2.197358]\n",
      "1586 [D loss: 0.278868, acc.: 89.45%] [G loss: 2.251966]\n",
      "1587 [D loss: 0.286056, acc.: 88.67%] [G loss: 2.365701]\n",
      "1588 [D loss: 0.288155, acc.: 89.84%] [G loss: 2.091836]\n",
      "1589 [D loss: 0.274651, acc.: 90.23%] [G loss: 2.447529]\n",
      "1590 [D loss: 0.266109, acc.: 88.67%] [G loss: 2.185498]\n",
      "1591 [D loss: 0.261380, acc.: 91.41%] [G loss: 2.073437]\n",
      "1592 [D loss: 0.269503, acc.: 87.89%] [G loss: 2.283993]\n",
      "1593 [D loss: 0.276783, acc.: 89.06%] [G loss: 2.373351]\n",
      "1594 [D loss: 0.256381, acc.: 90.62%] [G loss: 2.217578]\n",
      "1595 [D loss: 0.285827, acc.: 88.67%] [G loss: 2.344990]\n",
      "1596 [D loss: 0.298930, acc.: 86.72%] [G loss: 2.502759]\n",
      "1597 [D loss: 0.272280, acc.: 88.28%] [G loss: 2.378340]\n",
      "1598 [D loss: 0.287014, acc.: 86.72%] [G loss: 2.418416]\n",
      "1599 [D loss: 0.242574, acc.: 89.45%] [G loss: 2.360748]\n",
      "1600 [D loss: 0.267368, acc.: 89.45%] [G loss: 2.176752]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "1601 [D loss: 0.259114, acc.: 89.84%] [G loss: 2.119362]\n",
      "1602 [D loss: 0.274778, acc.: 89.06%] [G loss: 2.417669]\n",
      "1603 [D loss: 0.285441, acc.: 85.94%] [G loss: 2.301597]\n",
      "1604 [D loss: 0.281911, acc.: 88.67%] [G loss: 2.260185]\n",
      "1605 [D loss: 0.286776, acc.: 89.06%] [G loss: 2.353634]\n",
      "1606 [D loss: 0.276240, acc.: 89.06%] [G loss: 2.271549]\n",
      "1607 [D loss: 0.243524, acc.: 90.23%] [G loss: 2.273276]\n",
      "1608 [D loss: 0.299324, acc.: 86.33%] [G loss: 2.184834]\n",
      "1609 [D loss: 0.287234, acc.: 90.23%] [G loss: 2.375352]\n",
      "1610 [D loss: 0.256713, acc.: 89.84%] [G loss: 2.346276]\n",
      "1611 [D loss: 0.249681, acc.: 90.62%] [G loss: 2.247491]\n",
      "1612 [D loss: 0.264994, acc.: 87.11%] [G loss: 2.282210]\n",
      "1613 [D loss: 0.276360, acc.: 88.28%] [G loss: 2.188940]\n",
      "1614 [D loss: 0.283172, acc.: 87.11%] [G loss: 2.207013]\n",
      "1615 [D loss: 0.268298, acc.: 91.02%] [G loss: 2.399925]\n",
      "1616 [D loss: 0.278988, acc.: 86.72%] [G loss: 2.408300]\n",
      "1617 [D loss: 0.266055, acc.: 89.06%] [G loss: 2.168240]\n",
      "1618 [D loss: 0.247607, acc.: 89.84%] [G loss: 2.430115]\n",
      "1619 [D loss: 0.263955, acc.: 89.06%] [G loss: 2.562186]\n",
      "1620 [D loss: 0.261578, acc.: 87.50%] [G loss: 2.406880]\n",
      "1621 [D loss: 0.265619, acc.: 89.45%] [G loss: 2.296286]\n",
      "1622 [D loss: 0.270829, acc.: 89.84%] [G loss: 2.633109]\n",
      "1623 [D loss: 0.288867, acc.: 86.72%] [G loss: 2.422615]\n",
      "1624 [D loss: 0.260893, acc.: 89.84%] [G loss: 2.328933]\n",
      "1625 [D loss: 0.253127, acc.: 89.84%] [G loss: 2.443678]\n",
      "1626 [D loss: 0.281755, acc.: 89.45%] [G loss: 2.311080]\n",
      "1627 [D loss: 0.314937, acc.: 86.72%] [G loss: 2.473303]\n",
      "1628 [D loss: 0.279225, acc.: 89.45%] [G loss: 2.430740]\n",
      "1629 [D loss: 0.257220, acc.: 90.23%] [G loss: 2.480309]\n",
      "1630 [D loss: 0.262793, acc.: 88.67%] [G loss: 2.456576]\n",
      "1631 [D loss: 0.296422, acc.: 86.72%] [G loss: 2.373568]\n",
      "1632 [D loss: 0.302487, acc.: 85.55%] [G loss: 2.433940]\n",
      "1633 [D loss: 0.268669, acc.: 88.28%] [G loss: 2.464845]\n",
      "1634 [D loss: 0.280800, acc.: 88.28%] [G loss: 2.444358]\n",
      "1635 [D loss: 0.270291, acc.: 88.67%] [G loss: 2.513575]\n",
      "1636 [D loss: 0.240108, acc.: 91.02%] [G loss: 2.435064]\n",
      "1637 [D loss: 0.312172, acc.: 87.11%] [G loss: 2.437564]\n",
      "1638 [D loss: 0.280754, acc.: 89.45%] [G loss: 2.406053]\n",
      "1639 [D loss: 0.273157, acc.: 89.06%] [G loss: 2.254551]\n",
      "1640 [D loss: 0.274143, acc.: 89.84%] [G loss: 2.392759]\n",
      "1641 [D loss: 0.236944, acc.: 91.02%] [G loss: 2.340077]\n",
      "1642 [D loss: 0.278780, acc.: 89.06%] [G loss: 2.323025]\n",
      "1643 [D loss: 0.278833, acc.: 87.50%] [G loss: 2.488268]\n",
      "1644 [D loss: 0.259808, acc.: 90.62%] [G loss: 2.376220]\n",
      "1645 [D loss: 0.281963, acc.: 87.89%] [G loss: 2.397791]\n",
      "1646 [D loss: 0.238920, acc.: 91.41%] [G loss: 2.338909]\n",
      "1647 [D loss: 0.262894, acc.: 87.89%] [G loss: 2.261014]\n",
      "1648 [D loss: 0.264734, acc.: 87.50%] [G loss: 2.215594]\n",
      "1649 [D loss: 0.255047, acc.: 89.84%] [G loss: 2.435043]\n",
      "1650 [D loss: 0.261885, acc.: 90.23%] [G loss: 2.484327]\n",
      "1651 [D loss: 0.262678, acc.: 88.28%] [G loss: 2.066360]\n",
      "1652 [D loss: 0.283974, acc.: 87.50%] [G loss: 2.285940]\n",
      "1653 [D loss: 0.269607, acc.: 89.06%] [G loss: 2.391274]\n",
      "1654 [D loss: 0.262674, acc.: 87.89%] [G loss: 2.459026]\n",
      "1655 [D loss: 0.235990, acc.: 91.02%] [G loss: 2.380072]\n",
      "1656 [D loss: 0.263021, acc.: 89.84%] [G loss: 2.498191]\n",
      "1657 [D loss: 0.271238, acc.: 89.06%] [G loss: 2.419291]\n",
      "1658 [D loss: 0.253253, acc.: 90.62%] [G loss: 2.543016]\n",
      "1659 [D loss: 0.261486, acc.: 87.50%] [G loss: 2.485586]\n",
      "1660 [D loss: 0.244279, acc.: 90.23%] [G loss: 2.494931]\n",
      "1661 [D loss: 0.253564, acc.: 89.06%] [G loss: 2.288703]\n",
      "1662 [D loss: 0.267014, acc.: 87.89%] [G loss: 2.395380]\n",
      "1663 [D loss: 0.246706, acc.: 90.23%] [G loss: 2.560683]\n",
      "1664 [D loss: 0.278127, acc.: 87.50%] [G loss: 2.436933]\n",
      "1665 [D loss: 0.252382, acc.: 89.06%] [G loss: 2.268751]\n",
      "1666 [D loss: 0.220556, acc.: 92.97%] [G loss: 2.270174]\n",
      "1667 [D loss: 0.266633, acc.: 88.28%] [G loss: 2.554225]\n",
      "1668 [D loss: 0.245045, acc.: 90.23%] [G loss: 2.667452]\n",
      "1669 [D loss: 0.258511, acc.: 88.67%] [G loss: 1.962552]\n",
      "1670 [D loss: 0.268773, acc.: 88.28%] [G loss: 2.306715]\n",
      "1671 [D loss: 0.251289, acc.: 90.62%] [G loss: 2.619247]\n",
      "1672 [D loss: 0.259081, acc.: 88.67%] [G loss: 2.491336]\n",
      "1673 [D loss: 0.272826, acc.: 87.50%] [G loss: 2.327114]\n",
      "1674 [D loss: 0.242235, acc.: 91.80%] [G loss: 2.373127]\n",
      "1675 [D loss: 0.231452, acc.: 91.80%] [G loss: 2.416761]\n",
      "1676 [D loss: 0.254878, acc.: 90.23%] [G loss: 2.425210]\n",
      "1677 [D loss: 0.276068, acc.: 87.11%] [G loss: 2.525545]\n",
      "1678 [D loss: 0.267725, acc.: 88.67%] [G loss: 2.664869]\n",
      "1679 [D loss: 0.257681, acc.: 88.67%] [G loss: 2.450436]\n",
      "1680 [D loss: 0.222740, acc.: 93.36%] [G loss: 2.683789]\n",
      "1681 [D loss: 0.313279, acc.: 85.16%] [G loss: 2.505484]\n",
      "1682 [D loss: 0.261269, acc.: 87.50%] [G loss: 2.534670]\n",
      "1683 [D loss: 0.274678, acc.: 88.28%] [G loss: 2.372945]\n",
      "1684 [D loss: 0.258208, acc.: 88.67%] [G loss: 2.580359]\n",
      "1685 [D loss: 0.238546, acc.: 91.80%] [G loss: 2.461235]\n",
      "1686 [D loss: 0.223263, acc.: 91.41%] [G loss: 2.470780]\n",
      "1687 [D loss: 0.263615, acc.: 89.06%] [G loss: 2.555604]\n",
      "1688 [D loss: 0.250679, acc.: 89.84%] [G loss: 2.406448]\n",
      "1689 [D loss: 0.248097, acc.: 90.62%] [G loss: 2.603638]\n",
      "1690 [D loss: 0.254311, acc.: 89.84%] [G loss: 2.469884]\n",
      "1691 [D loss: 0.272516, acc.: 87.50%] [G loss: 2.571867]\n",
      "1692 [D loss: 0.286038, acc.: 87.11%] [G loss: 2.536197]\n",
      "1693 [D loss: 0.274364, acc.: 87.89%] [G loss: 2.490479]\n",
      "1694 [D loss: 0.275653, acc.: 86.72%] [G loss: 2.441829]\n",
      "1695 [D loss: 0.258526, acc.: 89.84%] [G loss: 2.542695]\n",
      "1696 [D loss: 0.265315, acc.: 88.67%] [G loss: 2.724680]\n",
      "1697 [D loss: 0.257359, acc.: 87.89%] [G loss: 2.636025]\n",
      "1698 [D loss: 0.258512, acc.: 89.06%] [G loss: 2.553119]\n",
      "1699 [D loss: 0.225438, acc.: 91.02%] [G loss: 2.473466]\n",
      "1700 [D loss: 0.282389, acc.: 87.89%] [G loss: 2.555612]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "1701 [D loss: 0.232867, acc.: 92.19%] [G loss: 2.576931]\n",
      "1702 [D loss: 0.283849, acc.: 87.50%] [G loss: 2.521502]\n",
      "1703 [D loss: 0.263951, acc.: 87.89%] [G loss: 2.662386]\n",
      "1704 [D loss: 0.257444, acc.: 86.33%] [G loss: 2.524043]\n",
      "1705 [D loss: 0.309876, acc.: 84.38%] [G loss: 2.563829]\n",
      "1706 [D loss: 0.249415, acc.: 90.23%] [G loss: 2.549428]\n",
      "1707 [D loss: 0.261339, acc.: 87.89%] [G loss: 2.539162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1708 [D loss: 0.224455, acc.: 92.97%] [G loss: 2.626466]\n",
      "1709 [D loss: 0.264471, acc.: 87.11%] [G loss: 2.682793]\n",
      "1710 [D loss: 0.247672, acc.: 89.84%] [G loss: 2.522092]\n",
      "1711 [D loss: 0.272879, acc.: 88.67%] [G loss: 2.570588]\n",
      "1712 [D loss: 0.239651, acc.: 89.06%] [G loss: 2.434738]\n",
      "1713 [D loss: 0.247633, acc.: 91.41%] [G loss: 2.508288]\n",
      "1714 [D loss: 0.246433, acc.: 91.41%] [G loss: 2.531415]\n",
      "1715 [D loss: 0.243023, acc.: 91.02%] [G loss: 2.442124]\n",
      "1716 [D loss: 0.235466, acc.: 91.80%] [G loss: 2.761473]\n",
      "1717 [D loss: 0.260709, acc.: 89.06%] [G loss: 2.497486]\n",
      "1718 [D loss: 0.271576, acc.: 89.06%] [G loss: 2.625915]\n",
      "1719 [D loss: 0.247916, acc.: 90.23%] [G loss: 2.618317]\n",
      "1720 [D loss: 0.254372, acc.: 89.06%] [G loss: 2.546269]\n",
      "1721 [D loss: 0.224392, acc.: 89.45%] [G loss: 2.473611]\n",
      "1722 [D loss: 0.241546, acc.: 90.62%] [G loss: 2.811258]\n",
      "1723 [D loss: 0.255922, acc.: 87.89%] [G loss: 2.633577]\n",
      "1724 [D loss: 0.243465, acc.: 90.62%] [G loss: 2.500945]\n",
      "1725 [D loss: 0.258223, acc.: 88.28%] [G loss: 2.536617]\n",
      "1726 [D loss: 0.215024, acc.: 91.80%] [G loss: 2.430908]\n",
      "1727 [D loss: 0.229845, acc.: 89.45%] [G loss: 2.581402]\n",
      "1728 [D loss: 0.233257, acc.: 89.84%] [G loss: 2.730467]\n",
      "1729 [D loss: 0.226682, acc.: 91.41%] [G loss: 2.694187]\n",
      "1730 [D loss: 0.252976, acc.: 88.28%] [G loss: 2.581394]\n",
      "1731 [D loss: 0.274075, acc.: 88.67%] [G loss: 2.505036]\n",
      "1732 [D loss: 0.258140, acc.: 87.11%] [G loss: 2.637807]\n",
      "1733 [D loss: 0.271650, acc.: 88.28%] [G loss: 2.538435]\n",
      "1734 [D loss: 0.252738, acc.: 91.41%] [G loss: 2.514462]\n",
      "1735 [D loss: 0.254521, acc.: 89.06%] [G loss: 2.517401]\n",
      "1736 [D loss: 0.251745, acc.: 91.02%] [G loss: 2.715758]\n",
      "1737 [D loss: 0.215911, acc.: 91.02%] [G loss: 2.723384]\n",
      "1738 [D loss: 0.245931, acc.: 89.06%] [G loss: 2.558741]\n",
      "1739 [D loss: 0.267571, acc.: 88.67%] [G loss: 2.534430]\n",
      "1740 [D loss: 0.240354, acc.: 89.84%] [G loss: 2.416792]\n",
      "1741 [D loss: 0.229004, acc.: 91.02%] [G loss: 2.256560]\n",
      "1742 [D loss: 0.242562, acc.: 89.45%] [G loss: 2.926415]\n",
      "1743 [D loss: 0.248550, acc.: 88.67%] [G loss: 2.743052]\n",
      "1744 [D loss: 0.274448, acc.: 87.50%] [G loss: 2.494318]\n",
      "1745 [D loss: 0.238219, acc.: 88.28%] [G loss: 2.606496]\n",
      "1746 [D loss: 0.256286, acc.: 90.62%] [G loss: 2.551119]\n",
      "1747 [D loss: 0.267077, acc.: 89.84%] [G loss: 2.499334]\n",
      "1748 [D loss: 0.231615, acc.: 89.84%] [G loss: 2.675034]\n",
      "1749 [D loss: 0.220776, acc.: 93.36%] [G loss: 2.583845]\n",
      "1750 [D loss: 0.254977, acc.: 89.84%] [G loss: 2.427678]\n",
      "1751 [D loss: 0.265524, acc.: 88.67%] [G loss: 2.647661]\n",
      "1752 [D loss: 0.231870, acc.: 91.80%] [G loss: 2.522707]\n",
      "1753 [D loss: 0.199865, acc.: 94.14%] [G loss: 2.602763]\n",
      "1754 [D loss: 0.216333, acc.: 92.58%] [G loss: 2.631730]\n",
      "1755 [D loss: 0.240358, acc.: 88.67%] [G loss: 2.728607]\n",
      "1756 [D loss: 0.239505, acc.: 89.45%] [G loss: 2.676942]\n",
      "1757 [D loss: 0.253205, acc.: 88.28%] [G loss: 2.485344]\n",
      "1758 [D loss: 0.222441, acc.: 91.41%] [G loss: 2.594170]\n",
      "1759 [D loss: 0.190907, acc.: 92.19%] [G loss: 2.664271]\n",
      "1760 [D loss: 0.232103, acc.: 91.02%] [G loss: 2.641025]\n",
      "1761 [D loss: 0.247647, acc.: 89.06%] [G loss: 2.744990]\n",
      "1762 [D loss: 0.222016, acc.: 91.80%] [G loss: 2.743442]\n",
      "1763 [D loss: 0.264575, acc.: 88.67%] [G loss: 2.767034]\n",
      "1764 [D loss: 0.226507, acc.: 91.02%] [G loss: 2.519035]\n",
      "1765 [D loss: 0.260181, acc.: 87.11%] [G loss: 2.650345]\n",
      "1766 [D loss: 0.212959, acc.: 91.41%] [G loss: 2.615755]\n",
      "1767 [D loss: 0.234923, acc.: 90.62%] [G loss: 2.295361]\n",
      "1768 [D loss: 0.245139, acc.: 91.02%] [G loss: 2.609969]\n",
      "1769 [D loss: 0.223513, acc.: 91.41%] [G loss: 2.785304]\n",
      "1770 [D loss: 0.234639, acc.: 91.41%] [G loss: 2.773639]\n",
      "1771 [D loss: 0.248081, acc.: 89.06%] [G loss: 2.529873]\n",
      "1772 [D loss: 0.219466, acc.: 92.19%] [G loss: 2.447117]\n",
      "1773 [D loss: 0.219736, acc.: 92.19%] [G loss: 2.554242]\n",
      "1774 [D loss: 0.218051, acc.: 89.06%] [G loss: 2.776266]\n",
      "1775 [D loss: 0.278103, acc.: 86.72%] [G loss: 2.488903]\n",
      "1776 [D loss: 0.234027, acc.: 91.41%] [G loss: 2.671074]\n",
      "1777 [D loss: 0.264970, acc.: 89.84%] [G loss: 2.330907]\n",
      "1778 [D loss: 0.240991, acc.: 90.23%] [G loss: 2.583473]\n",
      "1779 [D loss: 0.218982, acc.: 91.41%] [G loss: 2.781631]\n",
      "1780 [D loss: 0.225592, acc.: 91.41%] [G loss: 2.462260]\n",
      "1781 [D loss: 0.226484, acc.: 91.41%] [G loss: 2.665016]\n",
      "1782 [D loss: 0.258237, acc.: 89.84%] [G loss: 2.583841]\n",
      "1783 [D loss: 0.249064, acc.: 88.28%] [G loss: 2.976459]\n",
      "1784 [D loss: 0.276615, acc.: 85.94%] [G loss: 2.548573]\n",
      "1785 [D loss: 0.210816, acc.: 92.19%] [G loss: 2.667567]\n",
      "1786 [D loss: 0.227798, acc.: 90.62%] [G loss: 2.720845]\n",
      "1787 [D loss: 0.236224, acc.: 90.23%] [G loss: 2.484239]\n",
      "1788 [D loss: 0.210410, acc.: 91.41%] [G loss: 2.805202]\n",
      "1789 [D loss: 0.225301, acc.: 90.23%] [G loss: 2.846405]\n",
      "1790 [D loss: 0.198653, acc.: 92.19%] [G loss: 2.462681]\n",
      "1791 [D loss: 0.250951, acc.: 90.23%] [G loss: 2.635404]\n",
      "1792 [D loss: 0.265898, acc.: 88.28%] [G loss: 2.774756]\n",
      "1793 [D loss: 0.233094, acc.: 92.97%] [G loss: 2.668882]\n",
      "1794 [D loss: 0.237526, acc.: 91.41%] [G loss: 2.937155]\n",
      "1795 [D loss: 0.261229, acc.: 89.06%] [G loss: 2.449342]\n",
      "1796 [D loss: 0.221572, acc.: 91.41%] [G loss: 2.469349]\n",
      "1797 [D loss: 0.219448, acc.: 92.58%] [G loss: 2.736900]\n",
      "1798 [D loss: 0.243496, acc.: 87.89%] [G loss: 2.620605]\n",
      "1799 [D loss: 0.235373, acc.: 88.67%] [G loss: 2.412338]\n",
      "1800 [D loss: 0.199765, acc.: 90.62%] [G loss: 2.484708]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "1801 [D loss: 0.248348, acc.: 89.45%] [G loss: 2.731883]\n",
      "1802 [D loss: 0.244652, acc.: 89.06%] [G loss: 2.753165]\n",
      "1803 [D loss: 0.220542, acc.: 89.84%] [G loss: 2.872561]\n",
      "1804 [D loss: 0.224154, acc.: 90.23%] [G loss: 2.683065]\n",
      "1805 [D loss: 0.228658, acc.: 87.89%] [G loss: 2.781121]\n",
      "1806 [D loss: 0.237116, acc.: 88.28%] [G loss: 2.595702]\n",
      "1807 [D loss: 0.232552, acc.: 89.84%] [G loss: 2.550256]\n",
      "1808 [D loss: 0.210453, acc.: 90.23%] [G loss: 2.676018]\n",
      "1809 [D loss: 0.224234, acc.: 90.62%] [G loss: 2.720039]\n",
      "1810 [D loss: 0.212166, acc.: 89.84%] [G loss: 2.833875]\n",
      "1811 [D loss: 0.239596, acc.: 89.45%] [G loss: 2.578743]\n",
      "1812 [D loss: 0.234656, acc.: 89.45%] [G loss: 2.763907]\n",
      "1813 [D loss: 0.220724, acc.: 91.41%] [G loss: 3.065661]\n",
      "1814 [D loss: 0.234426, acc.: 89.06%] [G loss: 2.667348]\n",
      "1815 [D loss: 0.260291, acc.: 88.67%] [G loss: 2.823846]\n",
      "1816 [D loss: 0.243491, acc.: 90.23%] [G loss: 2.826899]\n",
      "1817 [D loss: 0.239666, acc.: 90.23%] [G loss: 2.487147]\n",
      "1818 [D loss: 0.237269, acc.: 89.84%] [G loss: 2.702402]\n",
      "1819 [D loss: 0.209510, acc.: 92.19%] [G loss: 2.817859]\n",
      "1820 [D loss: 0.247876, acc.: 88.28%] [G loss: 2.656542]\n",
      "1821 [D loss: 0.226536, acc.: 92.58%] [G loss: 2.603774]\n",
      "1822 [D loss: 0.227508, acc.: 91.80%] [G loss: 2.766096]\n",
      "1823 [D loss: 0.240985, acc.: 88.67%] [G loss: 2.801687]\n",
      "1824 [D loss: 0.245251, acc.: 89.45%] [G loss: 2.776279]\n",
      "1825 [D loss: 0.236079, acc.: 89.45%] [G loss: 2.595914]\n",
      "1826 [D loss: 0.221740, acc.: 90.62%] [G loss: 2.569260]\n",
      "1827 [D loss: 0.212869, acc.: 91.41%] [G loss: 2.827093]\n",
      "1828 [D loss: 0.224683, acc.: 90.62%] [G loss: 2.707039]\n",
      "1829 [D loss: 0.252749, acc.: 89.45%] [G loss: 2.820729]\n",
      "1830 [D loss: 0.187518, acc.: 93.36%] [G loss: 2.795363]\n",
      "1831 [D loss: 0.242844, acc.: 89.45%] [G loss: 2.679780]\n",
      "1832 [D loss: 0.248074, acc.: 89.06%] [G loss: 2.656447]\n",
      "1833 [D loss: 0.230129, acc.: 89.84%] [G loss: 2.802213]\n",
      "1834 [D loss: 0.217785, acc.: 92.19%] [G loss: 2.830180]\n",
      "1835 [D loss: 0.205685, acc.: 91.41%] [G loss: 2.719782]\n",
      "1836 [D loss: 0.208477, acc.: 93.36%] [G loss: 2.933726]\n",
      "1837 [D loss: 0.231509, acc.: 90.62%] [G loss: 3.051339]\n",
      "1838 [D loss: 0.226097, acc.: 89.84%] [G loss: 2.882113]\n",
      "1839 [D loss: 0.228786, acc.: 89.45%] [G loss: 2.737665]\n",
      "1840 [D loss: 0.197154, acc.: 91.02%] [G loss: 2.672211]\n",
      "1841 [D loss: 0.214281, acc.: 90.23%] [G loss: 2.591186]\n",
      "1842 [D loss: 0.243758, acc.: 91.02%] [G loss: 2.833175]\n",
      "1843 [D loss: 0.272474, acc.: 87.50%] [G loss: 2.882695]\n",
      "1844 [D loss: 0.247767, acc.: 87.89%] [G loss: 2.847789]\n",
      "1845 [D loss: 0.196472, acc.: 92.19%] [G loss: 2.917604]\n",
      "1846 [D loss: 0.236892, acc.: 89.45%] [G loss: 2.639511]\n",
      "1847 [D loss: 0.245302, acc.: 89.45%] [G loss: 2.971442]\n",
      "1848 [D loss: 0.230011, acc.: 89.06%] [G loss: 2.806211]\n",
      "1849 [D loss: 0.214960, acc.: 90.23%] [G loss: 2.452536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1850 [D loss: 0.210927, acc.: 91.02%] [G loss: 2.952666]\n",
      "1851 [D loss: 0.223487, acc.: 89.84%] [G loss: 3.064902]\n",
      "1852 [D loss: 0.226568, acc.: 89.45%] [G loss: 2.618999]\n",
      "1853 [D loss: 0.215686, acc.: 92.97%] [G loss: 2.929680]\n",
      "1854 [D loss: 0.209303, acc.: 87.89%] [G loss: 3.017206]\n",
      "1855 [D loss: 0.239044, acc.: 90.23%] [G loss: 2.615219]\n",
      "1856 [D loss: 0.201921, acc.: 92.58%] [G loss: 2.752103]\n",
      "1857 [D loss: 0.229350, acc.: 90.62%] [G loss: 3.094285]\n",
      "1858 [D loss: 0.220379, acc.: 89.84%] [G loss: 3.020247]\n",
      "1859 [D loss: 0.258954, acc.: 87.11%] [G loss: 2.580580]\n",
      "1860 [D loss: 0.219575, acc.: 92.19%] [G loss: 2.777477]\n",
      "1861 [D loss: 0.208831, acc.: 91.02%] [G loss: 2.936878]\n",
      "1862 [D loss: 0.216122, acc.: 91.80%] [G loss: 2.871621]\n",
      "1863 [D loss: 0.234623, acc.: 90.62%] [G loss: 2.751443]\n",
      "1864 [D loss: 0.210356, acc.: 90.23%] [G loss: 2.858923]\n",
      "1865 [D loss: 0.222794, acc.: 90.23%] [G loss: 2.816053]\n",
      "1866 [D loss: 0.203767, acc.: 93.75%] [G loss: 2.799397]\n",
      "1867 [D loss: 0.218394, acc.: 89.84%] [G loss: 2.824759]\n",
      "1868 [D loss: 0.214884, acc.: 91.41%] [G loss: 3.048313]\n",
      "1869 [D loss: 0.233638, acc.: 89.84%] [G loss: 2.781443]\n",
      "1870 [D loss: 0.208851, acc.: 92.19%] [G loss: 2.789343]\n",
      "1871 [D loss: 0.210940, acc.: 91.02%] [G loss: 2.933422]\n",
      "1872 [D loss: 0.183633, acc.: 92.58%] [G loss: 2.787394]\n",
      "1873 [D loss: 0.209970, acc.: 92.19%] [G loss: 2.903189]\n",
      "1874 [D loss: 0.242167, acc.: 89.45%] [G loss: 3.169000]\n",
      "1875 [D loss: 0.208857, acc.: 91.02%] [G loss: 2.889190]\n",
      "1876 [D loss: 0.217499, acc.: 89.84%] [G loss: 2.974556]\n",
      "1877 [D loss: 0.235841, acc.: 91.41%] [G loss: 2.735116]\n",
      "1878 [D loss: 0.213948, acc.: 90.23%] [G loss: 2.997120]\n",
      "1879 [D loss: 0.220134, acc.: 88.67%] [G loss: 2.929262]\n",
      "1880 [D loss: 0.265768, acc.: 87.50%] [G loss: 2.720817]\n",
      "1881 [D loss: 0.235351, acc.: 88.28%] [G loss: 2.920183]\n",
      "1882 [D loss: 0.235313, acc.: 88.67%] [G loss: 2.884296]\n",
      "1883 [D loss: 0.185327, acc.: 92.58%] [G loss: 2.915259]\n",
      "1884 [D loss: 0.233853, acc.: 89.84%] [G loss: 2.718147]\n",
      "1885 [D loss: 0.256771, acc.: 87.50%] [G loss: 2.979559]\n",
      "1886 [D loss: 0.238482, acc.: 89.45%] [G loss: 2.941251]\n",
      "1887 [D loss: 0.239561, acc.: 90.23%] [G loss: 2.847302]\n",
      "1888 [D loss: 0.236629, acc.: 90.62%] [G loss: 2.647238]\n",
      "1889 [D loss: 0.219687, acc.: 90.23%] [G loss: 2.776150]\n",
      "1890 [D loss: 0.197447, acc.: 91.80%] [G loss: 2.924641]\n",
      "1891 [D loss: 0.209828, acc.: 90.62%] [G loss: 2.650216]\n",
      "1892 [D loss: 0.187754, acc.: 92.97%] [G loss: 2.783829]\n",
      "1893 [D loss: 0.215130, acc.: 90.62%] [G loss: 2.838892]\n",
      "1894 [D loss: 0.237820, acc.: 89.45%] [G loss: 2.991965]\n",
      "1895 [D loss: 0.212883, acc.: 91.80%] [G loss: 2.970220]\n",
      "1896 [D loss: 0.229673, acc.: 89.45%] [G loss: 2.804877]\n",
      "1897 [D loss: 0.226407, acc.: 90.23%] [G loss: 2.867842]\n",
      "1898 [D loss: 0.248621, acc.: 87.50%] [G loss: 2.965972]\n",
      "1899 [D loss: 0.233879, acc.: 90.62%] [G loss: 2.888918]\n",
      "1900 [D loss: 0.207881, acc.: 92.19%] [G loss: 2.833216]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "1901 [D loss: 0.239819, acc.: 88.67%] [G loss: 2.676211]\n",
      "1902 [D loss: 0.216185, acc.: 92.58%] [G loss: 2.755352]\n",
      "1903 [D loss: 0.210870, acc.: 90.23%] [G loss: 2.633938]\n",
      "1904 [D loss: 0.215656, acc.: 90.62%] [G loss: 2.894839]\n",
      "1905 [D loss: 0.188786, acc.: 92.97%] [G loss: 3.174479]\n",
      "1906 [D loss: 0.220347, acc.: 91.02%] [G loss: 2.970588]\n",
      "1907 [D loss: 0.222777, acc.: 90.62%] [G loss: 2.924906]\n",
      "1908 [D loss: 0.196284, acc.: 92.58%] [G loss: 2.806526]\n",
      "1909 [D loss: 0.244181, acc.: 90.62%] [G loss: 2.740408]\n",
      "1910 [D loss: 0.258276, acc.: 88.67%] [G loss: 3.110467]\n",
      "1911 [D loss: 0.221289, acc.: 90.23%] [G loss: 2.967143]\n",
      "1912 [D loss: 0.221040, acc.: 90.62%] [G loss: 2.640518]\n",
      "1913 [D loss: 0.227155, acc.: 90.62%] [G loss: 2.755661]\n",
      "1914 [D loss: 0.206021, acc.: 91.02%] [G loss: 2.811781]\n",
      "1915 [D loss: 0.232224, acc.: 89.45%] [G loss: 2.936644]\n",
      "1916 [D loss: 0.230438, acc.: 89.84%] [G loss: 2.980677]\n",
      "1917 [D loss: 0.218097, acc.: 91.80%] [G loss: 2.729386]\n",
      "1918 [D loss: 0.233309, acc.: 91.02%] [G loss: 2.941111]\n",
      "1919 [D loss: 0.231013, acc.: 89.84%] [G loss: 2.925708]\n",
      "1920 [D loss: 0.202570, acc.: 90.23%] [G loss: 2.648363]\n",
      "1921 [D loss: 0.216673, acc.: 91.02%] [G loss: 2.645675]\n",
      "1922 [D loss: 0.212039, acc.: 90.23%] [G loss: 2.955174]\n",
      "1923 [D loss: 0.209821, acc.: 91.41%] [G loss: 2.905955]\n",
      "1924 [D loss: 0.220931, acc.: 89.45%] [G loss: 2.965578]\n",
      "1925 [D loss: 0.185117, acc.: 92.97%] [G loss: 2.664107]\n",
      "1926 [D loss: 0.236883, acc.: 88.67%] [G loss: 2.950702]\n",
      "1927 [D loss: 0.215464, acc.: 91.41%] [G loss: 3.069266]\n",
      "1928 [D loss: 0.208714, acc.: 92.19%] [G loss: 2.570792]\n",
      "1929 [D loss: 0.240614, acc.: 91.02%] [G loss: 2.630012]\n",
      "1930 [D loss: 0.223712, acc.: 89.84%] [G loss: 2.881491]\n",
      "1931 [D loss: 0.229401, acc.: 88.67%] [G loss: 3.107068]\n",
      "1932 [D loss: 0.230778, acc.: 90.62%] [G loss: 3.026392]\n",
      "1933 [D loss: 0.201313, acc.: 91.41%] [G loss: 2.746097]\n",
      "1934 [D loss: 0.234867, acc.: 91.02%] [G loss: 2.632219]\n",
      "1935 [D loss: 0.210932, acc.: 91.41%] [G loss: 2.983132]\n",
      "1936 [D loss: 0.237586, acc.: 90.23%] [G loss: 2.744881]\n",
      "1937 [D loss: 0.236002, acc.: 91.41%] [G loss: 2.913931]\n",
      "1938 [D loss: 0.212122, acc.: 91.80%] [G loss: 2.778613]\n",
      "1939 [D loss: 0.186793, acc.: 91.80%] [G loss: 2.907143]\n",
      "1940 [D loss: 0.246367, acc.: 88.67%] [G loss: 3.050361]\n",
      "1941 [D loss: 0.181936, acc.: 92.58%] [G loss: 2.821736]\n",
      "1942 [D loss: 0.225573, acc.: 90.23%] [G loss: 3.007274]\n",
      "1943 [D loss: 0.212581, acc.: 91.41%] [G loss: 2.795476]\n",
      "1944 [D loss: 0.222429, acc.: 91.41%] [G loss: 2.745970]\n",
      "1945 [D loss: 0.241333, acc.: 88.67%] [G loss: 2.961555]\n",
      "1946 [D loss: 0.207257, acc.: 91.41%] [G loss: 2.918773]\n",
      "1947 [D loss: 0.215660, acc.: 90.62%] [G loss: 2.896863]\n",
      "1948 [D loss: 0.206225, acc.: 92.19%] [G loss: 2.695761]\n",
      "1949 [D loss: 0.219364, acc.: 91.02%] [G loss: 2.547985]\n",
      "1950 [D loss: 0.198038, acc.: 91.80%] [G loss: 2.907786]\n",
      "1951 [D loss: 0.200174, acc.: 92.58%] [G loss: 3.125309]\n",
      "1952 [D loss: 0.207675, acc.: 89.84%] [G loss: 2.912802]\n",
      "1953 [D loss: 0.249321, acc.: 89.84%] [G loss: 2.725350]\n",
      "1954 [D loss: 0.215585, acc.: 89.84%] [G loss: 3.351698]\n",
      "1955 [D loss: 0.245324, acc.: 89.06%] [G loss: 2.870800]\n",
      "1956 [D loss: 0.208915, acc.: 91.02%] [G loss: 3.059030]\n",
      "1957 [D loss: 0.205250, acc.: 92.19%] [G loss: 2.839097]\n",
      "1958 [D loss: 0.220669, acc.: 89.06%] [G loss: 2.732782]\n",
      "1959 [D loss: 0.205991, acc.: 92.19%] [G loss: 2.925960]\n",
      "1960 [D loss: 0.213772, acc.: 92.97%] [G loss: 2.916655]\n",
      "1961 [D loss: 0.218799, acc.: 89.06%] [G loss: 3.020471]\n",
      "1962 [D loss: 0.258997, acc.: 89.45%] [G loss: 3.061935]\n",
      "1963 [D loss: 0.211039, acc.: 92.58%] [G loss: 2.949525]\n",
      "1964 [D loss: 0.195183, acc.: 93.36%] [G loss: 2.896352]\n",
      "1965 [D loss: 0.182142, acc.: 93.75%] [G loss: 2.917617]\n",
      "1966 [D loss: 0.209182, acc.: 91.41%] [G loss: 3.160790]\n",
      "1967 [D loss: 0.201954, acc.: 90.62%] [G loss: 3.007844]\n",
      "1968 [D loss: 0.264796, acc.: 88.67%] [G loss: 3.058990]\n",
      "1969 [D loss: 0.224777, acc.: 90.23%] [G loss: 3.076879]\n",
      "1970 [D loss: 0.229286, acc.: 89.45%] [G loss: 2.981603]\n",
      "1971 [D loss: 0.211119, acc.: 91.80%] [G loss: 2.781678]\n",
      "1972 [D loss: 0.226858, acc.: 90.23%] [G loss: 2.912930]\n",
      "1973 [D loss: 0.207139, acc.: 92.58%] [G loss: 2.978151]\n",
      "1974 [D loss: 0.214840, acc.: 91.02%] [G loss: 2.729027]\n",
      "1975 [D loss: 0.211514, acc.: 90.62%] [G loss: 2.691049]\n",
      "1976 [D loss: 0.202146, acc.: 92.58%] [G loss: 3.018296]\n",
      "1977 [D loss: 0.270965, acc.: 87.50%] [G loss: 2.838811]\n",
      "1978 [D loss: 0.216033, acc.: 90.62%] [G loss: 3.178333]\n",
      "1979 [D loss: 0.207147, acc.: 90.62%] [G loss: 3.016289]\n",
      "1980 [D loss: 0.214825, acc.: 91.41%] [G loss: 2.793286]\n",
      "1981 [D loss: 0.191209, acc.: 91.80%] [G loss: 2.959672]\n",
      "1982 [D loss: 0.234323, acc.: 90.62%] [G loss: 2.712371]\n",
      "1983 [D loss: 0.241019, acc.: 89.84%] [G loss: 3.035838]\n",
      "1984 [D loss: 0.184245, acc.: 92.19%] [G loss: 2.917029]\n",
      "1985 [D loss: 0.221095, acc.: 91.02%] [G loss: 2.999333]\n",
      "1986 [D loss: 0.211300, acc.: 91.02%] [G loss: 2.882119]\n",
      "1987 [D loss: 0.241510, acc.: 90.62%] [G loss: 3.055357]\n",
      "1988 [D loss: 0.212756, acc.: 90.62%] [G loss: 3.046362]\n",
      "1989 [D loss: 0.229257, acc.: 90.23%] [G loss: 2.980382]\n",
      "1990 [D loss: 0.174409, acc.: 93.36%] [G loss: 2.918425]\n",
      "1991 [D loss: 0.229388, acc.: 88.67%] [G loss: 2.837902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992 [D loss: 0.196247, acc.: 91.02%] [G loss: 3.181689]\n",
      "1993 [D loss: 0.246102, acc.: 89.45%] [G loss: 3.041157]\n",
      "1994 [D loss: 0.210147, acc.: 91.80%] [G loss: 3.268687]\n",
      "1995 [D loss: 0.225471, acc.: 90.23%] [G loss: 2.715370]\n",
      "1996 [D loss: 0.221405, acc.: 90.23%] [G loss: 3.033989]\n",
      "1997 [D loss: 0.175568, acc.: 92.97%] [G loss: 3.122747]\n",
      "1998 [D loss: 0.207928, acc.: 90.23%] [G loss: 3.119969]\n",
      "1999 [D loss: 0.183780, acc.: 92.58%] [G loss: 2.791487]\n",
      "2000 [D loss: 0.225443, acc.: 89.84%] [G loss: 2.927643]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "2001 [D loss: 0.217021, acc.: 90.62%] [G loss: 2.742185]\n",
      "2002 [D loss: 0.221623, acc.: 89.84%] [G loss: 2.997497]\n",
      "2003 [D loss: 0.199477, acc.: 91.02%] [G loss: 3.068550]\n",
      "2004 [D loss: 0.237307, acc.: 91.02%] [G loss: 3.020389]\n",
      "2005 [D loss: 0.249426, acc.: 88.28%] [G loss: 3.152505]\n",
      "2006 [D loss: 0.256464, acc.: 89.06%] [G loss: 3.006216]\n",
      "2007 [D loss: 0.266564, acc.: 87.11%] [G loss: 2.899145]\n",
      "2008 [D loss: 0.236600, acc.: 89.84%] [G loss: 2.921183]\n",
      "2009 [D loss: 0.236682, acc.: 89.84%] [G loss: 3.060474]\n",
      "2010 [D loss: 0.253274, acc.: 86.33%] [G loss: 3.322021]\n",
      "2011 [D loss: 0.192248, acc.: 91.41%] [G loss: 3.018130]\n",
      "2012 [D loss: 0.215835, acc.: 89.84%] [G loss: 2.827931]\n",
      "2013 [D loss: 0.220518, acc.: 89.45%] [G loss: 2.863508]\n",
      "2014 [D loss: 0.204163, acc.: 91.41%] [G loss: 3.514803]\n",
      "2015 [D loss: 0.203504, acc.: 92.58%] [G loss: 3.154964]\n",
      "2016 [D loss: 0.217334, acc.: 89.84%] [G loss: 2.918327]\n",
      "2017 [D loss: 0.195157, acc.: 92.19%] [G loss: 3.068993]\n",
      "2018 [D loss: 0.231090, acc.: 89.06%] [G loss: 3.135685]\n",
      "2019 [D loss: 0.234322, acc.: 90.62%] [G loss: 3.095814]\n",
      "2020 [D loss: 0.202547, acc.: 92.58%] [G loss: 3.205386]\n",
      "2021 [D loss: 0.225138, acc.: 91.02%] [G loss: 2.917205]\n",
      "2022 [D loss: 0.248775, acc.: 89.45%] [G loss: 2.732329]\n",
      "2023 [D loss: 0.188000, acc.: 92.58%] [G loss: 3.052997]\n",
      "2024 [D loss: 0.201500, acc.: 91.41%] [G loss: 2.761076]\n",
      "2025 [D loss: 0.217625, acc.: 91.02%] [G loss: 2.829118]\n",
      "2026 [D loss: 0.211897, acc.: 89.45%] [G loss: 3.057563]\n",
      "2027 [D loss: 0.209074, acc.: 89.84%] [G loss: 2.646793]\n",
      "2028 [D loss: 0.184670, acc.: 92.97%] [G loss: 2.800539]\n",
      "2029 [D loss: 0.223301, acc.: 89.45%] [G loss: 2.880044]\n",
      "2030 [D loss: 0.211330, acc.: 89.06%] [G loss: 2.999670]\n",
      "2031 [D loss: 0.220367, acc.: 89.84%] [G loss: 2.812921]\n",
      "2032 [D loss: 0.229035, acc.: 91.41%] [G loss: 3.007291]\n",
      "2033 [D loss: 0.225667, acc.: 90.62%] [G loss: 3.145912]\n",
      "2034 [D loss: 0.222765, acc.: 90.62%] [G loss: 3.296925]\n",
      "2035 [D loss: 0.203958, acc.: 92.97%] [G loss: 2.998390]\n",
      "2036 [D loss: 0.201878, acc.: 91.80%] [G loss: 3.088121]\n",
      "2037 [D loss: 0.191468, acc.: 92.19%] [G loss: 2.723144]\n",
      "2038 [D loss: 0.241175, acc.: 88.28%] [G loss: 2.998967]\n",
      "2039 [D loss: 0.190935, acc.: 90.23%] [G loss: 3.153767]\n",
      "2040 [D loss: 0.213166, acc.: 90.62%] [G loss: 2.901671]\n",
      "2041 [D loss: 0.203624, acc.: 92.19%] [G loss: 2.714038]\n",
      "2042 [D loss: 0.207163, acc.: 92.19%] [G loss: 3.178236]\n",
      "2043 [D loss: 0.206361, acc.: 91.02%] [G loss: 3.187853]\n",
      "2044 [D loss: 0.224526, acc.: 89.45%] [G loss: 2.922708]\n",
      "2045 [D loss: 0.196852, acc.: 92.58%] [G loss: 2.873978]\n",
      "2046 [D loss: 0.232806, acc.: 89.06%] [G loss: 3.058112]\n",
      "2047 [D loss: 0.207631, acc.: 91.41%] [G loss: 3.015543]\n",
      "2048 [D loss: 0.218794, acc.: 90.23%] [G loss: 2.777681]\n",
      "2049 [D loss: 0.224742, acc.: 91.41%] [G loss: 3.263552]\n",
      "2050 [D loss: 0.204425, acc.: 92.19%] [G loss: 3.254606]\n",
      "2051 [D loss: 0.225003, acc.: 89.06%] [G loss: 2.818990]\n",
      "2052 [D loss: 0.205807, acc.: 91.02%] [G loss: 2.885660]\n",
      "2053 [D loss: 0.211609, acc.: 90.62%] [G loss: 2.940320]\n",
      "2054 [D loss: 0.194859, acc.: 91.80%] [G loss: 3.122603]\n",
      "2055 [D loss: 0.230187, acc.: 89.84%] [G loss: 2.999328]\n",
      "2056 [D loss: 0.200907, acc.: 91.41%] [G loss: 3.275908]\n",
      "2057 [D loss: 0.174511, acc.: 93.75%] [G loss: 3.041347]\n",
      "2058 [D loss: 0.241045, acc.: 88.28%] [G loss: 3.083277]\n",
      "2059 [D loss: 0.221382, acc.: 90.23%] [G loss: 2.989101]\n",
      "2060 [D loss: 0.230688, acc.: 90.62%] [G loss: 2.733225]\n",
      "2061 [D loss: 0.218356, acc.: 91.02%] [G loss: 3.064336]\n",
      "2062 [D loss: 0.205310, acc.: 90.62%] [G loss: 3.164499]\n",
      "2063 [D loss: 0.217517, acc.: 90.23%] [G loss: 2.959534]\n",
      "2064 [D loss: 0.209103, acc.: 89.84%] [G loss: 2.785157]\n",
      "2065 [D loss: 0.220797, acc.: 89.84%] [G loss: 3.088568]\n",
      "2066 [D loss: 0.218678, acc.: 89.06%] [G loss: 2.965590]\n",
      "2067 [D loss: 0.218460, acc.: 90.62%] [G loss: 3.165111]\n",
      "2068 [D loss: 0.202094, acc.: 91.41%] [G loss: 2.885072]\n",
      "2069 [D loss: 0.233878, acc.: 89.06%] [G loss: 2.964550]\n",
      "2070 [D loss: 0.230096, acc.: 89.06%] [G loss: 3.264132]\n",
      "2071 [D loss: 0.202082, acc.: 91.02%] [G loss: 3.065031]\n",
      "2072 [D loss: 0.197201, acc.: 92.97%] [G loss: 2.861154]\n",
      "2073 [D loss: 0.211196, acc.: 91.02%] [G loss: 3.042985]\n",
      "2074 [D loss: 0.199829, acc.: 92.19%] [G loss: 3.153430]\n",
      "2075 [D loss: 0.239882, acc.: 89.06%] [G loss: 2.947021]\n",
      "2076 [D loss: 0.210346, acc.: 90.62%] [G loss: 2.933120]\n",
      "2077 [D loss: 0.233433, acc.: 89.45%] [G loss: 3.156827]\n",
      "2078 [D loss: 0.223979, acc.: 91.41%] [G loss: 3.097995]\n",
      "2079 [D loss: 0.199533, acc.: 91.02%] [G loss: 3.011507]\n",
      "2080 [D loss: 0.237192, acc.: 93.75%] [G loss: 2.879948]\n",
      "2081 [D loss: 0.209016, acc.: 89.84%] [G loss: 2.981643]\n",
      "2082 [D loss: 0.210499, acc.: 91.02%] [G loss: 3.069467]\n",
      "2083 [D loss: 0.186762, acc.: 92.58%] [G loss: 2.962605]\n",
      "2084 [D loss: 0.223607, acc.: 90.23%] [G loss: 3.137838]\n",
      "2085 [D loss: 0.227120, acc.: 90.23%] [G loss: 3.250710]\n",
      "2086 [D loss: 0.173749, acc.: 92.58%] [G loss: 3.136832]\n",
      "2087 [D loss: 0.217743, acc.: 90.23%] [G loss: 2.966366]\n",
      "2088 [D loss: 0.189030, acc.: 92.58%] [G loss: 3.002469]\n",
      "2089 [D loss: 0.180895, acc.: 91.41%] [G loss: 2.930001]\n",
      "2090 [D loss: 0.214791, acc.: 90.23%] [G loss: 2.914372]\n",
      "2091 [D loss: 0.225346, acc.: 90.23%] [G loss: 3.156355]\n",
      "2092 [D loss: 0.230448, acc.: 89.84%] [G loss: 2.878379]\n",
      "2093 [D loss: 0.227333, acc.: 89.84%] [G loss: 2.932276]\n",
      "2094 [D loss: 0.212134, acc.: 91.80%] [G loss: 3.019641]\n",
      "2095 [D loss: 0.223089, acc.: 89.45%] [G loss: 3.226598]\n",
      "2096 [D loss: 0.202033, acc.: 91.80%] [G loss: 3.093593]\n",
      "2097 [D loss: 0.192539, acc.: 93.36%] [G loss: 3.084801]\n",
      "2098 [D loss: 0.199631, acc.: 91.80%] [G loss: 3.152207]\n",
      "2099 [D loss: 0.213665, acc.: 91.02%] [G loss: 2.911082]\n",
      "2100 [D loss: 0.193291, acc.: 92.97%] [G loss: 2.970714]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "2101 [D loss: 0.212174, acc.: 91.41%] [G loss: 2.963308]\n",
      "2102 [D loss: 0.222805, acc.: 91.02%] [G loss: 2.772983]\n",
      "2103 [D loss: 0.203215, acc.: 90.62%] [G loss: 3.055772]\n",
      "2104 [D loss: 0.204181, acc.: 89.45%] [G loss: 3.133784]\n",
      "2105 [D loss: 0.255046, acc.: 88.67%] [G loss: 3.105575]\n",
      "2106 [D loss: 0.239152, acc.: 90.23%] [G loss: 3.258627]\n",
      "2107 [D loss: 0.234544, acc.: 89.45%] [G loss: 2.949957]\n",
      "2108 [D loss: 0.238235, acc.: 89.06%] [G loss: 2.957565]\n",
      "2109 [D loss: 0.211320, acc.: 91.02%] [G loss: 2.940480]\n",
      "2110 [D loss: 0.229975, acc.: 89.84%] [G loss: 3.131108]\n",
      "2111 [D loss: 0.203659, acc.: 91.41%] [G loss: 2.889274]\n",
      "2112 [D loss: 0.193573, acc.: 94.14%] [G loss: 3.400037]\n",
      "2113 [D loss: 0.211470, acc.: 92.19%] [G loss: 3.135469]\n",
      "2114 [D loss: 0.181299, acc.: 91.41%] [G loss: 2.912565]\n",
      "2115 [D loss: 0.193819, acc.: 92.58%] [G loss: 3.100419]\n",
      "2116 [D loss: 0.202559, acc.: 91.02%] [G loss: 3.205713]\n",
      "2117 [D loss: 0.201604, acc.: 91.41%] [G loss: 3.130931]\n",
      "2118 [D loss: 0.196537, acc.: 92.19%] [G loss: 2.943183]\n",
      "2119 [D loss: 0.215460, acc.: 90.62%] [G loss: 2.890545]\n",
      "2120 [D loss: 0.230329, acc.: 91.02%] [G loss: 3.157864]\n",
      "2121 [D loss: 0.217433, acc.: 89.84%] [G loss: 3.205979]\n",
      "2122 [D loss: 0.216551, acc.: 91.02%] [G loss: 2.952534]\n",
      "2123 [D loss: 0.185005, acc.: 91.02%] [G loss: 3.111631]\n",
      "2124 [D loss: 0.218998, acc.: 91.41%] [G loss: 2.956584]\n",
      "2125 [D loss: 0.203495, acc.: 92.19%] [G loss: 3.134561]\n",
      "2126 [D loss: 0.200067, acc.: 92.58%] [G loss: 3.115252]\n",
      "2127 [D loss: 0.230436, acc.: 88.67%] [G loss: 2.964697]\n",
      "2128 [D loss: 0.212751, acc.: 91.80%] [G loss: 3.005307]\n",
      "2129 [D loss: 0.212141, acc.: 90.62%] [G loss: 2.860380]\n",
      "2130 [D loss: 0.188540, acc.: 92.58%] [G loss: 3.008196]\n",
      "2131 [D loss: 0.268201, acc.: 88.67%] [G loss: 2.974958]\n",
      "2132 [D loss: 0.234003, acc.: 87.50%] [G loss: 3.031421]\n",
      "2133 [D loss: 0.227574, acc.: 89.06%] [G loss: 2.943660]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2134 [D loss: 0.219463, acc.: 91.41%] [G loss: 3.073784]\n",
      "2135 [D loss: 0.183985, acc.: 93.36%] [G loss: 3.085935]\n",
      "2136 [D loss: 0.230990, acc.: 89.45%] [G loss: 3.104795]\n",
      "2137 [D loss: 0.210798, acc.: 91.02%] [G loss: 3.005907]\n",
      "2138 [D loss: 0.205389, acc.: 90.62%] [G loss: 2.974116]\n",
      "2139 [D loss: 0.193206, acc.: 92.19%] [G loss: 3.057049]\n",
      "2140 [D loss: 0.249742, acc.: 89.84%] [G loss: 3.307632]\n",
      "2141 [D loss: 0.207872, acc.: 90.62%] [G loss: 3.187674]\n",
      "2142 [D loss: 0.216401, acc.: 90.62%] [G loss: 3.283356]\n",
      "2143 [D loss: 0.216751, acc.: 89.45%] [G loss: 2.801646]\n",
      "2144 [D loss: 0.205722, acc.: 90.23%] [G loss: 2.974492]\n",
      "2145 [D loss: 0.214554, acc.: 90.62%] [G loss: 2.996907]\n",
      "2146 [D loss: 0.195574, acc.: 92.19%] [G loss: 3.192800]\n",
      "2147 [D loss: 0.205726, acc.: 92.19%] [G loss: 3.199484]\n",
      "2148 [D loss: 0.192787, acc.: 91.41%] [G loss: 3.043826]\n",
      "2149 [D loss: 0.178636, acc.: 92.97%] [G loss: 2.999194]\n",
      "2150 [D loss: 0.220410, acc.: 92.58%] [G loss: 3.365135]\n",
      "2151 [D loss: 0.209948, acc.: 89.84%] [G loss: 2.945966]\n",
      "2152 [D loss: 0.223594, acc.: 91.02%] [G loss: 2.722994]\n",
      "2153 [D loss: 0.166822, acc.: 94.53%] [G loss: 3.005914]\n",
      "2154 [D loss: 0.248487, acc.: 89.06%] [G loss: 3.223574]\n",
      "2155 [D loss: 0.225700, acc.: 89.06%] [G loss: 2.985037]\n",
      "2156 [D loss: 0.185994, acc.: 91.02%] [G loss: 2.988265]\n",
      "2157 [D loss: 0.181142, acc.: 92.97%] [G loss: 2.923306]\n",
      "2158 [D loss: 0.190009, acc.: 92.97%] [G loss: 2.870867]\n",
      "2159 [D loss: 0.222051, acc.: 89.84%] [G loss: 3.109346]\n",
      "2160 [D loss: 0.198833, acc.: 92.58%] [G loss: 3.053412]\n",
      "2161 [D loss: 0.214517, acc.: 89.45%] [G loss: 3.269352]\n",
      "2162 [D loss: 0.213928, acc.: 90.62%] [G loss: 3.262241]\n",
      "2163 [D loss: 0.164181, acc.: 93.36%] [G loss: 3.452131]\n",
      "2164 [D loss: 0.186942, acc.: 91.41%] [G loss: 3.393573]\n",
      "2165 [D loss: 0.196402, acc.: 91.41%] [G loss: 3.128435]\n",
      "2166 [D loss: 0.220216, acc.: 91.41%] [G loss: 3.154410]\n",
      "2167 [D loss: 0.215546, acc.: 89.84%] [G loss: 3.236310]\n",
      "2168 [D loss: 0.244631, acc.: 87.89%] [G loss: 3.252640]\n",
      "2169 [D loss: 0.215851, acc.: 90.62%] [G loss: 3.502826]\n",
      "2170 [D loss: 0.208757, acc.: 90.62%] [G loss: 3.202156]\n",
      "2171 [D loss: 0.189462, acc.: 91.80%] [G loss: 3.262826]\n",
      "2172 [D loss: 0.199853, acc.: 91.80%] [G loss: 2.883604]\n",
      "2173 [D loss: 0.205224, acc.: 91.02%] [G loss: 2.985588]\n",
      "2174 [D loss: 0.222320, acc.: 91.02%] [G loss: 2.766974]\n",
      "2175 [D loss: 0.203228, acc.: 91.02%] [G loss: 3.223614]\n",
      "2176 [D loss: 0.192370, acc.: 92.19%] [G loss: 3.037737]\n",
      "2177 [D loss: 0.190751, acc.: 93.36%] [G loss: 3.328560]\n",
      "2178 [D loss: 0.215089, acc.: 89.84%] [G loss: 2.821841]\n",
      "2179 [D loss: 0.215428, acc.: 90.62%] [G loss: 2.953265]\n",
      "2180 [D loss: 0.196691, acc.: 90.62%] [G loss: 3.190347]\n",
      "2181 [D loss: 0.187850, acc.: 92.97%] [G loss: 3.255147]\n",
      "2182 [D loss: 0.202886, acc.: 92.19%] [G loss: 3.237001]\n",
      "2183 [D loss: 0.196394, acc.: 91.41%] [G loss: 3.073750]\n",
      "2184 [D loss: 0.234055, acc.: 90.23%] [G loss: 3.021050]\n",
      "2185 [D loss: 0.207113, acc.: 92.19%] [G loss: 3.063568]\n",
      "2186 [D loss: 0.196630, acc.: 92.19%] [G loss: 3.066818]\n",
      "2187 [D loss: 0.215422, acc.: 91.02%] [G loss: 2.944604]\n",
      "2188 [D loss: 0.185279, acc.: 91.80%] [G loss: 3.279956]\n",
      "2189 [D loss: 0.214277, acc.: 89.84%] [G loss: 3.192859]\n",
      "2190 [D loss: 0.230139, acc.: 89.84%] [G loss: 3.176060]\n",
      "2191 [D loss: 0.220472, acc.: 92.19%] [G loss: 3.460794]\n",
      "2192 [D loss: 0.269076, acc.: 87.89%] [G loss: 3.005270]\n",
      "2193 [D loss: 0.224041, acc.: 89.06%] [G loss: 3.156213]\n",
      "2194 [D loss: 0.233129, acc.: 88.67%] [G loss: 3.105529]\n",
      "2195 [D loss: 0.243977, acc.: 91.02%] [G loss: 3.186563]\n",
      "2196 [D loss: 0.198355, acc.: 89.84%] [G loss: 3.045789]\n",
      "2197 [D loss: 0.233577, acc.: 89.84%] [G loss: 3.220819]\n",
      "2198 [D loss: 0.183694, acc.: 91.80%] [G loss: 3.166556]\n",
      "2199 [D loss: 0.247516, acc.: 90.23%] [G loss: 2.880817]\n",
      "2200 [D loss: 0.199606, acc.: 91.80%] [G loss: 2.974819]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "2201 [D loss: 0.214164, acc.: 89.45%] [G loss: 2.970410]\n",
      "2202 [D loss: 0.198646, acc.: 91.41%] [G loss: 3.131428]\n",
      "2203 [D loss: 0.242383, acc.: 88.67%] [G loss: 3.092318]\n",
      "2204 [D loss: 0.199446, acc.: 91.02%] [G loss: 2.993641]\n",
      "2205 [D loss: 0.230475, acc.: 90.23%] [G loss: 3.354920]\n",
      "2206 [D loss: 0.189246, acc.: 92.19%] [G loss: 2.883416]\n",
      "2207 [D loss: 0.262577, acc.: 88.28%] [G loss: 2.798587]\n",
      "2208 [D loss: 0.168712, acc.: 92.58%] [G loss: 3.051610]\n",
      "2209 [D loss: 0.196900, acc.: 91.02%] [G loss: 3.162792]\n",
      "2210 [D loss: 0.210585, acc.: 89.06%] [G loss: 3.237046]\n",
      "2211 [D loss: 0.229091, acc.: 91.02%] [G loss: 3.241120]\n",
      "2212 [D loss: 0.224574, acc.: 89.45%] [G loss: 3.099832]\n",
      "2213 [D loss: 0.238942, acc.: 90.23%] [G loss: 3.226493]\n",
      "2214 [D loss: 0.238385, acc.: 90.62%] [G loss: 3.300359]\n",
      "2215 [D loss: 0.203679, acc.: 91.80%] [G loss: 3.039148]\n",
      "2216 [D loss: 0.188401, acc.: 93.75%] [G loss: 3.058748]\n",
      "2217 [D loss: 0.214514, acc.: 91.80%] [G loss: 3.049688]\n",
      "2218 [D loss: 0.197041, acc.: 92.58%] [G loss: 3.088702]\n",
      "2219 [D loss: 0.177465, acc.: 92.58%] [G loss: 3.364819]\n",
      "2220 [D loss: 0.184712, acc.: 92.97%] [G loss: 3.072927]\n",
      "2221 [D loss: 0.184396, acc.: 93.36%] [G loss: 3.166329]\n",
      "2222 [D loss: 0.213916, acc.: 89.84%] [G loss: 3.230324]\n",
      "2223 [D loss: 0.215682, acc.: 91.02%] [G loss: 3.153645]\n",
      "2224 [D loss: 0.194210, acc.: 92.58%] [G loss: 2.916576]\n",
      "2225 [D loss: 0.222380, acc.: 90.23%] [G loss: 3.132199]\n",
      "2226 [D loss: 0.180301, acc.: 92.97%] [G loss: 3.153386]\n",
      "2227 [D loss: 0.224921, acc.: 89.06%] [G loss: 3.281483]\n",
      "2228 [D loss: 0.227966, acc.: 91.41%] [G loss: 3.190290]\n",
      "2229 [D loss: 0.229226, acc.: 90.62%] [G loss: 2.998214]\n",
      "2230 [D loss: 0.206556, acc.: 91.80%] [G loss: 2.782921]\n",
      "2231 [D loss: 0.206996, acc.: 92.58%] [G loss: 3.204694]\n",
      "2232 [D loss: 0.193263, acc.: 92.19%] [G loss: 3.152663]\n",
      "2233 [D loss: 0.219610, acc.: 91.80%] [G loss: 3.185849]\n",
      "2234 [D loss: 0.196603, acc.: 90.23%] [G loss: 3.133090]\n",
      "2235 [D loss: 0.210255, acc.: 91.80%] [G loss: 3.149138]\n",
      "2236 [D loss: 0.197647, acc.: 92.19%] [G loss: 3.052918]\n",
      "2237 [D loss: 0.192059, acc.: 91.41%] [G loss: 2.886629]\n",
      "2238 [D loss: 0.195992, acc.: 91.80%] [G loss: 3.015599]\n",
      "2239 [D loss: 0.161908, acc.: 93.36%] [G loss: 2.905667]\n",
      "2240 [D loss: 0.191604, acc.: 91.41%] [G loss: 2.745752]\n",
      "2241 [D loss: 0.185368, acc.: 92.58%] [G loss: 3.226386]\n",
      "2242 [D loss: 0.191469, acc.: 91.02%] [G loss: 3.239865]\n",
      "2243 [D loss: 0.228322, acc.: 91.41%] [G loss: 3.238085]\n",
      "2244 [D loss: 0.212995, acc.: 90.23%] [G loss: 3.278016]\n",
      "2245 [D loss: 0.199487, acc.: 92.58%] [G loss: 3.315711]\n",
      "2246 [D loss: 0.221928, acc.: 90.62%] [G loss: 3.321210]\n",
      "2247 [D loss: 0.247248, acc.: 89.45%] [G loss: 3.368126]\n",
      "2248 [D loss: 0.203919, acc.: 89.06%] [G loss: 2.953345]\n",
      "2249 [D loss: 0.218061, acc.: 90.23%] [G loss: 3.133359]\n",
      "2250 [D loss: 0.219641, acc.: 89.84%] [G loss: 3.143189]\n",
      "2251 [D loss: 0.263507, acc.: 89.06%] [G loss: 2.830932]\n",
      "2252 [D loss: 0.205466, acc.: 91.80%] [G loss: 3.304794]\n",
      "2253 [D loss: 0.206004, acc.: 92.19%] [G loss: 3.030130]\n",
      "2254 [D loss: 0.208348, acc.: 91.80%] [G loss: 2.903916]\n",
      "2255 [D loss: 0.199266, acc.: 90.62%] [G loss: 3.058744]\n",
      "2256 [D loss: 0.205712, acc.: 91.41%] [G loss: 3.401081]\n",
      "2257 [D loss: 0.166916, acc.: 94.14%] [G loss: 3.287470]\n",
      "2258 [D loss: 0.207220, acc.: 91.02%] [G loss: 2.922915]\n",
      "2259 [D loss: 0.207338, acc.: 90.23%] [G loss: 3.113737]\n",
      "2260 [D loss: 0.212421, acc.: 90.23%] [G loss: 3.321446]\n",
      "2261 [D loss: 0.218195, acc.: 91.02%] [G loss: 3.351888]\n",
      "2262 [D loss: 0.214410, acc.: 90.62%] [G loss: 3.104846]\n",
      "2263 [D loss: 0.214302, acc.: 91.80%] [G loss: 2.800261]\n",
      "2264 [D loss: 0.217990, acc.: 90.23%] [G loss: 3.149916]\n",
      "2265 [D loss: 0.215540, acc.: 90.62%] [G loss: 3.316911]\n",
      "2266 [D loss: 0.212065, acc.: 91.80%] [G loss: 3.209686]\n",
      "2267 [D loss: 0.177918, acc.: 93.75%] [G loss: 2.807775]\n",
      "2268 [D loss: 0.235685, acc.: 89.06%] [G loss: 2.931401]\n",
      "2269 [D loss: 0.219237, acc.: 91.02%] [G loss: 3.332932]\n",
      "2270 [D loss: 0.217973, acc.: 91.02%] [G loss: 3.075601]\n",
      "2271 [D loss: 0.214707, acc.: 91.80%] [G loss: 3.073035]\n",
      "2272 [D loss: 0.218887, acc.: 91.41%] [G loss: 3.439435]\n",
      "2273 [D loss: 0.213988, acc.: 91.80%] [G loss: 3.361391]\n",
      "2274 [D loss: 0.231043, acc.: 88.67%] [G loss: 3.037456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2275 [D loss: 0.224652, acc.: 89.06%] [G loss: 3.281452]\n",
      "2276 [D loss: 0.232231, acc.: 91.41%] [G loss: 3.170830]\n",
      "2277 [D loss: 0.200542, acc.: 91.02%] [G loss: 3.295934]\n",
      "2278 [D loss: 0.241018, acc.: 91.02%] [G loss: 3.066749]\n",
      "2279 [D loss: 0.182169, acc.: 91.41%] [G loss: 3.143420]\n",
      "2280 [D loss: 0.180503, acc.: 92.97%] [G loss: 3.171266]\n",
      "2281 [D loss: 0.239282, acc.: 91.41%] [G loss: 3.222250]\n",
      "2282 [D loss: 0.196424, acc.: 91.80%] [G loss: 3.010721]\n",
      "2283 [D loss: 0.230556, acc.: 90.62%] [G loss: 3.048601]\n",
      "2284 [D loss: 0.217047, acc.: 91.02%] [G loss: 3.350929]\n",
      "2285 [D loss: 0.221763, acc.: 91.02%] [G loss: 3.007643]\n",
      "2286 [D loss: 0.223204, acc.: 89.84%] [G loss: 3.315837]\n",
      "2287 [D loss: 0.215940, acc.: 91.41%] [G loss: 3.143287]\n",
      "2288 [D loss: 0.201180, acc.: 91.80%] [G loss: 3.110501]\n",
      "2289 [D loss: 0.205665, acc.: 92.97%] [G loss: 2.979028]\n",
      "2290 [D loss: 0.226960, acc.: 89.45%] [G loss: 3.211551]\n",
      "2291 [D loss: 0.183308, acc.: 91.80%] [G loss: 3.451357]\n",
      "2292 [D loss: 0.184296, acc.: 91.80%] [G loss: 3.032328]\n",
      "2293 [D loss: 0.242464, acc.: 87.89%] [G loss: 3.375021]\n",
      "2294 [D loss: 0.229442, acc.: 92.19%] [G loss: 3.379274]\n",
      "2295 [D loss: 0.252194, acc.: 88.28%] [G loss: 2.843152]\n",
      "2296 [D loss: 0.201987, acc.: 91.41%] [G loss: 3.217086]\n",
      "2297 [D loss: 0.224298, acc.: 89.84%] [G loss: 2.974423]\n",
      "2298 [D loss: 0.190848, acc.: 92.19%] [G loss: 3.115346]\n",
      "2299 [D loss: 0.181267, acc.: 93.75%] [G loss: 3.183660]\n",
      "2300 [D loss: 0.193533, acc.: 91.41%] [G loss: 3.102809]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "2301 [D loss: 0.200169, acc.: 90.62%] [G loss: 3.290485]\n",
      "2302 [D loss: 0.217160, acc.: 91.80%] [G loss: 2.987156]\n",
      "2303 [D loss: 0.245852, acc.: 89.45%] [G loss: 3.092485]\n",
      "2304 [D loss: 0.215777, acc.: 90.62%] [G loss: 3.143606]\n",
      "2305 [D loss: 0.223059, acc.: 89.84%] [G loss: 3.030327]\n",
      "2306 [D loss: 0.210156, acc.: 90.62%] [G loss: 3.037512]\n",
      "2307 [D loss: 0.216930, acc.: 91.41%] [G loss: 3.129269]\n",
      "2308 [D loss: 0.200610, acc.: 91.41%] [G loss: 3.355170]\n",
      "2309 [D loss: 0.191405, acc.: 92.97%] [G loss: 3.095017]\n",
      "2310 [D loss: 0.198922, acc.: 92.19%] [G loss: 3.225639]\n",
      "2311 [D loss: 0.186665, acc.: 92.19%] [G loss: 3.149856]\n",
      "2312 [D loss: 0.210609, acc.: 89.84%] [G loss: 3.253220]\n",
      "2313 [D loss: 0.241617, acc.: 89.45%] [G loss: 3.055662]\n",
      "2314 [D loss: 0.213965, acc.: 91.80%] [G loss: 2.799289]\n",
      "2315 [D loss: 0.233669, acc.: 91.41%] [G loss: 2.951923]\n",
      "2316 [D loss: 0.201513, acc.: 92.19%] [G loss: 3.066247]\n",
      "2317 [D loss: 0.227814, acc.: 89.45%] [G loss: 2.974270]\n",
      "2318 [D loss: 0.195693, acc.: 93.36%] [G loss: 3.156769]\n",
      "2319 [D loss: 0.207755, acc.: 89.84%] [G loss: 3.102733]\n",
      "2320 [D loss: 0.230668, acc.: 89.45%] [G loss: 3.464842]\n",
      "2321 [D loss: 0.204514, acc.: 91.02%] [G loss: 3.236957]\n",
      "2322 [D loss: 0.224634, acc.: 90.62%] [G loss: 3.049302]\n",
      "2323 [D loss: 0.204250, acc.: 90.62%] [G loss: 3.062972]\n",
      "2324 [D loss: 0.208408, acc.: 92.19%] [G loss: 3.262151]\n",
      "2325 [D loss: 0.210975, acc.: 90.23%] [G loss: 2.971018]\n",
      "2326 [D loss: 0.203716, acc.: 91.02%] [G loss: 2.943699]\n",
      "2327 [D loss: 0.172464, acc.: 91.41%] [G loss: 3.042873]\n",
      "2328 [D loss: 0.198290, acc.: 91.41%] [G loss: 3.351022]\n",
      "2329 [D loss: 0.194276, acc.: 90.62%] [G loss: 2.896100]\n",
      "2330 [D loss: 0.201174, acc.: 92.58%] [G loss: 2.860016]\n",
      "2331 [D loss: 0.205846, acc.: 91.41%] [G loss: 3.144577]\n",
      "2332 [D loss: 0.219217, acc.: 91.02%] [G loss: 3.156078]\n",
      "2333 [D loss: 0.212027, acc.: 90.62%] [G loss: 3.313425]\n",
      "2334 [D loss: 0.203410, acc.: 91.41%] [G loss: 3.340256]\n",
      "2335 [D loss: 0.219717, acc.: 91.02%] [G loss: 2.913776]\n",
      "2336 [D loss: 0.211460, acc.: 91.41%] [G loss: 3.133931]\n",
      "2337 [D loss: 0.211915, acc.: 91.80%] [G loss: 3.331097]\n",
      "2338 [D loss: 0.221085, acc.: 91.02%] [G loss: 3.195694]\n",
      "2339 [D loss: 0.228332, acc.: 90.62%] [G loss: 3.273916]\n",
      "2340 [D loss: 0.216362, acc.: 91.41%] [G loss: 3.210099]\n",
      "2341 [D loss: 0.144202, acc.: 93.75%] [G loss: 3.215620]\n",
      "2342 [D loss: 0.248509, acc.: 89.06%] [G loss: 3.492260]\n",
      "2343 [D loss: 0.224643, acc.: 90.62%] [G loss: 2.826189]\n",
      "2344 [D loss: 0.212438, acc.: 90.62%] [G loss: 3.183864]\n",
      "2345 [D loss: 0.228018, acc.: 89.06%] [G loss: 3.456525]\n",
      "2346 [D loss: 0.209689, acc.: 91.02%] [G loss: 3.281446]\n",
      "2347 [D loss: 0.201463, acc.: 92.19%] [G loss: 3.068725]\n",
      "2348 [D loss: 0.214071, acc.: 91.02%] [G loss: 3.288313]\n",
      "2349 [D loss: 0.213276, acc.: 91.41%] [G loss: 3.051543]\n",
      "2350 [D loss: 0.201935, acc.: 90.62%] [G loss: 3.029984]\n",
      "2351 [D loss: 0.196541, acc.: 91.80%] [G loss: 3.139462]\n",
      "2352 [D loss: 0.205000, acc.: 91.41%] [G loss: 2.998284]\n",
      "2353 [D loss: 0.200198, acc.: 90.23%] [G loss: 3.172865]\n",
      "2354 [D loss: 0.207812, acc.: 91.02%] [G loss: 2.905857]\n",
      "2355 [D loss: 0.201082, acc.: 91.80%] [G loss: 3.249576]\n",
      "2356 [D loss: 0.194662, acc.: 92.97%] [G loss: 3.415856]\n",
      "2357 [D loss: 0.221283, acc.: 91.80%] [G loss: 3.161609]\n",
      "2358 [D loss: 0.197834, acc.: 90.62%] [G loss: 3.214412]\n",
      "2359 [D loss: 0.183965, acc.: 93.75%] [G loss: 3.354725]\n",
      "2360 [D loss: 0.210056, acc.: 90.62%] [G loss: 3.038108]\n",
      "2361 [D loss: 0.213659, acc.: 91.02%] [G loss: 3.134238]\n",
      "2362 [D loss: 0.193184, acc.: 91.02%] [G loss: 3.398206]\n",
      "2363 [D loss: 0.221361, acc.: 89.84%] [G loss: 3.149602]\n",
      "2364 [D loss: 0.223840, acc.: 89.84%] [G loss: 3.270949]\n",
      "2365 [D loss: 0.193630, acc.: 91.80%] [G loss: 3.160594]\n",
      "2366 [D loss: 0.197174, acc.: 91.80%] [G loss: 2.982676]\n",
      "2367 [D loss: 0.192134, acc.: 92.19%] [G loss: 3.368881]\n",
      "2368 [D loss: 0.192122, acc.: 92.97%] [G loss: 3.496597]\n",
      "2369 [D loss: 0.203378, acc.: 91.41%] [G loss: 3.120941]\n",
      "2370 [D loss: 0.167646, acc.: 92.97%] [G loss: 3.243885]\n",
      "2371 [D loss: 0.201617, acc.: 91.41%] [G loss: 3.039000]\n",
      "2372 [D loss: 0.192642, acc.: 91.41%] [G loss: 3.539606]\n",
      "2373 [D loss: 0.187543, acc.: 91.41%] [G loss: 3.084395]\n",
      "2374 [D loss: 0.216785, acc.: 89.45%] [G loss: 2.866254]\n",
      "2375 [D loss: 0.176198, acc.: 93.36%] [G loss: 3.265710]\n",
      "2376 [D loss: 0.208163, acc.: 91.02%] [G loss: 2.932776]\n",
      "2377 [D loss: 0.177251, acc.: 94.14%] [G loss: 2.904549]\n",
      "2378 [D loss: 0.201540, acc.: 91.41%] [G loss: 3.280902]\n",
      "2379 [D loss: 0.191097, acc.: 93.36%] [G loss: 3.325974]\n",
      "2380 [D loss: 0.219720, acc.: 90.62%] [G loss: 3.224741]\n",
      "2381 [D loss: 0.231285, acc.: 89.84%] [G loss: 3.293183]\n",
      "2382 [D loss: 0.246082, acc.: 89.45%] [G loss: 3.175748]\n",
      "2383 [D loss: 0.212936, acc.: 91.02%] [G loss: 3.366607]\n",
      "2384 [D loss: 0.231714, acc.: 90.23%] [G loss: 3.309894]\n",
      "2385 [D loss: 0.251709, acc.: 89.06%] [G loss: 2.969955]\n",
      "2386 [D loss: 0.213186, acc.: 90.62%] [G loss: 3.250285]\n",
      "2387 [D loss: 0.198011, acc.: 92.19%] [G loss: 3.304995]\n",
      "2388 [D loss: 0.192422, acc.: 92.97%] [G loss: 3.288672]\n",
      "2389 [D loss: 0.187682, acc.: 91.41%] [G loss: 3.119811]\n",
      "2390 [D loss: 0.214406, acc.: 90.23%] [G loss: 3.201582]\n",
      "2391 [D loss: 0.170154, acc.: 92.58%] [G loss: 3.193919]\n",
      "2392 [D loss: 0.218286, acc.: 90.23%] [G loss: 3.271704]\n",
      "2393 [D loss: 0.187184, acc.: 93.36%] [G loss: 3.339520]\n",
      "2394 [D loss: 0.181697, acc.: 92.58%] [G loss: 3.092495]\n",
      "2395 [D loss: 0.210223, acc.: 89.45%] [G loss: 3.225168]\n",
      "2396 [D loss: 0.208790, acc.: 91.41%] [G loss: 3.181085]\n",
      "2397 [D loss: 0.192276, acc.: 90.23%] [G loss: 3.203866]\n",
      "2398 [D loss: 0.217767, acc.: 90.23%] [G loss: 3.356561]\n",
      "2399 [D loss: 0.202327, acc.: 90.62%] [G loss: 3.439448]\n",
      "2400 [D loss: 0.197257, acc.: 90.23%] [G loss: 3.033784]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "2401 [D loss: 0.193146, acc.: 91.80%] [G loss: 3.350808]\n",
      "2402 [D loss: 0.225173, acc.: 91.02%] [G loss: 3.160629]\n",
      "2403 [D loss: 0.204418, acc.: 89.06%] [G loss: 3.278052]\n",
      "2404 [D loss: 0.184959, acc.: 92.58%] [G loss: 3.311118]\n",
      "2405 [D loss: 0.214150, acc.: 91.41%] [G loss: 2.883829]\n",
      "2406 [D loss: 0.174320, acc.: 92.19%] [G loss: 3.005190]\n",
      "2407 [D loss: 0.220574, acc.: 89.45%] [G loss: 3.184665]\n",
      "2408 [D loss: 0.216841, acc.: 91.80%] [G loss: 3.190943]\n",
      "2409 [D loss: 0.192379, acc.: 91.41%] [G loss: 3.073936]\n",
      "2410 [D loss: 0.244522, acc.: 89.84%] [G loss: 3.350209]\n",
      "2411 [D loss: 0.177301, acc.: 92.19%] [G loss: 2.916061]\n",
      "2412 [D loss: 0.230482, acc.: 88.67%] [G loss: 3.341505]\n",
      "2413 [D loss: 0.202031, acc.: 91.80%] [G loss: 3.206302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2414 [D loss: 0.236851, acc.: 91.80%] [G loss: 3.003191]\n",
      "2415 [D loss: 0.232490, acc.: 90.62%] [G loss: 3.158238]\n",
      "2416 [D loss: 0.246724, acc.: 88.67%] [G loss: 3.507708]\n",
      "2417 [D loss: 0.230109, acc.: 89.84%] [G loss: 3.073065]\n",
      "2418 [D loss: 0.208288, acc.: 91.80%] [G loss: 3.440991]\n",
      "2419 [D loss: 0.206832, acc.: 90.62%] [G loss: 2.937757]\n",
      "2420 [D loss: 0.215984, acc.: 92.19%] [G loss: 3.241250]\n",
      "2421 [D loss: 0.199777, acc.: 92.19%] [G loss: 3.150856]\n",
      "2422 [D loss: 0.239070, acc.: 88.67%] [G loss: 3.275987]\n",
      "2423 [D loss: 0.204099, acc.: 91.80%] [G loss: 3.020484]\n",
      "2424 [D loss: 0.206793, acc.: 90.23%] [G loss: 3.021471]\n",
      "2425 [D loss: 0.193194, acc.: 92.19%] [G loss: 3.416215]\n",
      "2426 [D loss: 0.202800, acc.: 90.62%] [G loss: 3.360479]\n",
      "2427 [D loss: 0.196537, acc.: 90.62%] [G loss: 3.139064]\n",
      "2428 [D loss: 0.213665, acc.: 91.02%] [G loss: 3.355772]\n",
      "2429 [D loss: 0.194403, acc.: 92.19%] [G loss: 3.141553]\n",
      "2430 [D loss: 0.194737, acc.: 91.80%] [G loss: 3.173600]\n",
      "2431 [D loss: 0.198432, acc.: 92.19%] [G loss: 3.406659]\n",
      "2432 [D loss: 0.229933, acc.: 90.23%] [G loss: 3.268789]\n",
      "2433 [D loss: 0.206458, acc.: 91.41%] [G loss: 3.130156]\n",
      "2434 [D loss: 0.227533, acc.: 89.84%] [G loss: 3.232217]\n",
      "2435 [D loss: 0.231435, acc.: 89.06%] [G loss: 3.132851]\n",
      "2436 [D loss: 0.181618, acc.: 91.41%] [G loss: 3.308877]\n",
      "2437 [D loss: 0.257257, acc.: 88.67%] [G loss: 3.171911]\n",
      "2438 [D loss: 0.190218, acc.: 91.41%] [G loss: 3.264711]\n",
      "2439 [D loss: 0.203876, acc.: 91.02%] [G loss: 3.204381]\n",
      "2440 [D loss: 0.207163, acc.: 90.62%] [G loss: 3.140460]\n",
      "2441 [D loss: 0.177570, acc.: 93.36%] [G loss: 3.437878]\n",
      "2442 [D loss: 0.233261, acc.: 89.84%] [G loss: 3.162833]\n",
      "2443 [D loss: 0.185733, acc.: 92.19%] [G loss: 3.376172]\n",
      "2444 [D loss: 0.219204, acc.: 91.41%] [G loss: 2.955442]\n",
      "2445 [D loss: 0.209221, acc.: 91.41%] [G loss: 3.465775]\n",
      "2446 [D loss: 0.220662, acc.: 89.84%] [G loss: 2.956653]\n",
      "2447 [D loss: 0.236129, acc.: 87.89%] [G loss: 3.381776]\n",
      "2448 [D loss: 0.243496, acc.: 89.84%] [G loss: 3.166591]\n",
      "2449 [D loss: 0.187131, acc.: 91.41%] [G loss: 3.110697]\n",
      "2450 [D loss: 0.211260, acc.: 91.80%] [G loss: 3.342140]\n",
      "2451 [D loss: 0.224102, acc.: 92.19%] [G loss: 3.409406]\n",
      "2452 [D loss: 0.203818, acc.: 92.97%] [G loss: 3.037072]\n",
      "2453 [D loss: 0.213139, acc.: 92.19%] [G loss: 3.126156]\n",
      "2454 [D loss: 0.238215, acc.: 89.45%] [G loss: 3.145202]\n",
      "2455 [D loss: 0.205573, acc.: 90.23%] [G loss: 3.439660]\n",
      "2456 [D loss: 0.261647, acc.: 88.28%] [G loss: 3.363818]\n",
      "2457 [D loss: 0.196917, acc.: 91.41%] [G loss: 3.128800]\n",
      "2458 [D loss: 0.246358, acc.: 89.84%] [G loss: 3.000515]\n",
      "2459 [D loss: 0.200879, acc.: 91.41%] [G loss: 3.230189]\n",
      "2460 [D loss: 0.219478, acc.: 90.62%] [G loss: 3.219990]\n",
      "2461 [D loss: 0.212496, acc.: 90.23%] [G loss: 3.344606]\n",
      "2462 [D loss: 0.197407, acc.: 91.41%] [G loss: 3.232017]\n",
      "2463 [D loss: 0.190218, acc.: 91.41%] [G loss: 3.314260]\n",
      "2464 [D loss: 0.205822, acc.: 91.41%] [G loss: 3.219020]\n",
      "2465 [D loss: 0.225488, acc.: 89.84%] [G loss: 3.254303]\n",
      "2466 [D loss: 0.192480, acc.: 92.19%] [G loss: 3.285633]\n",
      "2467 [D loss: 0.207374, acc.: 91.41%] [G loss: 3.234631]\n",
      "2468 [D loss: 0.196224, acc.: 91.80%] [G loss: 2.820317]\n",
      "2469 [D loss: 0.231824, acc.: 88.28%] [G loss: 3.320978]\n",
      "2470 [D loss: 0.196944, acc.: 90.23%] [G loss: 3.157745]\n",
      "2471 [D loss: 0.200860, acc.: 92.19%] [G loss: 3.084700]\n",
      "2472 [D loss: 0.201613, acc.: 91.02%] [G loss: 3.190653]\n",
      "2473 [D loss: 0.212926, acc.: 91.02%] [G loss: 3.476667]\n",
      "2474 [D loss: 0.191926, acc.: 91.41%] [G loss: 2.950434]\n",
      "2475 [D loss: 0.222089, acc.: 88.67%] [G loss: 3.003731]\n",
      "2476 [D loss: 0.213230, acc.: 91.02%] [G loss: 3.278872]\n",
      "2477 [D loss: 0.179596, acc.: 91.80%] [G loss: 3.042247]\n",
      "2478 [D loss: 0.211350, acc.: 91.41%] [G loss: 3.154471]\n",
      "2479 [D loss: 0.213725, acc.: 89.84%] [G loss: 3.246612]\n",
      "2480 [D loss: 0.214086, acc.: 91.02%] [G loss: 3.332980]\n",
      "2481 [D loss: 0.227469, acc.: 91.41%] [G loss: 3.211604]\n",
      "2482 [D loss: 0.193567, acc.: 92.19%] [G loss: 3.217747]\n",
      "2483 [D loss: 0.246225, acc.: 89.06%] [G loss: 3.373024]\n",
      "2484 [D loss: 0.183919, acc.: 92.19%] [G loss: 3.350583]\n",
      "2485 [D loss: 0.214334, acc.: 91.02%] [G loss: 2.903113]\n",
      "2486 [D loss: 0.184365, acc.: 92.58%] [G loss: 3.144449]\n",
      "2487 [D loss: 0.187409, acc.: 92.19%] [G loss: 3.029873]\n",
      "2488 [D loss: 0.201258, acc.: 91.41%] [G loss: 3.206818]\n",
      "2489 [D loss: 0.203441, acc.: 91.02%] [G loss: 3.395872]\n",
      "2490 [D loss: 0.192020, acc.: 91.80%] [G loss: 3.081560]\n",
      "2491 [D loss: 0.215031, acc.: 90.62%] [G loss: 3.383673]\n",
      "2492 [D loss: 0.213397, acc.: 91.02%] [G loss: 3.217081]\n",
      "2493 [D loss: 0.192621, acc.: 91.80%] [G loss: 3.287825]\n",
      "2494 [D loss: 0.241208, acc.: 88.67%] [G loss: 3.264749]\n",
      "2495 [D loss: 0.192392, acc.: 92.58%] [G loss: 3.312368]\n",
      "2496 [D loss: 0.225688, acc.: 91.41%] [G loss: 3.261054]\n",
      "2497 [D loss: 0.214254, acc.: 90.23%] [G loss: 3.490336]\n",
      "2498 [D loss: 0.180303, acc.: 91.41%] [G loss: 3.288170]\n",
      "2499 [D loss: 0.184350, acc.: 92.19%] [G loss: 3.209671]\n",
      "2500 [D loss: 0.184252, acc.: 91.02%] [G loss: 3.175933]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "2501 [D loss: 0.196793, acc.: 92.19%] [G loss: 3.423556]\n",
      "2502 [D loss: 0.190590, acc.: 91.41%] [G loss: 3.284766]\n",
      "2503 [D loss: 0.211684, acc.: 91.02%] [G loss: 3.391437]\n",
      "2504 [D loss: 0.207555, acc.: 91.02%] [G loss: 3.168168]\n",
      "2505 [D loss: 0.210704, acc.: 92.97%] [G loss: 3.070888]\n",
      "2506 [D loss: 0.204600, acc.: 91.41%] [G loss: 3.211924]\n",
      "2507 [D loss: 0.207701, acc.: 91.80%] [G loss: 3.289884]\n",
      "2508 [D loss: 0.185388, acc.: 92.58%] [G loss: 3.119686]\n",
      "2509 [D loss: 0.176176, acc.: 92.19%] [G loss: 3.014886]\n",
      "2510 [D loss: 0.193604, acc.: 92.19%] [G loss: 3.109193]\n",
      "2511 [D loss: 0.182529, acc.: 91.80%] [G loss: 3.314270]\n",
      "2512 [D loss: 0.207971, acc.: 91.41%] [G loss: 3.258308]\n",
      "2513 [D loss: 0.196313, acc.: 91.02%] [G loss: 3.269292]\n",
      "2514 [D loss: 0.214621, acc.: 90.23%] [G loss: 3.316855]\n",
      "2515 [D loss: 0.212149, acc.: 90.62%] [G loss: 3.192888]\n",
      "2516 [D loss: 0.217939, acc.: 91.02%] [G loss: 3.124085]\n",
      "2517 [D loss: 0.216287, acc.: 91.41%] [G loss: 3.135726]\n",
      "2518 [D loss: 0.242780, acc.: 89.84%] [G loss: 3.487696]\n",
      "2519 [D loss: 0.201194, acc.: 92.19%] [G loss: 3.296064]\n",
      "2520 [D loss: 0.232270, acc.: 89.06%] [G loss: 2.896719]\n",
      "2521 [D loss: 0.170132, acc.: 94.92%] [G loss: 3.306956]\n",
      "2522 [D loss: 0.220224, acc.: 90.62%] [G loss: 3.275946]\n",
      "2523 [D loss: 0.208347, acc.: 91.02%] [G loss: 3.136351]\n",
      "2524 [D loss: 0.205233, acc.: 91.80%] [G loss: 3.085042]\n",
      "2525 [D loss: 0.203668, acc.: 89.45%] [G loss: 3.314468]\n",
      "2526 [D loss: 0.198729, acc.: 92.19%] [G loss: 3.408333]\n",
      "2527 [D loss: 0.203511, acc.: 91.80%] [G loss: 2.945177]\n",
      "2528 [D loss: 0.184499, acc.: 92.97%] [G loss: 2.988588]\n",
      "2529 [D loss: 0.182485, acc.: 93.75%] [G loss: 3.304148]\n",
      "2530 [D loss: 0.230169, acc.: 91.02%] [G loss: 3.219836]\n",
      "2531 [D loss: 0.211646, acc.: 90.23%] [G loss: 3.147986]\n",
      "2532 [D loss: 0.196188, acc.: 91.02%] [G loss: 3.185045]\n",
      "2533 [D loss: 0.198964, acc.: 91.41%] [G loss: 3.204357]\n",
      "2534 [D loss: 0.198560, acc.: 92.19%] [G loss: 2.926935]\n",
      "2535 [D loss: 0.191320, acc.: 91.41%] [G loss: 3.129260]\n",
      "2536 [D loss: 0.217310, acc.: 90.62%] [G loss: 3.268131]\n",
      "2537 [D loss: 0.202636, acc.: 91.80%] [G loss: 3.487891]\n",
      "2538 [D loss: 0.221455, acc.: 89.84%] [G loss: 3.123630]\n",
      "2539 [D loss: 0.204463, acc.: 90.62%] [G loss: 3.026426]\n",
      "2540 [D loss: 0.219764, acc.: 90.23%] [G loss: 3.093863]\n",
      "2541 [D loss: 0.239491, acc.: 91.02%] [G loss: 3.151663]\n",
      "2542 [D loss: 0.205344, acc.: 90.62%] [G loss: 2.958336]\n",
      "2543 [D loss: 0.226828, acc.: 90.62%] [G loss: 3.369145]\n",
      "2544 [D loss: 0.216093, acc.: 90.23%] [G loss: 3.378037]\n",
      "2545 [D loss: 0.203673, acc.: 92.58%] [G loss: 3.026812]\n",
      "2546 [D loss: 0.214913, acc.: 92.97%] [G loss: 3.348916]\n",
      "2547 [D loss: 0.201780, acc.: 91.80%] [G loss: 3.149491]\n",
      "2548 [D loss: 0.165248, acc.: 92.97%] [G loss: 3.287557]\n",
      "2549 [D loss: 0.194781, acc.: 91.02%] [G loss: 3.123958]\n",
      "2550 [D loss: 0.185907, acc.: 93.36%] [G loss: 3.148721]\n",
      "2551 [D loss: 0.175523, acc.: 92.58%] [G loss: 3.161795]\n",
      "2552 [D loss: 0.182547, acc.: 91.80%] [G loss: 3.297870]\n",
      "2553 [D loss: 0.213539, acc.: 91.80%] [G loss: 3.369352]\n",
      "2554 [D loss: 0.216594, acc.: 91.02%] [G loss: 3.044610]\n",
      "2555 [D loss: 0.211340, acc.: 91.41%] [G loss: 3.128636]\n",
      "2556 [D loss: 0.203713, acc.: 91.80%] [G loss: 3.509209]\n",
      "2557 [D loss: 0.220230, acc.: 91.41%] [G loss: 3.066447]\n",
      "2558 [D loss: 0.267812, acc.: 87.50%] [G loss: 3.375917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2559 [D loss: 0.176378, acc.: 92.19%] [G loss: 3.661723]\n",
      "2560 [D loss: 0.238530, acc.: 89.45%] [G loss: 3.014262]\n",
      "2561 [D loss: 0.183671, acc.: 92.19%] [G loss: 2.859663]\n",
      "2562 [D loss: 0.189385, acc.: 93.36%] [G loss: 3.251221]\n",
      "2563 [D loss: 0.188649, acc.: 93.36%] [G loss: 3.182690]\n",
      "2564 [D loss: 0.227027, acc.: 90.23%] [G loss: 3.145096]\n",
      "2565 [D loss: 0.210059, acc.: 90.62%] [G loss: 3.206783]\n",
      "2566 [D loss: 0.210049, acc.: 91.02%] [G loss: 3.153347]\n",
      "2567 [D loss: 0.193715, acc.: 92.58%] [G loss: 3.177844]\n",
      "2568 [D loss: 0.206168, acc.: 91.80%] [G loss: 3.231830]\n",
      "2569 [D loss: 0.216104, acc.: 90.62%] [G loss: 3.039986]\n",
      "2570 [D loss: 0.223701, acc.: 87.89%] [G loss: 3.602054]\n",
      "2571 [D loss: 0.207273, acc.: 91.80%] [G loss: 3.323538]\n",
      "2572 [D loss: 0.208961, acc.: 91.41%] [G loss: 3.131543]\n",
      "2573 [D loss: 0.197129, acc.: 92.97%] [G loss: 3.213288]\n",
      "2574 [D loss: 0.207091, acc.: 91.80%] [G loss: 3.437648]\n",
      "2575 [D loss: 0.207766, acc.: 91.02%] [G loss: 3.143322]\n",
      "2576 [D loss: 0.228699, acc.: 90.62%] [G loss: 3.101854]\n",
      "2577 [D loss: 0.208852, acc.: 91.02%] [G loss: 3.279945]\n",
      "2578 [D loss: 0.183513, acc.: 92.97%] [G loss: 3.353735]\n",
      "2579 [D loss: 0.235687, acc.: 91.02%] [G loss: 2.973797]\n",
      "2580 [D loss: 0.206691, acc.: 91.02%] [G loss: 3.086731]\n",
      "2581 [D loss: 0.231070, acc.: 90.62%] [G loss: 3.502276]\n",
      "2582 [D loss: 0.215888, acc.: 90.62%] [G loss: 3.296191]\n",
      "2583 [D loss: 0.207385, acc.: 90.62%] [G loss: 3.018980]\n",
      "2584 [D loss: 0.196657, acc.: 92.58%] [G loss: 2.742295]\n",
      "2585 [D loss: 0.182707, acc.: 93.36%] [G loss: 3.619001]\n",
      "2586 [D loss: 0.190296, acc.: 91.80%] [G loss: 3.264366]\n",
      "2587 [D loss: 0.196876, acc.: 91.41%] [G loss: 3.172379]\n",
      "2588 [D loss: 0.175059, acc.: 92.97%] [G loss: 3.553018]\n",
      "2589 [D loss: 0.223045, acc.: 91.80%] [G loss: 3.247486]\n",
      "2590 [D loss: 0.189991, acc.: 91.41%] [G loss: 3.313724]\n",
      "2591 [D loss: 0.215258, acc.: 91.41%] [G loss: 2.892887]\n",
      "2592 [D loss: 0.194380, acc.: 91.41%] [G loss: 3.103327]\n",
      "2593 [D loss: 0.186610, acc.: 91.80%] [G loss: 3.230661]\n",
      "2594 [D loss: 0.240340, acc.: 88.67%] [G loss: 3.130957]\n",
      "2595 [D loss: 0.196537, acc.: 91.80%] [G loss: 3.452522]\n",
      "2596 [D loss: 0.202810, acc.: 91.41%] [G loss: 3.194619]\n",
      "2597 [D loss: 0.188590, acc.: 92.19%] [G loss: 3.370735]\n",
      "2598 [D loss: 0.202242, acc.: 91.41%] [G loss: 3.155831]\n",
      "2599 [D loss: 0.204744, acc.: 89.06%] [G loss: 3.285654]\n",
      "2600 [D loss: 0.220633, acc.: 91.41%] [G loss: 3.331769]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "2601 [D loss: 0.218626, acc.: 90.62%] [G loss: 3.111688]\n",
      "2602 [D loss: 0.192738, acc.: 92.97%] [G loss: 3.206731]\n",
      "2603 [D loss: 0.226340, acc.: 90.62%] [G loss: 3.631671]\n",
      "2604 [D loss: 0.216307, acc.: 91.02%] [G loss: 2.976626]\n",
      "2605 [D loss: 0.197169, acc.: 92.19%] [G loss: 3.029006]\n",
      "2606 [D loss: 0.214620, acc.: 89.84%] [G loss: 3.441943]\n",
      "2607 [D loss: 0.230181, acc.: 91.41%] [G loss: 3.367579]\n",
      "2608 [D loss: 0.261065, acc.: 89.06%] [G loss: 3.144915]\n",
      "2609 [D loss: 0.222172, acc.: 91.41%] [G loss: 3.233712]\n",
      "2610 [D loss: 0.194956, acc.: 90.62%] [G loss: 3.050303]\n",
      "2611 [D loss: 0.211343, acc.: 92.19%] [G loss: 3.400209]\n",
      "2612 [D loss: 0.220013, acc.: 90.23%] [G loss: 3.440033]\n",
      "2613 [D loss: 0.199982, acc.: 90.62%] [G loss: 3.124286]\n",
      "2614 [D loss: 0.197417, acc.: 91.41%] [G loss: 2.983484]\n",
      "2615 [D loss: 0.184456, acc.: 91.02%] [G loss: 3.311065]\n",
      "2616 [D loss: 0.210374, acc.: 92.58%] [G loss: 3.130868]\n",
      "2617 [D loss: 0.209242, acc.: 91.80%] [G loss: 3.260077]\n",
      "2618 [D loss: 0.236243, acc.: 91.02%] [G loss: 3.325497]\n",
      "2619 [D loss: 0.204091, acc.: 90.62%] [G loss: 3.312187]\n",
      "2620 [D loss: 0.230145, acc.: 90.23%] [G loss: 3.070756]\n",
      "2621 [D loss: 0.228014, acc.: 89.84%] [G loss: 2.988451]\n",
      "2622 [D loss: 0.214284, acc.: 91.02%] [G loss: 3.216099]\n",
      "2623 [D loss: 0.187887, acc.: 91.80%] [G loss: 3.275978]\n",
      "2624 [D loss: 0.221125, acc.: 91.02%] [G loss: 3.046199]\n",
      "2625 [D loss: 0.209038, acc.: 90.23%] [G loss: 3.269843]\n",
      "2626 [D loss: 0.221440, acc.: 89.84%] [G loss: 3.115097]\n",
      "2627 [D loss: 0.196799, acc.: 92.19%] [G loss: 3.200405]\n",
      "2628 [D loss: 0.236598, acc.: 89.84%] [G loss: 3.151040]\n",
      "2629 [D loss: 0.251747, acc.: 88.67%] [G loss: 3.291625]\n",
      "2630 [D loss: 0.219959, acc.: 89.84%] [G loss: 3.130847]\n",
      "2631 [D loss: 0.212156, acc.: 91.80%] [G loss: 2.903116]\n",
      "2632 [D loss: 0.214613, acc.: 90.62%] [G loss: 3.107710]\n",
      "2633 [D loss: 0.222553, acc.: 89.84%] [G loss: 3.271193]\n",
      "2634 [D loss: 0.211523, acc.: 91.41%] [G loss: 3.384054]\n",
      "2635 [D loss: 0.205517, acc.: 92.58%] [G loss: 3.254505]\n",
      "2636 [D loss: 0.209449, acc.: 91.41%] [G loss: 2.875579]\n",
      "2637 [D loss: 0.169383, acc.: 92.97%] [G loss: 2.936696]\n",
      "2638 [D loss: 0.219793, acc.: 90.62%] [G loss: 3.195768]\n",
      "2639 [D loss: 0.205156, acc.: 92.19%] [G loss: 3.060359]\n",
      "2640 [D loss: 0.230474, acc.: 89.84%] [G loss: 3.290122]\n",
      "2641 [D loss: 0.265200, acc.: 90.62%] [G loss: 3.056962]\n",
      "2642 [D loss: 0.216697, acc.: 90.23%] [G loss: 3.282175]\n",
      "2643 [D loss: 0.223154, acc.: 89.84%] [G loss: 3.105403]\n",
      "2644 [D loss: 0.181697, acc.: 92.58%] [G loss: 3.072684]\n",
      "2645 [D loss: 0.213917, acc.: 92.19%] [G loss: 2.828149]\n",
      "2646 [D loss: 0.211397, acc.: 91.41%] [G loss: 3.148987]\n",
      "2647 [D loss: 0.222374, acc.: 90.62%] [G loss: 3.250884]\n",
      "2648 [D loss: 0.196634, acc.: 92.19%] [G loss: 2.993202]\n",
      "2649 [D loss: 0.213454, acc.: 91.80%] [G loss: 3.117545]\n",
      "2650 [D loss: 0.215681, acc.: 90.62%] [G loss: 3.417742]\n",
      "2651 [D loss: 0.214749, acc.: 90.62%] [G loss: 3.514575]\n",
      "2652 [D loss: 0.218173, acc.: 91.80%] [G loss: 3.090658]\n",
      "2653 [D loss: 0.192309, acc.: 92.97%] [G loss: 3.171037]\n",
      "2654 [D loss: 0.188088, acc.: 92.97%] [G loss: 3.137555]\n",
      "2655 [D loss: 0.224774, acc.: 87.89%] [G loss: 3.102230]\n",
      "2656 [D loss: 0.220605, acc.: 91.02%] [G loss: 3.274932]\n",
      "2657 [D loss: 0.192703, acc.: 93.36%] [G loss: 3.193842]\n",
      "2658 [D loss: 0.210274, acc.: 91.41%] [G loss: 3.242170]\n",
      "2659 [D loss: 0.224126, acc.: 91.02%] [G loss: 2.935164]\n",
      "2660 [D loss: 0.217883, acc.: 90.62%] [G loss: 3.222157]\n",
      "2661 [D loss: 0.214762, acc.: 91.80%] [G loss: 3.607780]\n",
      "2662 [D loss: 0.201073, acc.: 92.97%] [G loss: 3.239764]\n",
      "2663 [D loss: 0.209400, acc.: 91.02%] [G loss: 2.988708]\n",
      "2664 [D loss: 0.202957, acc.: 91.80%] [G loss: 3.307002]\n",
      "2665 [D loss: 0.225118, acc.: 90.62%] [G loss: 3.271238]\n",
      "2666 [D loss: 0.181337, acc.: 91.80%] [G loss: 3.230058]\n",
      "2667 [D loss: 0.211962, acc.: 90.62%] [G loss: 2.957808]\n",
      "2668 [D loss: 0.201931, acc.: 91.80%] [G loss: 3.196898]\n",
      "2669 [D loss: 0.226260, acc.: 89.06%] [G loss: 2.988323]\n",
      "2670 [D loss: 0.201072, acc.: 91.02%] [G loss: 2.976098]\n",
      "2671 [D loss: 0.204756, acc.: 91.80%] [G loss: 2.714540]\n",
      "2672 [D loss: 0.196247, acc.: 91.80%] [G loss: 3.272210]\n",
      "2673 [D loss: 0.192074, acc.: 91.02%] [G loss: 3.193704]\n",
      "2674 [D loss: 0.243945, acc.: 88.67%] [G loss: 3.350926]\n",
      "2675 [D loss: 0.245728, acc.: 88.67%] [G loss: 3.176730]\n",
      "2676 [D loss: 0.208285, acc.: 90.62%] [G loss: 3.293543]\n",
      "2677 [D loss: 0.209298, acc.: 91.41%] [G loss: 3.071722]\n",
      "2678 [D loss: 0.231837, acc.: 90.62%] [G loss: 3.269406]\n",
      "2679 [D loss: 0.228543, acc.: 88.67%] [G loss: 3.223392]\n",
      "2680 [D loss: 0.212406, acc.: 90.62%] [G loss: 3.461363]\n",
      "2681 [D loss: 0.224463, acc.: 89.84%] [G loss: 3.312151]\n",
      "2682 [D loss: 0.220253, acc.: 91.41%] [G loss: 2.967600]\n",
      "2683 [D loss: 0.208258, acc.: 90.23%] [G loss: 3.213308]\n",
      "2684 [D loss: 0.190937, acc.: 91.41%] [G loss: 3.161750]\n",
      "2685 [D loss: 0.187202, acc.: 93.36%] [G loss: 3.520025]\n",
      "2686 [D loss: 0.211861, acc.: 91.41%] [G loss: 2.971958]\n",
      "2687 [D loss: 0.213758, acc.: 89.84%] [G loss: 2.826926]\n",
      "2688 [D loss: 0.204041, acc.: 92.19%] [G loss: 3.182568]\n",
      "2689 [D loss: 0.200273, acc.: 92.97%] [G loss: 3.117547]\n",
      "2690 [D loss: 0.208683, acc.: 91.02%] [G loss: 3.263055]\n",
      "2691 [D loss: 0.193426, acc.: 91.41%] [G loss: 3.100737]\n",
      "2692 [D loss: 0.217094, acc.: 91.80%] [G loss: 3.079549]\n",
      "2693 [D loss: 0.211119, acc.: 90.62%] [G loss: 3.094202]\n",
      "2694 [D loss: 0.190326, acc.: 91.41%] [G loss: 3.116126]\n",
      "2695 [D loss: 0.221390, acc.: 89.84%] [G loss: 3.105728]\n",
      "2696 [D loss: 0.194050, acc.: 91.80%] [G loss: 3.034516]\n",
      "2697 [D loss: 0.217123, acc.: 92.19%] [G loss: 3.348238]\n",
      "2698 [D loss: 0.209015, acc.: 90.23%] [G loss: 3.586139]\n",
      "2699 [D loss: 0.220855, acc.: 90.23%] [G loss: 3.029629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700 [D loss: 0.212220, acc.: 91.41%] [G loss: 3.124276]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "2701 [D loss: 0.219580, acc.: 90.62%] [G loss: 2.930785]\n",
      "2702 [D loss: 0.191008, acc.: 92.19%] [G loss: 3.077821]\n",
      "2703 [D loss: 0.205870, acc.: 91.02%] [G loss: 3.477695]\n",
      "2704 [D loss: 0.200296, acc.: 91.02%] [G loss: 3.080699]\n",
      "2705 [D loss: 0.210938, acc.: 90.62%] [G loss: 3.029742]\n",
      "2706 [D loss: 0.246559, acc.: 89.45%] [G loss: 3.187012]\n",
      "2707 [D loss: 0.218246, acc.: 90.23%] [G loss: 3.384808]\n",
      "2708 [D loss: 0.228643, acc.: 90.23%] [G loss: 3.405452]\n",
      "2709 [D loss: 0.211266, acc.: 91.02%] [G loss: 3.120187]\n",
      "2710 [D loss: 0.203197, acc.: 90.62%] [G loss: 3.416649]\n",
      "2711 [D loss: 0.211702, acc.: 91.80%] [G loss: 3.209375]\n",
      "2712 [D loss: 0.212382, acc.: 91.02%] [G loss: 3.166868]\n",
      "2713 [D loss: 0.206742, acc.: 92.58%] [G loss: 3.135849]\n",
      "2714 [D loss: 0.217331, acc.: 89.45%] [G loss: 3.217376]\n",
      "2715 [D loss: 0.219336, acc.: 90.23%] [G loss: 3.259252]\n",
      "2716 [D loss: 0.201778, acc.: 91.02%] [G loss: 2.816821]\n",
      "2717 [D loss: 0.203876, acc.: 89.45%] [G loss: 3.206762]\n",
      "2718 [D loss: 0.189041, acc.: 92.97%] [G loss: 3.261209]\n",
      "2719 [D loss: 0.212008, acc.: 91.02%] [G loss: 3.240453]\n",
      "2720 [D loss: 0.198917, acc.: 91.41%] [G loss: 3.162364]\n",
      "2721 [D loss: 0.238190, acc.: 90.62%] [G loss: 3.497412]\n",
      "2722 [D loss: 0.240812, acc.: 90.23%] [G loss: 3.139856]\n",
      "2723 [D loss: 0.204344, acc.: 91.80%] [G loss: 3.019367]\n",
      "2724 [D loss: 0.211560, acc.: 91.80%] [G loss: 2.975099]\n",
      "2725 [D loss: 0.204669, acc.: 91.41%] [G loss: 3.160645]\n",
      "2726 [D loss: 0.227865, acc.: 88.67%] [G loss: 2.879574]\n",
      "2727 [D loss: 0.245618, acc.: 89.84%] [G loss: 3.033396]\n",
      "2728 [D loss: 0.228328, acc.: 90.23%] [G loss: 3.156353]\n",
      "2729 [D loss: 0.227516, acc.: 89.84%] [G loss: 2.973426]\n",
      "2730 [D loss: 0.200461, acc.: 90.62%] [G loss: 3.207511]\n",
      "2731 [D loss: 0.208884, acc.: 90.62%] [G loss: 3.205637]\n",
      "2732 [D loss: 0.201499, acc.: 91.41%] [G loss: 2.997328]\n",
      "2733 [D loss: 0.233309, acc.: 91.80%] [G loss: 3.189396]\n",
      "2734 [D loss: 0.239988, acc.: 90.23%] [G loss: 3.052715]\n",
      "2735 [D loss: 0.216398, acc.: 91.41%] [G loss: 3.153080]\n",
      "2736 [D loss: 0.216506, acc.: 91.02%] [G loss: 3.111970]\n",
      "2737 [D loss: 0.235796, acc.: 91.41%] [G loss: 3.110421]\n",
      "2738 [D loss: 0.199540, acc.: 91.41%] [G loss: 3.122620]\n",
      "2739 [D loss: 0.201706, acc.: 91.41%] [G loss: 3.304176]\n",
      "2740 [D loss: 0.231920, acc.: 89.84%] [G loss: 3.248751]\n",
      "2741 [D loss: 0.223916, acc.: 90.62%] [G loss: 3.126880]\n",
      "2742 [D loss: 0.203819, acc.: 92.19%] [G loss: 3.136147]\n",
      "2743 [D loss: 0.201845, acc.: 92.19%] [G loss: 2.977922]\n",
      "2744 [D loss: 0.202970, acc.: 91.02%] [G loss: 3.339865]\n",
      "2745 [D loss: 0.194959, acc.: 92.58%] [G loss: 3.067463]\n",
      "2746 [D loss: 0.219409, acc.: 90.23%] [G loss: 3.041714]\n",
      "2747 [D loss: 0.182217, acc.: 93.36%] [G loss: 3.200295]\n",
      "2748 [D loss: 0.206161, acc.: 91.41%] [G loss: 3.431655]\n",
      "2749 [D loss: 0.210148, acc.: 91.80%] [G loss: 3.030974]\n",
      "2750 [D loss: 0.237214, acc.: 91.41%] [G loss: 3.173768]\n",
      "2751 [D loss: 0.217616, acc.: 92.19%] [G loss: 2.957855]\n",
      "2752 [D loss: 0.201809, acc.: 91.80%] [G loss: 2.920434]\n",
      "2753 [D loss: 0.217517, acc.: 92.19%] [G loss: 2.917353]\n",
      "2754 [D loss: 0.204379, acc.: 91.02%] [G loss: 3.200807]\n",
      "2755 [D loss: 0.202001, acc.: 90.23%] [G loss: 3.102582]\n",
      "2756 [D loss: 0.191861, acc.: 92.19%] [G loss: 3.005161]\n",
      "2757 [D loss: 0.188803, acc.: 92.97%] [G loss: 3.065907]\n",
      "2758 [D loss: 0.213627, acc.: 92.19%] [G loss: 3.257033]\n",
      "2759 [D loss: 0.202221, acc.: 91.41%] [G loss: 3.124806]\n",
      "2760 [D loss: 0.217150, acc.: 91.80%] [G loss: 3.272250]\n",
      "2761 [D loss: 0.205060, acc.: 92.97%] [G loss: 3.370774]\n",
      "2762 [D loss: 0.190675, acc.: 92.19%] [G loss: 3.276641]\n",
      "2763 [D loss: 0.200989, acc.: 93.36%] [G loss: 3.150920]\n",
      "2764 [D loss: 0.202569, acc.: 91.80%] [G loss: 2.947427]\n",
      "2765 [D loss: 0.205243, acc.: 90.62%] [G loss: 3.149637]\n",
      "2766 [D loss: 0.214703, acc.: 91.02%] [G loss: 3.281968]\n",
      "2767 [D loss: 0.226801, acc.: 87.50%] [G loss: 3.105678]\n",
      "2768 [D loss: 0.179727, acc.: 93.36%] [G loss: 3.316728]\n",
      "2769 [D loss: 0.209073, acc.: 92.58%] [G loss: 3.294197]\n",
      "2770 [D loss: 0.192343, acc.: 92.58%] [G loss: 3.131069]\n",
      "2771 [D loss: 0.218981, acc.: 90.23%] [G loss: 3.097826]\n",
      "2772 [D loss: 0.194577, acc.: 92.58%] [G loss: 3.266787]\n",
      "2773 [D loss: 0.237538, acc.: 91.02%] [G loss: 3.008238]\n",
      "2774 [D loss: 0.226140, acc.: 89.45%] [G loss: 3.236126]\n",
      "2775 [D loss: 0.235985, acc.: 89.06%] [G loss: 3.202347]\n",
      "2776 [D loss: 0.229022, acc.: 89.84%] [G loss: 3.222835]\n",
      "2777 [D loss: 0.175651, acc.: 92.19%] [G loss: 2.994357]\n",
      "2778 [D loss: 0.215463, acc.: 92.19%] [G loss: 3.327135]\n",
      "2779 [D loss: 0.228915, acc.: 91.02%] [G loss: 3.206046]\n",
      "2780 [D loss: 0.229501, acc.: 91.41%] [G loss: 3.242928]\n",
      "2781 [D loss: 0.237385, acc.: 90.62%] [G loss: 3.227432]\n",
      "2782 [D loss: 0.221630, acc.: 90.23%] [G loss: 3.367574]\n",
      "2783 [D loss: 0.253041, acc.: 89.84%] [G loss: 3.392805]\n",
      "2784 [D loss: 0.209934, acc.: 91.80%] [G loss: 2.949676]\n",
      "2785 [D loss: 0.201352, acc.: 91.41%] [G loss: 3.118697]\n",
      "2786 [D loss: 0.198065, acc.: 92.19%] [G loss: 3.144179]\n",
      "2787 [D loss: 0.211725, acc.: 91.02%] [G loss: 3.349175]\n",
      "2788 [D loss: 0.189053, acc.: 91.41%] [G loss: 2.952060]\n",
      "2789 [D loss: 0.186531, acc.: 92.19%] [G loss: 2.972139]\n",
      "2790 [D loss: 0.245116, acc.: 89.84%] [G loss: 3.139902]\n",
      "2791 [D loss: 0.212215, acc.: 92.19%] [G loss: 2.883217]\n",
      "2792 [D loss: 0.224570, acc.: 91.02%] [G loss: 3.222315]\n",
      "2793 [D loss: 0.207773, acc.: 91.02%] [G loss: 3.231936]\n",
      "2794 [D loss: 0.224357, acc.: 91.02%] [G loss: 3.050577]\n",
      "2795 [D loss: 0.202728, acc.: 92.97%] [G loss: 3.219614]\n",
      "2796 [D loss: 0.205316, acc.: 91.02%] [G loss: 3.172734]\n",
      "2797 [D loss: 0.182260, acc.: 92.19%] [G loss: 3.118995]\n",
      "2798 [D loss: 0.219993, acc.: 91.41%] [G loss: 3.171935]\n",
      "2799 [D loss: 0.211852, acc.: 91.02%] [G loss: 3.678681]\n",
      "2800 [D loss: 0.208569, acc.: 91.80%] [G loss: 3.496585]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "2801 [D loss: 0.189078, acc.: 92.19%] [G loss: 3.143259]\n",
      "2802 [D loss: 0.210502, acc.: 91.02%] [G loss: 3.205246]\n",
      "2803 [D loss: 0.222053, acc.: 90.62%] [G loss: 2.788785]\n",
      "2804 [D loss: 0.221429, acc.: 91.02%] [G loss: 3.011054]\n",
      "2805 [D loss: 0.208235, acc.: 90.23%] [G loss: 3.339736]\n",
      "2806 [D loss: 0.222060, acc.: 92.58%] [G loss: 3.128745]\n",
      "2807 [D loss: 0.197116, acc.: 91.41%] [G loss: 3.052969]\n",
      "2808 [D loss: 0.211450, acc.: 91.02%] [G loss: 2.992853]\n",
      "2809 [D loss: 0.190455, acc.: 92.19%] [G loss: 2.968651]\n",
      "2810 [D loss: 0.209485, acc.: 91.41%] [G loss: 3.219046]\n",
      "2811 [D loss: 0.213901, acc.: 91.41%] [G loss: 3.194580]\n",
      "2812 [D loss: 0.196621, acc.: 91.41%] [G loss: 3.192155]\n",
      "2813 [D loss: 0.207488, acc.: 91.02%] [G loss: 3.156534]\n",
      "2814 [D loss: 0.239781, acc.: 88.28%] [G loss: 3.246863]\n",
      "2815 [D loss: 0.214062, acc.: 91.80%] [G loss: 3.100226]\n",
      "2816 [D loss: 0.201883, acc.: 91.80%] [G loss: 2.902625]\n",
      "2817 [D loss: 0.211413, acc.: 89.84%] [G loss: 3.211799]\n",
      "2818 [D loss: 0.208282, acc.: 91.80%] [G loss: 3.030344]\n",
      "2819 [D loss: 0.201759, acc.: 91.80%] [G loss: 3.104671]\n",
      "2820 [D loss: 0.216781, acc.: 91.80%] [G loss: 3.091707]\n",
      "2821 [D loss: 0.239469, acc.: 89.06%] [G loss: 2.929404]\n",
      "2822 [D loss: 0.240479, acc.: 89.06%] [G loss: 3.199339]\n",
      "2823 [D loss: 0.225455, acc.: 90.62%] [G loss: 3.113740]\n",
      "2824 [D loss: 0.227890, acc.: 90.23%] [G loss: 3.176816]\n",
      "2825 [D loss: 0.205934, acc.: 91.41%] [G loss: 3.179979]\n",
      "2826 [D loss: 0.216228, acc.: 91.02%] [G loss: 3.016911]\n",
      "2827 [D loss: 0.216054, acc.: 89.84%] [G loss: 3.155327]\n",
      "2828 [D loss: 0.196578, acc.: 91.80%] [G loss: 3.005744]\n",
      "2829 [D loss: 0.226562, acc.: 91.41%] [G loss: 3.123713]\n",
      "2830 [D loss: 0.206752, acc.: 92.19%] [G loss: 3.278170]\n",
      "2831 [D loss: 0.222678, acc.: 89.84%] [G loss: 3.121211]\n",
      "2832 [D loss: 0.195302, acc.: 92.58%] [G loss: 3.307201]\n",
      "2833 [D loss: 0.195206, acc.: 92.19%] [G loss: 3.001764]\n",
      "2834 [D loss: 0.201060, acc.: 92.19%] [G loss: 3.007203]\n",
      "2835 [D loss: 0.214404, acc.: 90.62%] [G loss: 2.754783]\n",
      "2836 [D loss: 0.197497, acc.: 92.19%] [G loss: 2.749402]\n",
      "2837 [D loss: 0.200827, acc.: 91.80%] [G loss: 3.190746]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2838 [D loss: 0.243294, acc.: 90.23%] [G loss: 3.065261]\n",
      "2839 [D loss: 0.195007, acc.: 91.02%] [G loss: 3.210704]\n",
      "2840 [D loss: 0.216364, acc.: 90.62%] [G loss: 2.944989]\n",
      "2841 [D loss: 0.240584, acc.: 90.62%] [G loss: 2.937423]\n",
      "2842 [D loss: 0.218841, acc.: 91.80%] [G loss: 3.009230]\n",
      "2843 [D loss: 0.203002, acc.: 91.41%] [G loss: 2.932748]\n",
      "2844 [D loss: 0.173832, acc.: 93.75%] [G loss: 3.085459]\n",
      "2845 [D loss: 0.191645, acc.: 92.19%] [G loss: 3.319849]\n",
      "2846 [D loss: 0.199385, acc.: 91.80%] [G loss: 3.098363]\n",
      "2847 [D loss: 0.214601, acc.: 90.62%] [G loss: 3.189726]\n",
      "2848 [D loss: 0.228302, acc.: 88.67%] [G loss: 3.414276]\n",
      "2849 [D loss: 0.250706, acc.: 87.89%] [G loss: 3.497111]\n",
      "2850 [D loss: 0.255832, acc.: 89.06%] [G loss: 2.932391]\n",
      "2851 [D loss: 0.203018, acc.: 91.02%] [G loss: 2.953836]\n",
      "2852 [D loss: 0.179897, acc.: 93.75%] [G loss: 3.109203]\n",
      "2853 [D loss: 0.208747, acc.: 91.41%] [G loss: 2.774713]\n",
      "2854 [D loss: 0.217689, acc.: 90.23%] [G loss: 3.331470]\n",
      "2855 [D loss: 0.198384, acc.: 92.58%] [G loss: 3.273600]\n",
      "2856 [D loss: 0.222517, acc.: 91.02%] [G loss: 3.304390]\n",
      "2857 [D loss: 0.196237, acc.: 92.19%] [G loss: 3.043726]\n",
      "2858 [D loss: 0.186231, acc.: 92.19%] [G loss: 3.024780]\n",
      "2859 [D loss: 0.211675, acc.: 90.23%] [G loss: 2.816954]\n",
      "2860 [D loss: 0.207752, acc.: 91.41%] [G loss: 3.568004]\n",
      "2861 [D loss: 0.207245, acc.: 91.02%] [G loss: 3.208238]\n",
      "2862 [D loss: 0.218638, acc.: 91.41%] [G loss: 3.123018]\n",
      "2863 [D loss: 0.203710, acc.: 92.58%] [G loss: 2.830668]\n",
      "2864 [D loss: 0.202325, acc.: 91.41%] [G loss: 3.325255]\n",
      "2865 [D loss: 0.188608, acc.: 92.58%] [G loss: 2.996592]\n",
      "2866 [D loss: 0.240311, acc.: 89.45%] [G loss: 3.293154]\n",
      "2867 [D loss: 0.189292, acc.: 92.58%] [G loss: 3.158048]\n",
      "2868 [D loss: 0.196714, acc.: 92.58%] [G loss: 3.080719]\n",
      "2869 [D loss: 0.193638, acc.: 91.80%] [G loss: 3.090932]\n",
      "2870 [D loss: 0.195830, acc.: 91.02%] [G loss: 3.078808]\n",
      "2871 [D loss: 0.200643, acc.: 91.80%] [G loss: 2.959836]\n",
      "2872 [D loss: 0.206286, acc.: 90.23%] [G loss: 3.253634]\n",
      "2873 [D loss: 0.244939, acc.: 89.84%] [G loss: 2.972800]\n",
      "2874 [D loss: 0.189827, acc.: 92.97%] [G loss: 3.068232]\n",
      "2875 [D loss: 0.221831, acc.: 91.02%] [G loss: 2.930350]\n",
      "2876 [D loss: 0.201577, acc.: 91.41%] [G loss: 3.209090]\n",
      "2877 [D loss: 0.213770, acc.: 91.02%] [G loss: 3.065969]\n",
      "2878 [D loss: 0.220726, acc.: 91.80%] [G loss: 3.143718]\n",
      "2879 [D loss: 0.243720, acc.: 90.62%] [G loss: 3.084076]\n",
      "2880 [D loss: 0.190759, acc.: 90.62%] [G loss: 2.981375]\n",
      "2881 [D loss: 0.224295, acc.: 89.84%] [G loss: 3.121711]\n",
      "2882 [D loss: 0.255085, acc.: 91.02%] [G loss: 3.129622]\n",
      "2883 [D loss: 0.232915, acc.: 88.67%] [G loss: 3.059938]\n",
      "2884 [D loss: 0.198651, acc.: 91.80%] [G loss: 3.249655]\n",
      "2885 [D loss: 0.214950, acc.: 90.23%] [G loss: 3.406881]\n",
      "2886 [D loss: 0.214731, acc.: 91.02%] [G loss: 3.290114]\n",
      "2887 [D loss: 0.215612, acc.: 90.62%] [G loss: 3.005145]\n",
      "2888 [D loss: 0.216108, acc.: 91.41%] [G loss: 2.706490]\n",
      "2889 [D loss: 0.196077, acc.: 91.80%] [G loss: 3.025328]\n",
      "2890 [D loss: 0.223097, acc.: 91.02%] [G loss: 3.068004]\n",
      "2891 [D loss: 0.232410, acc.: 91.41%] [G loss: 3.076252]\n",
      "2892 [D loss: 0.202813, acc.: 91.80%] [G loss: 3.342985]\n",
      "2893 [D loss: 0.216456, acc.: 89.84%] [G loss: 3.313113]\n",
      "2894 [D loss: 0.211623, acc.: 92.19%] [G loss: 3.337626]\n",
      "2895 [D loss: 0.190747, acc.: 91.02%] [G loss: 2.968019]\n",
      "2896 [D loss: 0.207946, acc.: 91.02%] [G loss: 3.291148]\n",
      "2897 [D loss: 0.203091, acc.: 90.23%] [G loss: 3.256581]\n",
      "2898 [D loss: 0.219954, acc.: 90.62%] [G loss: 2.967642]\n",
      "2899 [D loss: 0.211368, acc.: 92.19%] [G loss: 3.207234]\n",
      "2900 [D loss: 0.185524, acc.: 92.58%] [G loss: 3.025198]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "2901 [D loss: 0.194281, acc.: 92.58%] [G loss: 3.212779]\n",
      "2902 [D loss: 0.191683, acc.: 92.58%] [G loss: 2.960339]\n",
      "2903 [D loss: 0.229012, acc.: 91.80%] [G loss: 3.178741]\n",
      "2904 [D loss: 0.230731, acc.: 91.02%] [G loss: 3.393848]\n",
      "2905 [D loss: 0.197406, acc.: 91.41%] [G loss: 3.224918]\n",
      "2906 [D loss: 0.218957, acc.: 91.02%] [G loss: 3.097602]\n",
      "2907 [D loss: 0.232900, acc.: 90.62%] [G loss: 3.575549]\n",
      "2908 [D loss: 0.203125, acc.: 92.19%] [G loss: 3.112154]\n",
      "2909 [D loss: 0.204260, acc.: 91.41%] [G loss: 3.211125]\n",
      "2910 [D loss: 0.223903, acc.: 90.23%] [G loss: 3.304169]\n",
      "2911 [D loss: 0.204200, acc.: 91.02%] [G loss: 3.162368]\n",
      "2912 [D loss: 0.205178, acc.: 91.41%] [G loss: 3.070465]\n",
      "2913 [D loss: 0.222784, acc.: 90.62%] [G loss: 2.958793]\n",
      "2914 [D loss: 0.211734, acc.: 90.23%] [G loss: 3.138868]\n",
      "2915 [D loss: 0.208459, acc.: 92.19%] [G loss: 3.049666]\n",
      "2916 [D loss: 0.192168, acc.: 91.80%] [G loss: 3.270214]\n",
      "2917 [D loss: 0.204258, acc.: 90.23%] [G loss: 3.105744]\n",
      "2918 [D loss: 0.225674, acc.: 90.62%] [G loss: 3.528401]\n",
      "2919 [D loss: 0.194207, acc.: 91.02%] [G loss: 3.225350]\n",
      "2920 [D loss: 0.222634, acc.: 90.23%] [G loss: 3.057140]\n",
      "2921 [D loss: 0.222090, acc.: 91.02%] [G loss: 2.920489]\n",
      "2922 [D loss: 0.192757, acc.: 91.41%] [G loss: 3.017361]\n",
      "2923 [D loss: 0.204774, acc.: 92.58%] [G loss: 3.041206]\n",
      "2924 [D loss: 0.184224, acc.: 92.19%] [G loss: 3.125085]\n",
      "2925 [D loss: 0.201552, acc.: 92.19%] [G loss: 3.371188]\n",
      "2926 [D loss: 0.214414, acc.: 91.41%] [G loss: 3.391672]\n",
      "2927 [D loss: 0.227033, acc.: 92.58%] [G loss: 3.001104]\n",
      "2928 [D loss: 0.202657, acc.: 92.97%] [G loss: 3.125625]\n",
      "2929 [D loss: 0.217659, acc.: 92.58%] [G loss: 3.262186]\n",
      "2930 [D loss: 0.220377, acc.: 91.41%] [G loss: 3.141082]\n",
      "2931 [D loss: 0.206590, acc.: 91.80%] [G loss: 2.735708]\n",
      "2932 [D loss: 0.187605, acc.: 91.80%] [G loss: 3.121071]\n",
      "2933 [D loss: 0.199607, acc.: 92.58%] [G loss: 3.056630]\n",
      "2934 [D loss: 0.197238, acc.: 90.62%] [G loss: 3.082740]\n",
      "2935 [D loss: 0.194091, acc.: 92.19%] [G loss: 3.350145]\n",
      "2936 [D loss: 0.242670, acc.: 89.06%] [G loss: 2.933233]\n",
      "2937 [D loss: 0.226623, acc.: 90.62%] [G loss: 2.955472]\n",
      "2938 [D loss: 0.239677, acc.: 89.84%] [G loss: 3.322569]\n",
      "2939 [D loss: 0.237638, acc.: 91.02%] [G loss: 3.196862]\n",
      "2940 [D loss: 0.208904, acc.: 89.45%] [G loss: 2.882094]\n",
      "2941 [D loss: 0.226985, acc.: 90.62%] [G loss: 3.199859]\n",
      "2942 [D loss: 0.242009, acc.: 90.23%] [G loss: 3.018086]\n",
      "2943 [D loss: 0.220683, acc.: 90.23%] [G loss: 3.031970]\n",
      "2944 [D loss: 0.209323, acc.: 91.41%] [G loss: 3.050550]\n",
      "2945 [D loss: 0.205154, acc.: 91.02%] [G loss: 3.112027]\n",
      "2946 [D loss: 0.233221, acc.: 90.23%] [G loss: 3.205612]\n",
      "2947 [D loss: 0.235927, acc.: 90.23%] [G loss: 3.086137]\n",
      "2948 [D loss: 0.206284, acc.: 91.02%] [G loss: 2.873277]\n",
      "2949 [D loss: 0.207081, acc.: 91.80%] [G loss: 3.040172]\n",
      "2950 [D loss: 0.198250, acc.: 90.62%] [G loss: 3.068075]\n",
      "2951 [D loss: 0.214933, acc.: 90.62%] [G loss: 3.122757]\n",
      "2952 [D loss: 0.192074, acc.: 93.75%] [G loss: 3.078891]\n",
      "2953 [D loss: 0.212613, acc.: 91.41%] [G loss: 2.923090]\n",
      "2954 [D loss: 0.198578, acc.: 92.58%] [G loss: 3.289090]\n",
      "2955 [D loss: 0.197259, acc.: 91.02%] [G loss: 3.055978]\n",
      "2956 [D loss: 0.217442, acc.: 92.19%] [G loss: 3.549196]\n",
      "2957 [D loss: 0.240631, acc.: 89.84%] [G loss: 2.937535]\n",
      "2958 [D loss: 0.211186, acc.: 90.62%] [G loss: 3.098664]\n",
      "2959 [D loss: 0.205978, acc.: 91.02%] [G loss: 3.025364]\n",
      "2960 [D loss: 0.211958, acc.: 89.06%] [G loss: 3.235189]\n",
      "2961 [D loss: 0.209977, acc.: 91.02%] [G loss: 2.947124]\n",
      "2962 [D loss: 0.207928, acc.: 91.02%] [G loss: 3.041685]\n",
      "2963 [D loss: 0.211770, acc.: 91.02%] [G loss: 3.020598]\n",
      "2964 [D loss: 0.194059, acc.: 93.36%] [G loss: 3.235685]\n",
      "2965 [D loss: 0.219615, acc.: 90.23%] [G loss: 3.024272]\n",
      "2966 [D loss: 0.197878, acc.: 92.58%] [G loss: 3.112800]\n",
      "2967 [D loss: 0.183069, acc.: 91.80%] [G loss: 3.416576]\n",
      "2968 [D loss: 0.206612, acc.: 91.80%] [G loss: 3.162745]\n",
      "2969 [D loss: 0.201693, acc.: 92.19%] [G loss: 3.315686]\n",
      "2970 [D loss: 0.244072, acc.: 89.84%] [G loss: 3.236659]\n",
      "2971 [D loss: 0.222616, acc.: 91.41%] [G loss: 3.070442]\n",
      "2972 [D loss: 0.197265, acc.: 92.58%] [G loss: 3.151310]\n",
      "2973 [D loss: 0.218248, acc.: 89.45%] [G loss: 2.944528]\n",
      "2974 [D loss: 0.232391, acc.: 91.02%] [G loss: 3.034208]\n",
      "2975 [D loss: 0.202381, acc.: 91.41%] [G loss: 3.191291]\n",
      "2976 [D loss: 0.224592, acc.: 92.19%] [G loss: 3.040307]\n",
      "2977 [D loss: 0.208628, acc.: 91.41%] [G loss: 2.827917]\n",
      "2978 [D loss: 0.201015, acc.: 92.58%] [G loss: 2.960668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2979 [D loss: 0.211511, acc.: 91.80%] [G loss: 3.324703]\n",
      "2980 [D loss: 0.185678, acc.: 91.41%] [G loss: 3.217559]\n",
      "2981 [D loss: 0.194803, acc.: 91.80%] [G loss: 3.166070]\n",
      "2982 [D loss: 0.238427, acc.: 91.02%] [G loss: 3.387433]\n",
      "2983 [D loss: 0.211318, acc.: 92.19%] [G loss: 3.085310]\n",
      "2984 [D loss: 0.195085, acc.: 92.97%] [G loss: 3.206366]\n",
      "2985 [D loss: 0.221385, acc.: 91.02%] [G loss: 3.277457]\n",
      "2986 [D loss: 0.225380, acc.: 91.80%] [G loss: 3.309742]\n",
      "2987 [D loss: 0.203895, acc.: 92.19%] [G loss: 3.128898]\n",
      "2988 [D loss: 0.204154, acc.: 91.41%] [G loss: 2.979080]\n",
      "2989 [D loss: 0.203804, acc.: 91.02%] [G loss: 3.137160]\n",
      "2990 [D loss: 0.213056, acc.: 91.41%] [G loss: 3.215862]\n",
      "2991 [D loss: 0.239425, acc.: 89.84%] [G loss: 2.930157]\n",
      "2992 [D loss: 0.209333, acc.: 91.41%] [G loss: 3.305667]\n",
      "2993 [D loss: 0.224388, acc.: 91.41%] [G loss: 3.091745]\n",
      "2994 [D loss: 0.205014, acc.: 91.02%] [G loss: 3.211444]\n",
      "2995 [D loss: 0.224001, acc.: 90.23%] [G loss: 3.161223]\n",
      "2996 [D loss: 0.204503, acc.: 91.41%] [G loss: 3.448369]\n",
      "2997 [D loss: 0.195629, acc.: 92.97%] [G loss: 3.111164]\n",
      "2998 [D loss: 0.209226, acc.: 91.80%] [G loss: 3.124035]\n",
      "2999 [D loss: 0.212201, acc.: 90.62%] [G loss: 3.206277]\n",
      "3000 [D loss: 0.219027, acc.: 90.62%] [G loss: 3.221127]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "3001 [D loss: 0.232017, acc.: 91.80%] [G loss: 2.866733]\n",
      "3002 [D loss: 0.199377, acc.: 92.19%] [G loss: 3.144996]\n",
      "3003 [D loss: 0.209196, acc.: 91.41%] [G loss: 3.100303]\n",
      "3004 [D loss: 0.234676, acc.: 90.62%] [G loss: 3.209833]\n",
      "3005 [D loss: 0.188809, acc.: 92.58%] [G loss: 3.065945]\n",
      "3006 [D loss: 0.202386, acc.: 91.80%] [G loss: 3.107544]\n",
      "3007 [D loss: 0.188256, acc.: 92.97%] [G loss: 2.935828]\n",
      "3008 [D loss: 0.194332, acc.: 92.58%] [G loss: 3.188672]\n",
      "3009 [D loss: 0.224525, acc.: 89.06%] [G loss: 3.328135]\n",
      "3010 [D loss: 0.225254, acc.: 90.23%] [G loss: 3.407020]\n",
      "3011 [D loss: 0.173263, acc.: 92.19%] [G loss: 3.031134]\n",
      "3012 [D loss: 0.211435, acc.: 90.62%] [G loss: 3.121877]\n",
      "3013 [D loss: 0.217971, acc.: 90.62%] [G loss: 3.215405]\n",
      "3014 [D loss: 0.192352, acc.: 91.80%] [G loss: 3.226454]\n",
      "3015 [D loss: 0.212724, acc.: 92.58%] [G loss: 3.251786]\n",
      "3016 [D loss: 0.190991, acc.: 92.58%] [G loss: 3.275452]\n",
      "3017 [D loss: 0.228134, acc.: 90.23%] [G loss: 3.064819]\n",
      "3018 [D loss: 0.191190, acc.: 91.80%] [G loss: 3.280941]\n",
      "3019 [D loss: 0.199606, acc.: 90.62%] [G loss: 3.065660]\n",
      "3020 [D loss: 0.200240, acc.: 92.58%] [G loss: 3.196009]\n",
      "3021 [D loss: 0.204330, acc.: 91.41%] [G loss: 2.855434]\n",
      "3022 [D loss: 0.195566, acc.: 92.19%] [G loss: 3.179380]\n",
      "3023 [D loss: 0.216857, acc.: 91.41%] [G loss: 3.299242]\n",
      "3024 [D loss: 0.200226, acc.: 92.58%] [G loss: 3.201450]\n",
      "3025 [D loss: 0.192321, acc.: 91.02%] [G loss: 2.832619]\n",
      "3026 [D loss: 0.198016, acc.: 91.80%] [G loss: 3.535717]\n",
      "3027 [D loss: 0.213550, acc.: 90.23%] [G loss: 3.173779]\n",
      "3028 [D loss: 0.219888, acc.: 90.62%] [G loss: 3.303910]\n",
      "3029 [D loss: 0.226667, acc.: 90.23%] [G loss: 3.089322]\n",
      "3030 [D loss: 0.210739, acc.: 90.62%] [G loss: 3.219923]\n",
      "3031 [D loss: 0.212861, acc.: 90.62%] [G loss: 3.267246]\n",
      "3032 [D loss: 0.207934, acc.: 92.19%] [G loss: 3.042857]\n",
      "3033 [D loss: 0.201673, acc.: 90.62%] [G loss: 2.998422]\n",
      "3034 [D loss: 0.220181, acc.: 91.02%] [G loss: 3.337113]\n",
      "3035 [D loss: 0.221506, acc.: 91.80%] [G loss: 3.003151]\n",
      "3036 [D loss: 0.220010, acc.: 91.02%] [G loss: 3.313265]\n",
      "3037 [D loss: 0.212236, acc.: 91.02%] [G loss: 3.542239]\n",
      "3038 [D loss: 0.192477, acc.: 91.80%] [G loss: 3.319479]\n",
      "3039 [D loss: 0.197837, acc.: 92.19%] [G loss: 3.082268]\n",
      "3040 [D loss: 0.188097, acc.: 92.58%] [G loss: 3.331659]\n",
      "3041 [D loss: 0.195905, acc.: 91.41%] [G loss: 3.098474]\n",
      "3042 [D loss: 0.167307, acc.: 92.97%] [G loss: 2.889968]\n",
      "3043 [D loss: 0.156702, acc.: 93.75%] [G loss: 2.984452]\n",
      "3044 [D loss: 0.181117, acc.: 92.19%] [G loss: 3.261279]\n",
      "3045 [D loss: 0.209219, acc.: 90.23%] [G loss: 3.100273]\n",
      "3046 [D loss: 0.197283, acc.: 91.80%] [G loss: 3.197576]\n",
      "3047 [D loss: 0.199368, acc.: 92.19%] [G loss: 3.201232]\n",
      "3048 [D loss: 0.223107, acc.: 91.41%] [G loss: 3.142030]\n",
      "3049 [D loss: 0.212262, acc.: 91.41%] [G loss: 3.414814]\n",
      "3050 [D loss: 0.183815, acc.: 91.80%] [G loss: 3.058331]\n",
      "3051 [D loss: 0.193731, acc.: 92.97%] [G loss: 3.350430]\n",
      "3052 [D loss: 0.172197, acc.: 92.97%] [G loss: 3.317357]\n",
      "3053 [D loss: 0.224663, acc.: 91.41%] [G loss: 3.099212]\n",
      "3054 [D loss: 0.214517, acc.: 91.41%] [G loss: 3.386618]\n",
      "3055 [D loss: 0.226998, acc.: 90.23%] [G loss: 3.152882]\n",
      "3056 [D loss: 0.191340, acc.: 92.19%] [G loss: 3.339073]\n",
      "3057 [D loss: 0.217520, acc.: 90.62%] [G loss: 3.184716]\n",
      "3058 [D loss: 0.183581, acc.: 92.58%] [G loss: 2.982906]\n",
      "3059 [D loss: 0.187918, acc.: 92.97%] [G loss: 3.142533]\n",
      "3060 [D loss: 0.222318, acc.: 91.02%] [G loss: 3.317089]\n",
      "3061 [D loss: 0.210043, acc.: 91.02%] [G loss: 3.301617]\n",
      "3062 [D loss: 0.209191, acc.: 91.02%] [G loss: 3.492808]\n",
      "3063 [D loss: 0.216150, acc.: 92.19%] [G loss: 2.889612]\n",
      "3064 [D loss: 0.201207, acc.: 90.62%] [G loss: 3.047631]\n",
      "3065 [D loss: 0.227103, acc.: 90.62%] [G loss: 3.341296]\n",
      "3066 [D loss: 0.235211, acc.: 90.62%] [G loss: 3.138934]\n",
      "3067 [D loss: 0.226776, acc.: 90.62%] [G loss: 3.144364]\n",
      "3068 [D loss: 0.224350, acc.: 90.62%] [G loss: 3.295856]\n",
      "3069 [D loss: 0.216453, acc.: 91.41%] [G loss: 3.183878]\n",
      "3070 [D loss: 0.207013, acc.: 90.23%] [G loss: 3.317021]\n",
      "3071 [D loss: 0.218117, acc.: 91.41%] [G loss: 3.286632]\n",
      "3072 [D loss: 0.211832, acc.: 90.62%] [G loss: 3.395144]\n",
      "3073 [D loss: 0.227750, acc.: 91.02%] [G loss: 3.459885]\n",
      "3074 [D loss: 0.203267, acc.: 92.58%] [G loss: 3.479580]\n",
      "3075 [D loss: 0.207916, acc.: 91.80%] [G loss: 3.341011]\n",
      "3076 [D loss: 0.196295, acc.: 92.19%] [G loss: 3.079933]\n",
      "3077 [D loss: 0.223977, acc.: 91.02%] [G loss: 3.287322]\n",
      "3078 [D loss: 0.208874, acc.: 91.80%] [G loss: 3.308808]\n",
      "3079 [D loss: 0.233056, acc.: 90.23%] [G loss: 2.910253]\n",
      "3080 [D loss: 0.202720, acc.: 91.02%] [G loss: 3.142780]\n",
      "3081 [D loss: 0.196796, acc.: 90.62%] [G loss: 3.330246]\n",
      "3082 [D loss: 0.195291, acc.: 92.19%] [G loss: 3.068979]\n",
      "3083 [D loss: 0.206624, acc.: 91.41%] [G loss: 3.617435]\n",
      "3084 [D loss: 0.187603, acc.: 92.19%] [G loss: 3.029311]\n",
      "3085 [D loss: 0.217261, acc.: 91.41%] [G loss: 2.963022]\n",
      "3086 [D loss: 0.190183, acc.: 92.58%] [G loss: 3.297285]\n",
      "3087 [D loss: 0.201320, acc.: 92.58%] [G loss: 2.960420]\n",
      "3088 [D loss: 0.197368, acc.: 92.58%] [G loss: 3.149951]\n",
      "3089 [D loss: 0.195384, acc.: 92.97%] [G loss: 3.338316]\n",
      "3090 [D loss: 0.195540, acc.: 91.02%] [G loss: 3.410829]\n",
      "3091 [D loss: 0.210840, acc.: 92.19%] [G loss: 3.471040]\n",
      "3092 [D loss: 0.209851, acc.: 90.23%] [G loss: 3.309668]\n",
      "3093 [D loss: 0.205875, acc.: 91.02%] [G loss: 3.118683]\n",
      "3094 [D loss: 0.194122, acc.: 92.19%] [G loss: 3.085128]\n",
      "3095 [D loss: 0.225744, acc.: 90.23%] [G loss: 3.300560]\n",
      "3096 [D loss: 0.197136, acc.: 91.80%] [G loss: 2.991074]\n",
      "3097 [D loss: 0.200957, acc.: 91.41%] [G loss: 3.214601]\n",
      "3098 [D loss: 0.210119, acc.: 92.58%] [G loss: 3.030673]\n",
      "3099 [D loss: 0.222851, acc.: 89.84%] [G loss: 3.172344]\n",
      "3100 [D loss: 0.209957, acc.: 91.02%] [G loss: 3.335405]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "3101 [D loss: 0.215448, acc.: 90.23%] [G loss: 3.384505]\n",
      "3102 [D loss: 0.184032, acc.: 91.80%] [G loss: 3.131398]\n",
      "3103 [D loss: 0.222039, acc.: 91.02%] [G loss: 2.994880]\n",
      "3104 [D loss: 0.189505, acc.: 92.19%] [G loss: 3.190711]\n",
      "3105 [D loss: 0.183247, acc.: 93.75%] [G loss: 2.936524]\n",
      "3106 [D loss: 0.198906, acc.: 92.58%] [G loss: 3.204681]\n",
      "3107 [D loss: 0.177199, acc.: 92.97%] [G loss: 3.332505]\n",
      "3108 [D loss: 0.200222, acc.: 92.19%] [G loss: 3.330916]\n",
      "3109 [D loss: 0.191122, acc.: 92.97%] [G loss: 3.279099]\n",
      "3110 [D loss: 0.196184, acc.: 90.62%] [G loss: 3.507065]\n",
      "3111 [D loss: 0.191422, acc.: 91.02%] [G loss: 3.346992]\n",
      "3112 [D loss: 0.192377, acc.: 91.80%] [G loss: 3.044113]\n",
      "3113 [D loss: 0.203929, acc.: 90.62%] [G loss: 3.306235]\n",
      "3114 [D loss: 0.213990, acc.: 90.23%] [G loss: 3.416866]\n",
      "3115 [D loss: 0.219373, acc.: 91.02%] [G loss: 3.224245]\n",
      "3116 [D loss: 0.193741, acc.: 93.36%] [G loss: 2.974664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3117 [D loss: 0.195940, acc.: 92.19%] [G loss: 3.408197]\n",
      "3118 [D loss: 0.231025, acc.: 90.62%] [G loss: 3.085618]\n",
      "3119 [D loss: 0.216703, acc.: 90.62%] [G loss: 3.164589]\n",
      "3120 [D loss: 0.233317, acc.: 89.45%] [G loss: 3.148004]\n",
      "3121 [D loss: 0.240336, acc.: 89.45%] [G loss: 3.092536]\n",
      "3122 [D loss: 0.190789, acc.: 92.19%] [G loss: 3.451956]\n",
      "3123 [D loss: 0.216323, acc.: 90.23%] [G loss: 3.259128]\n",
      "3124 [D loss: 0.234514, acc.: 91.41%] [G loss: 3.118442]\n",
      "3125 [D loss: 0.202324, acc.: 91.80%] [G loss: 3.312601]\n",
      "3126 [D loss: 0.213363, acc.: 91.80%] [G loss: 3.283113]\n",
      "3127 [D loss: 0.198406, acc.: 91.02%] [G loss: 3.167377]\n",
      "3128 [D loss: 0.221531, acc.: 91.80%] [G loss: 3.296798]\n",
      "3129 [D loss: 0.199500, acc.: 92.97%] [G loss: 3.140116]\n",
      "3130 [D loss: 0.196401, acc.: 91.80%] [G loss: 3.147294]\n",
      "3131 [D loss: 0.193101, acc.: 93.36%] [G loss: 3.197191]\n",
      "3132 [D loss: 0.222473, acc.: 90.62%] [G loss: 3.270658]\n",
      "3133 [D loss: 0.185645, acc.: 91.80%] [G loss: 3.186844]\n",
      "3134 [D loss: 0.213252, acc.: 91.02%] [G loss: 3.318032]\n",
      "3135 [D loss: 0.219373, acc.: 90.23%] [G loss: 3.025106]\n",
      "3136 [D loss: 0.211724, acc.: 91.41%] [G loss: 3.123807]\n",
      "3137 [D loss: 0.218028, acc.: 90.23%] [G loss: 3.479026]\n",
      "3138 [D loss: 0.219927, acc.: 89.45%] [G loss: 3.457708]\n",
      "3139 [D loss: 0.190751, acc.: 92.58%] [G loss: 3.315433]\n",
      "3140 [D loss: 0.201322, acc.: 92.58%] [G loss: 3.068905]\n",
      "3141 [D loss: 0.182120, acc.: 93.36%] [G loss: 3.291862]\n",
      "3142 [D loss: 0.189003, acc.: 92.97%] [G loss: 3.231809]\n",
      "3143 [D loss: 0.202400, acc.: 91.41%] [G loss: 3.330629]\n",
      "3144 [D loss: 0.225433, acc.: 91.02%] [G loss: 3.434539]\n",
      "3145 [D loss: 0.215361, acc.: 90.23%] [G loss: 3.324819]\n",
      "3146 [D loss: 0.204824, acc.: 92.19%] [G loss: 3.109562]\n",
      "3147 [D loss: 0.204224, acc.: 92.19%] [G loss: 2.797658]\n",
      "3148 [D loss: 0.212092, acc.: 91.80%] [G loss: 3.095719]\n",
      "3149 [D loss: 0.226769, acc.: 91.41%] [G loss: 3.466821]\n",
      "3150 [D loss: 0.225603, acc.: 90.23%] [G loss: 3.298541]\n",
      "3151 [D loss: 0.223732, acc.: 91.80%] [G loss: 3.277074]\n",
      "3152 [D loss: 0.198561, acc.: 91.02%] [G loss: 3.015480]\n",
      "3153 [D loss: 0.211268, acc.: 91.02%] [G loss: 3.173247]\n",
      "3154 [D loss: 0.213521, acc.: 91.41%] [G loss: 3.106696]\n",
      "3155 [D loss: 0.210914, acc.: 91.41%] [G loss: 3.125571]\n",
      "3156 [D loss: 0.219757, acc.: 89.84%] [G loss: 3.357738]\n",
      "3157 [D loss: 0.190685, acc.: 92.19%] [G loss: 3.088465]\n",
      "3158 [D loss: 0.227896, acc.: 90.23%] [G loss: 3.215074]\n",
      "3159 [D loss: 0.211845, acc.: 91.41%] [G loss: 2.992632]\n",
      "3160 [D loss: 0.224884, acc.: 91.41%] [G loss: 3.109975]\n",
      "3161 [D loss: 0.226222, acc.: 91.02%] [G loss: 3.100462]\n",
      "3162 [D loss: 0.187507, acc.: 92.97%] [G loss: 3.256748]\n",
      "3163 [D loss: 0.216763, acc.: 92.19%] [G loss: 3.283087]\n",
      "3164 [D loss: 0.220836, acc.: 91.80%] [G loss: 3.158604]\n",
      "3165 [D loss: 0.188500, acc.: 92.97%] [G loss: 3.483452]\n",
      "3166 [D loss: 0.214957, acc.: 91.41%] [G loss: 3.172903]\n",
      "3167 [D loss: 0.220999, acc.: 91.41%] [G loss: 3.181406]\n",
      "3168 [D loss: 0.214598, acc.: 91.41%] [G loss: 3.230310]\n",
      "3169 [D loss: 0.204998, acc.: 90.23%] [G loss: 3.136363]\n",
      "3170 [D loss: 0.196741, acc.: 91.80%] [G loss: 2.899514]\n",
      "3171 [D loss: 0.230022, acc.: 91.80%] [G loss: 3.065900]\n",
      "3172 [D loss: 0.201079, acc.: 91.80%] [G loss: 3.368276]\n",
      "3173 [D loss: 0.191887, acc.: 91.80%] [G loss: 3.103655]\n",
      "3174 [D loss: 0.200310, acc.: 91.02%] [G loss: 3.357081]\n",
      "3175 [D loss: 0.212763, acc.: 91.41%] [G loss: 2.996772]\n",
      "3176 [D loss: 0.206079, acc.: 91.41%] [G loss: 3.586283]\n",
      "3177 [D loss: 0.212782, acc.: 91.02%] [G loss: 3.352648]\n",
      "3178 [D loss: 0.195341, acc.: 91.02%] [G loss: 3.010005]\n",
      "3179 [D loss: 0.197884, acc.: 91.02%] [G loss: 3.249298]\n",
      "3180 [D loss: 0.199229, acc.: 92.19%] [G loss: 3.025589]\n",
      "3181 [D loss: 0.188968, acc.: 92.19%] [G loss: 3.450088]\n",
      "3182 [D loss: 0.212510, acc.: 91.41%] [G loss: 3.545036]\n",
      "3183 [D loss: 0.216220, acc.: 91.41%] [G loss: 3.211382]\n",
      "3184 [D loss: 0.225765, acc.: 90.23%] [G loss: 3.252222]\n",
      "3185 [D loss: 0.199743, acc.: 92.97%] [G loss: 3.278162]\n",
      "3186 [D loss: 0.208404, acc.: 91.80%] [G loss: 3.452623]\n",
      "3187 [D loss: 0.189848, acc.: 92.19%] [G loss: 3.202784]\n",
      "3188 [D loss: 0.226660, acc.: 89.84%] [G loss: 2.968460]\n",
      "3189 [D loss: 0.170297, acc.: 93.36%] [G loss: 3.265767]\n",
      "3190 [D loss: 0.228343, acc.: 90.23%] [G loss: 3.592309]\n",
      "3191 [D loss: 0.219568, acc.: 91.02%] [G loss: 3.231797]\n",
      "3192 [D loss: 0.201338, acc.: 90.23%] [G loss: 3.460486]\n",
      "3193 [D loss: 0.186798, acc.: 91.80%] [G loss: 3.250945]\n",
      "3194 [D loss: 0.206447, acc.: 91.41%] [G loss: 3.362270]\n",
      "3195 [D loss: 0.209608, acc.: 91.41%] [G loss: 3.180187]\n",
      "3196 [D loss: 0.217882, acc.: 91.80%] [G loss: 3.109370]\n",
      "3197 [D loss: 0.196314, acc.: 91.41%] [G loss: 3.198680]\n",
      "3198 [D loss: 0.205602, acc.: 93.36%] [G loss: 3.175334]\n",
      "3199 [D loss: 0.179981, acc.: 92.58%] [G loss: 3.106400]\n",
      "3200 [D loss: 0.220477, acc.: 91.02%] [G loss: 3.242038]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "3201 [D loss: 0.233347, acc.: 90.62%] [G loss: 3.309777]\n",
      "3202 [D loss: 0.232011, acc.: 89.84%] [G loss: 3.789644]\n",
      "3203 [D loss: 0.214172, acc.: 91.80%] [G loss: 3.185011]\n",
      "3204 [D loss: 0.189801, acc.: 92.58%] [G loss: 3.357525]\n",
      "3205 [D loss: 0.233599, acc.: 89.45%] [G loss: 3.424223]\n",
      "3206 [D loss: 0.186851, acc.: 92.58%] [G loss: 3.215299]\n",
      "3207 [D loss: 0.198277, acc.: 90.62%] [G loss: 3.543712]\n",
      "3208 [D loss: 0.225405, acc.: 91.02%] [G loss: 3.447191]\n",
      "3209 [D loss: 0.239403, acc.: 91.02%] [G loss: 3.406028]\n",
      "3210 [D loss: 0.221166, acc.: 90.62%] [G loss: 3.022648]\n",
      "3211 [D loss: 0.203324, acc.: 92.19%] [G loss: 3.099089]\n",
      "3212 [D loss: 0.211060, acc.: 91.80%] [G loss: 3.149696]\n",
      "3213 [D loss: 0.184697, acc.: 92.97%] [G loss: 3.129386]\n",
      "3214 [D loss: 0.220664, acc.: 91.02%] [G loss: 3.278293]\n",
      "3215 [D loss: 0.226972, acc.: 92.19%] [G loss: 3.217737]\n",
      "3216 [D loss: 0.205877, acc.: 92.19%] [G loss: 3.291633]\n",
      "3217 [D loss: 0.216176, acc.: 92.19%] [G loss: 3.069711]\n",
      "3218 [D loss: 0.204579, acc.: 91.41%] [G loss: 3.121467]\n",
      "3219 [D loss: 0.214221, acc.: 90.23%] [G loss: 3.542645]\n",
      "3220 [D loss: 0.208283, acc.: 91.41%] [G loss: 3.052250]\n",
      "3221 [D loss: 0.196289, acc.: 91.80%] [G loss: 3.318716]\n",
      "3222 [D loss: 0.190922, acc.: 91.80%] [G loss: 3.057340]\n",
      "3223 [D loss: 0.212303, acc.: 91.02%] [G loss: 3.204530]\n",
      "3224 [D loss: 0.198862, acc.: 92.19%] [G loss: 2.968183]\n",
      "3225 [D loss: 0.225579, acc.: 89.84%] [G loss: 3.128552]\n",
      "3226 [D loss: 0.192476, acc.: 92.97%] [G loss: 3.273734]\n",
      "3227 [D loss: 0.214307, acc.: 90.62%] [G loss: 2.927193]\n",
      "3228 [D loss: 0.191715, acc.: 92.19%] [G loss: 3.149623]\n",
      "3229 [D loss: 0.213296, acc.: 90.62%] [G loss: 2.993270]\n",
      "3230 [D loss: 0.213900, acc.: 91.80%] [G loss: 2.925036]\n",
      "3231 [D loss: 0.206615, acc.: 91.80%] [G loss: 3.291905]\n",
      "3232 [D loss: 0.224509, acc.: 89.84%] [G loss: 3.253692]\n",
      "3233 [D loss: 0.189308, acc.: 92.58%] [G loss: 2.973036]\n",
      "3234 [D loss: 0.211181, acc.: 92.97%] [G loss: 3.090702]\n",
      "3235 [D loss: 0.203015, acc.: 91.41%] [G loss: 3.408561]\n",
      "3236 [D loss: 0.207118, acc.: 91.41%] [G loss: 3.145394]\n",
      "3237 [D loss: 0.210783, acc.: 90.62%] [G loss: 3.174033]\n",
      "3238 [D loss: 0.255086, acc.: 89.45%] [G loss: 3.564940]\n",
      "3239 [D loss: 0.212375, acc.: 91.80%] [G loss: 3.103292]\n",
      "3240 [D loss: 0.214791, acc.: 90.62%] [G loss: 3.240141]\n",
      "3241 [D loss: 0.194393, acc.: 92.19%] [G loss: 3.110832]\n",
      "3242 [D loss: 0.183421, acc.: 92.19%] [G loss: 3.444480]\n",
      "3243 [D loss: 0.186419, acc.: 91.41%] [G loss: 3.539167]\n",
      "3244 [D loss: 0.185226, acc.: 91.80%] [G loss: 3.225130]\n",
      "3245 [D loss: 0.200214, acc.: 91.41%] [G loss: 3.152427]\n",
      "3246 [D loss: 0.184302, acc.: 91.80%] [G loss: 3.147004]\n",
      "3247 [D loss: 0.195897, acc.: 91.80%] [G loss: 3.287860]\n",
      "3248 [D loss: 0.224198, acc.: 91.80%] [G loss: 3.465256]\n",
      "3249 [D loss: 0.216325, acc.: 92.58%] [G loss: 3.205582]\n",
      "3250 [D loss: 0.239991, acc.: 89.06%] [G loss: 3.230465]\n",
      "3251 [D loss: 0.186486, acc.: 92.58%] [G loss: 3.505818]\n",
      "3252 [D loss: 0.199293, acc.: 91.80%] [G loss: 3.062879]\n",
      "3253 [D loss: 0.215882, acc.: 89.06%] [G loss: 3.455777]\n",
      "3254 [D loss: 0.206481, acc.: 90.62%] [G loss: 3.364944]\n",
      "3255 [D loss: 0.239839, acc.: 90.23%] [G loss: 3.098241]\n",
      "3256 [D loss: 0.213441, acc.: 91.02%] [G loss: 3.474855]\n",
      "3257 [D loss: 0.207478, acc.: 92.58%] [G loss: 2.982246]\n",
      "3258 [D loss: 0.218163, acc.: 91.80%] [G loss: 3.177981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3259 [D loss: 0.221804, acc.: 90.62%] [G loss: 3.052696]\n",
      "3260 [D loss: 0.193144, acc.: 92.19%] [G loss: 2.881387]\n",
      "3261 [D loss: 0.218332, acc.: 91.41%] [G loss: 3.051818]\n",
      "3262 [D loss: 0.183322, acc.: 92.58%] [G loss: 2.914141]\n",
      "3263 [D loss: 0.202424, acc.: 92.19%] [G loss: 3.057936]\n",
      "3264 [D loss: 0.200236, acc.: 91.02%] [G loss: 2.961039]\n",
      "3265 [D loss: 0.206610, acc.: 91.80%] [G loss: 3.192878]\n",
      "3266 [D loss: 0.197734, acc.: 93.36%] [G loss: 2.864965]\n",
      "3267 [D loss: 0.212524, acc.: 91.02%] [G loss: 3.254114]\n",
      "3268 [D loss: 0.209803, acc.: 92.19%] [G loss: 2.847898]\n",
      "3269 [D loss: 0.207668, acc.: 91.02%] [G loss: 3.192830]\n",
      "3270 [D loss: 0.194166, acc.: 91.02%] [G loss: 3.168416]\n",
      "3271 [D loss: 0.220314, acc.: 90.23%] [G loss: 2.955105]\n",
      "3272 [D loss: 0.224404, acc.: 91.41%] [G loss: 3.072029]\n",
      "3273 [D loss: 0.196743, acc.: 91.41%] [G loss: 3.283156]\n",
      "3274 [D loss: 0.223828, acc.: 91.41%] [G loss: 3.260068]\n",
      "3275 [D loss: 0.204185, acc.: 91.41%] [G loss: 3.050850]\n",
      "3276 [D loss: 0.194490, acc.: 91.41%] [G loss: 3.066475]\n",
      "3277 [D loss: 0.195522, acc.: 91.41%] [G loss: 2.909095]\n",
      "3278 [D loss: 0.212444, acc.: 90.23%] [G loss: 3.124650]\n",
      "3279 [D loss: 0.202820, acc.: 91.41%] [G loss: 3.155084]\n",
      "3280 [D loss: 0.215668, acc.: 91.80%] [G loss: 3.433294]\n",
      "3281 [D loss: 0.210779, acc.: 91.41%] [G loss: 2.975239]\n",
      "3282 [D loss: 0.204497, acc.: 91.80%] [G loss: 3.140426]\n",
      "3283 [D loss: 0.204102, acc.: 91.02%] [G loss: 2.949132]\n",
      "3284 [D loss: 0.202639, acc.: 91.80%] [G loss: 3.434707]\n",
      "3285 [D loss: 0.206475, acc.: 91.02%] [G loss: 3.111924]\n",
      "3286 [D loss: 0.213133, acc.: 91.80%] [G loss: 2.986420]\n",
      "3287 [D loss: 0.184574, acc.: 92.19%] [G loss: 3.543615]\n",
      "3288 [D loss: 0.188088, acc.: 91.41%] [G loss: 3.177788]\n",
      "3289 [D loss: 0.183980, acc.: 91.41%] [G loss: 3.195214]\n",
      "3290 [D loss: 0.224933, acc.: 91.02%] [G loss: 3.012475]\n",
      "3291 [D loss: 0.198253, acc.: 91.80%] [G loss: 3.543246]\n",
      "3292 [D loss: 0.220965, acc.: 90.23%] [G loss: 3.899518]\n",
      "3293 [D loss: 0.169843, acc.: 93.75%] [G loss: 3.239940]\n",
      "3294 [D loss: 0.210026, acc.: 91.02%] [G loss: 3.729266]\n",
      "3295 [D loss: 0.193041, acc.: 92.19%] [G loss: 3.153130]\n",
      "3296 [D loss: 0.207865, acc.: 91.80%] [G loss: 3.270716]\n",
      "3297 [D loss: 0.197157, acc.: 92.19%] [G loss: 3.304580]\n",
      "3298 [D loss: 0.194056, acc.: 92.58%] [G loss: 3.290468]\n",
      "3299 [D loss: 0.176167, acc.: 92.19%] [G loss: 3.201046]\n",
      "3300 [D loss: 0.194867, acc.: 91.41%] [G loss: 3.423197]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "3301 [D loss: 0.204436, acc.: 91.41%] [G loss: 3.046041]\n",
      "3302 [D loss: 0.240118, acc.: 89.84%] [G loss: 3.287334]\n",
      "3303 [D loss: 0.222726, acc.: 91.02%] [G loss: 3.259734]\n",
      "3304 [D loss: 0.208611, acc.: 90.62%] [G loss: 3.129224]\n",
      "3305 [D loss: 0.206734, acc.: 89.84%] [G loss: 3.329067]\n",
      "3306 [D loss: 0.215956, acc.: 91.80%] [G loss: 3.126997]\n",
      "3307 [D loss: 0.210619, acc.: 90.62%] [G loss: 3.354697]\n",
      "3308 [D loss: 0.210876, acc.: 90.62%] [G loss: 3.377768]\n",
      "3309 [D loss: 0.212928, acc.: 91.02%] [G loss: 3.066642]\n",
      "3310 [D loss: 0.185660, acc.: 93.75%] [G loss: 3.040122]\n",
      "3311 [D loss: 0.190607, acc.: 92.19%] [G loss: 3.075358]\n",
      "3312 [D loss: 0.203459, acc.: 92.58%] [G loss: 3.177912]\n",
      "3313 [D loss: 0.218927, acc.: 91.80%] [G loss: 3.132863]\n",
      "3314 [D loss: 0.206524, acc.: 91.80%] [G loss: 3.132871]\n",
      "3315 [D loss: 0.202158, acc.: 90.62%] [G loss: 3.361956]\n",
      "3316 [D loss: 0.198216, acc.: 91.80%] [G loss: 3.101040]\n",
      "3317 [D loss: 0.226388, acc.: 89.84%] [G loss: 3.408162]\n",
      "3318 [D loss: 0.185296, acc.: 92.19%] [G loss: 3.057657]\n",
      "3319 [D loss: 0.206773, acc.: 91.41%] [G loss: 3.119082]\n",
      "3320 [D loss: 0.192056, acc.: 91.41%] [G loss: 3.180272]\n",
      "3321 [D loss: 0.194784, acc.: 91.80%] [G loss: 3.075628]\n",
      "3322 [D loss: 0.201608, acc.: 90.62%] [G loss: 3.097691]\n",
      "3323 [D loss: 0.186132, acc.: 92.19%] [G loss: 3.156557]\n",
      "3324 [D loss: 0.187775, acc.: 91.80%] [G loss: 3.284870]\n",
      "3325 [D loss: 0.215513, acc.: 92.19%] [G loss: 3.137035]\n",
      "3326 [D loss: 0.187866, acc.: 92.19%] [G loss: 3.442157]\n",
      "3327 [D loss: 0.200156, acc.: 91.80%] [G loss: 2.881019]\n",
      "3328 [D loss: 0.205317, acc.: 91.41%] [G loss: 3.459813]\n",
      "3329 [D loss: 0.181805, acc.: 92.58%] [G loss: 3.317702]\n",
      "3330 [D loss: 0.199307, acc.: 91.80%] [G loss: 2.967425]\n",
      "3331 [D loss: 0.192614, acc.: 92.19%] [G loss: 2.976670]\n",
      "3332 [D loss: 0.205720, acc.: 92.19%] [G loss: 3.013364]\n",
      "3333 [D loss: 0.205126, acc.: 91.80%] [G loss: 2.977066]\n",
      "3334 [D loss: 0.241423, acc.: 90.62%] [G loss: 3.026599]\n",
      "3335 [D loss: 0.210388, acc.: 91.41%] [G loss: 3.282688]\n",
      "3336 [D loss: 0.201090, acc.: 91.80%] [G loss: 3.361847]\n",
      "3337 [D loss: 0.201607, acc.: 92.19%] [G loss: 3.086428]\n",
      "3338 [D loss: 0.216182, acc.: 91.80%] [G loss: 3.332206]\n",
      "3339 [D loss: 0.186282, acc.: 92.19%] [G loss: 3.354530]\n",
      "3340 [D loss: 0.195564, acc.: 92.58%] [G loss: 3.057531]\n",
      "3341 [D loss: 0.188130, acc.: 92.19%] [G loss: 3.121835]\n",
      "3342 [D loss: 0.184840, acc.: 91.80%] [G loss: 3.038897]\n",
      "3343 [D loss: 0.239975, acc.: 89.45%] [G loss: 3.194450]\n",
      "3344 [D loss: 0.226735, acc.: 90.62%] [G loss: 3.122894]\n",
      "3345 [D loss: 0.210561, acc.: 91.41%] [G loss: 3.134728]\n",
      "3346 [D loss: 0.189657, acc.: 92.19%] [G loss: 2.938384]\n",
      "3347 [D loss: 0.195676, acc.: 92.19%] [G loss: 3.015535]\n",
      "3348 [D loss: 0.200050, acc.: 91.41%] [G loss: 3.317209]\n",
      "3349 [D loss: 0.231600, acc.: 90.23%] [G loss: 2.953035]\n",
      "3350 [D loss: 0.187884, acc.: 91.80%] [G loss: 3.407667]\n",
      "3351 [D loss: 0.187800, acc.: 92.58%] [G loss: 3.254326]\n",
      "3352 [D loss: 0.208541, acc.: 90.62%] [G loss: 3.320827]\n",
      "3353 [D loss: 0.202574, acc.: 91.41%] [G loss: 3.153592]\n",
      "3354 [D loss: 0.203463, acc.: 90.23%] [G loss: 3.329486]\n",
      "3355 [D loss: 0.242585, acc.: 89.84%] [G loss: 3.292231]\n",
      "3356 [D loss: 0.187921, acc.: 92.58%] [G loss: 3.152986]\n",
      "3357 [D loss: 0.199888, acc.: 90.62%] [G loss: 2.989449]\n",
      "3358 [D loss: 0.206893, acc.: 90.62%] [G loss: 3.305603]\n",
      "3359 [D loss: 0.183902, acc.: 92.97%] [G loss: 3.104010]\n",
      "3360 [D loss: 0.185263, acc.: 91.80%] [G loss: 3.246132]\n",
      "3361 [D loss: 0.207580, acc.: 91.80%] [G loss: 3.048750]\n",
      "3362 [D loss: 0.208506, acc.: 91.02%] [G loss: 3.394691]\n",
      "3363 [D loss: 0.203413, acc.: 91.41%] [G loss: 3.085441]\n",
      "3364 [D loss: 0.202481, acc.: 91.02%] [G loss: 3.439817]\n",
      "3365 [D loss: 0.225954, acc.: 91.80%] [G loss: 3.137878]\n",
      "3366 [D loss: 0.195462, acc.: 91.02%] [G loss: 2.973507]\n",
      "3367 [D loss: 0.206593, acc.: 91.02%] [G loss: 3.581103]\n",
      "3368 [D loss: 0.195514, acc.: 92.97%] [G loss: 3.169446]\n",
      "3369 [D loss: 0.226299, acc.: 90.23%] [G loss: 3.044584]\n",
      "3370 [D loss: 0.205382, acc.: 91.41%] [G loss: 2.903177]\n",
      "3371 [D loss: 0.207679, acc.: 91.80%] [G loss: 3.090264]\n",
      "3372 [D loss: 0.209829, acc.: 90.62%] [G loss: 3.092697]\n",
      "3373 [D loss: 0.196746, acc.: 91.80%] [G loss: 3.353837]\n",
      "3374 [D loss: 0.205223, acc.: 91.41%] [G loss: 3.495733]\n",
      "3375 [D loss: 0.212118, acc.: 91.80%] [G loss: 2.866082]\n",
      "3376 [D loss: 0.205252, acc.: 91.80%] [G loss: 3.131950]\n",
      "3377 [D loss: 0.201209, acc.: 91.80%] [G loss: 3.265904]\n",
      "3378 [D loss: 0.193172, acc.: 93.36%] [G loss: 3.139741]\n",
      "3379 [D loss: 0.202517, acc.: 92.97%] [G loss: 3.188735]\n",
      "3380 [D loss: 0.198484, acc.: 91.80%] [G loss: 3.051961]\n",
      "3381 [D loss: 0.202068, acc.: 91.80%] [G loss: 3.599007]\n",
      "3382 [D loss: 0.199420, acc.: 92.19%] [G loss: 3.014851]\n",
      "3383 [D loss: 0.167113, acc.: 93.36%] [G loss: 3.329414]\n",
      "3384 [D loss: 0.175882, acc.: 92.97%] [G loss: 3.186007]\n",
      "3385 [D loss: 0.193070, acc.: 92.19%] [G loss: 3.187698]\n",
      "3386 [D loss: 0.187661, acc.: 91.02%] [G loss: 3.493646]\n",
      "3387 [D loss: 0.231391, acc.: 91.41%] [G loss: 3.113063]\n",
      "3388 [D loss: 0.177212, acc.: 93.36%] [G loss: 3.193049]\n",
      "3389 [D loss: 0.195297, acc.: 92.19%] [G loss: 3.130418]\n",
      "3390 [D loss: 0.210413, acc.: 91.02%] [G loss: 3.523447]\n",
      "3391 [D loss: 0.190691, acc.: 91.80%] [G loss: 3.497752]\n",
      "3392 [D loss: 0.211797, acc.: 90.62%] [G loss: 3.006942]\n",
      "3393 [D loss: 0.212316, acc.: 91.02%] [G loss: 3.160678]\n",
      "3394 [D loss: 0.193283, acc.: 92.19%] [G loss: 3.317650]\n",
      "3395 [D loss: 0.206102, acc.: 92.58%] [G loss: 3.375813]\n",
      "3396 [D loss: 0.198156, acc.: 93.36%] [G loss: 3.267814]\n",
      "3397 [D loss: 0.198181, acc.: 91.80%] [G loss: 3.358969]\n",
      "3398 [D loss: 0.205360, acc.: 91.02%] [G loss: 2.954949]\n",
      "3399 [D loss: 0.180414, acc.: 92.19%] [G loss: 3.526258]\n",
      "3400 [D loss: 0.182956, acc.: 92.19%] [G loss: 3.231486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "3401 [D loss: 0.199888, acc.: 91.80%] [G loss: 3.384395]\n",
      "3402 [D loss: 0.183790, acc.: 91.41%] [G loss: 3.559539]\n",
      "3403 [D loss: 0.247505, acc.: 89.06%] [G loss: 3.157753]\n",
      "3404 [D loss: 0.228188, acc.: 91.02%] [G loss: 2.892535]\n",
      "3405 [D loss: 0.196412, acc.: 91.41%] [G loss: 3.192333]\n",
      "3406 [D loss: 0.212600, acc.: 90.62%] [G loss: 3.312400]\n",
      "3407 [D loss: 0.199043, acc.: 90.62%] [G loss: 3.200106]\n",
      "3408 [D loss: 0.215473, acc.: 92.19%] [G loss: 3.063544]\n",
      "3409 [D loss: 0.199854, acc.: 91.80%] [G loss: 3.150291]\n",
      "3410 [D loss: 0.194444, acc.: 91.02%] [G loss: 3.078541]\n",
      "3411 [D loss: 0.192147, acc.: 92.58%] [G loss: 3.214370]\n",
      "3412 [D loss: 0.205374, acc.: 91.80%] [G loss: 2.935696]\n",
      "3413 [D loss: 0.189451, acc.: 91.80%] [G loss: 3.278944]\n",
      "3414 [D loss: 0.196999, acc.: 90.62%] [G loss: 3.388503]\n",
      "3415 [D loss: 0.178647, acc.: 91.02%] [G loss: 3.143029]\n",
      "3416 [D loss: 0.208887, acc.: 91.80%] [G loss: 3.255010]\n",
      "3417 [D loss: 0.184271, acc.: 92.19%] [G loss: 3.265577]\n",
      "3418 [D loss: 0.201014, acc.: 92.97%] [G loss: 2.925097]\n",
      "3419 [D loss: 0.215854, acc.: 91.02%] [G loss: 3.435017]\n",
      "3420 [D loss: 0.184750, acc.: 91.80%] [G loss: 3.128067]\n",
      "3421 [D loss: 0.232211, acc.: 91.02%] [G loss: 3.142759]\n",
      "3422 [D loss: 0.205194, acc.: 90.62%] [G loss: 3.203891]\n",
      "3423 [D loss: 0.199145, acc.: 92.58%] [G loss: 3.110902]\n",
      "3424 [D loss: 0.200605, acc.: 91.41%] [G loss: 3.271777]\n",
      "3425 [D loss: 0.204545, acc.: 92.19%] [G loss: 3.043434]\n",
      "3426 [D loss: 0.212967, acc.: 91.80%] [G loss: 3.057542]\n",
      "3427 [D loss: 0.206067, acc.: 91.41%] [G loss: 3.198545]\n",
      "3428 [D loss: 0.210265, acc.: 92.19%] [G loss: 2.977530]\n",
      "3429 [D loss: 0.232768, acc.: 91.80%] [G loss: 3.169768]\n",
      "3430 [D loss: 0.203023, acc.: 91.41%] [G loss: 3.455945]\n",
      "3431 [D loss: 0.216798, acc.: 91.41%] [G loss: 3.156199]\n",
      "3432 [D loss: 0.215665, acc.: 91.41%] [G loss: 3.038802]\n",
      "3433 [D loss: 0.208848, acc.: 91.41%] [G loss: 3.194989]\n",
      "3434 [D loss: 0.198090, acc.: 91.80%] [G loss: 3.266776]\n",
      "3435 [D loss: 0.211466, acc.: 91.41%] [G loss: 3.069241]\n",
      "3436 [D loss: 0.214423, acc.: 91.80%] [G loss: 3.487784]\n",
      "3437 [D loss: 0.201587, acc.: 91.41%] [G loss: 3.184204]\n",
      "3438 [D loss: 0.198936, acc.: 92.97%] [G loss: 3.231566]\n",
      "3439 [D loss: 0.215564, acc.: 91.02%] [G loss: 3.296341]\n",
      "3440 [D loss: 0.183698, acc.: 92.19%] [G loss: 3.098966]\n",
      "3441 [D loss: 0.173776, acc.: 93.36%] [G loss: 3.211058]\n",
      "3442 [D loss: 0.186522, acc.: 92.97%] [G loss: 3.354913]\n",
      "3443 [D loss: 0.203238, acc.: 91.80%] [G loss: 3.049506]\n",
      "3444 [D loss: 0.180581, acc.: 92.97%] [G loss: 2.924998]\n",
      "3445 [D loss: 0.183035, acc.: 91.41%] [G loss: 3.434848]\n",
      "3446 [D loss: 0.197751, acc.: 92.58%] [G loss: 3.618381]\n",
      "3447 [D loss: 0.217765, acc.: 91.80%] [G loss: 3.166264]\n",
      "3448 [D loss: 0.190052, acc.: 91.80%] [G loss: 3.651384]\n",
      "3449 [D loss: 0.215328, acc.: 91.80%] [G loss: 3.234695]\n",
      "3450 [D loss: 0.184412, acc.: 92.58%] [G loss: 3.236820]\n",
      "3451 [D loss: 0.209580, acc.: 90.62%] [G loss: 3.239303]\n",
      "3452 [D loss: 0.176366, acc.: 92.97%] [G loss: 3.362299]\n",
      "3453 [D loss: 0.212707, acc.: 91.41%] [G loss: 3.405751]\n",
      "3454 [D loss: 0.192070, acc.: 91.02%] [G loss: 3.221224]\n",
      "3455 [D loss: 0.193546, acc.: 93.36%] [G loss: 3.434537]\n",
      "3456 [D loss: 0.201897, acc.: 91.80%] [G loss: 3.132227]\n",
      "3457 [D loss: 0.175700, acc.: 92.97%] [G loss: 3.534909]\n",
      "3458 [D loss: 0.219161, acc.: 91.02%] [G loss: 3.423813]\n",
      "3459 [D loss: 0.213034, acc.: 90.62%] [G loss: 3.240185]\n",
      "3460 [D loss: 0.208896, acc.: 90.23%] [G loss: 2.991943]\n",
      "3461 [D loss: 0.201586, acc.: 91.02%] [G loss: 3.270989]\n",
      "3462 [D loss: 0.232444, acc.: 90.62%] [G loss: 3.289650]\n",
      "3463 [D loss: 0.186969, acc.: 92.19%] [G loss: 3.212525]\n",
      "3464 [D loss: 0.224856, acc.: 90.62%] [G loss: 3.316032]\n",
      "3465 [D loss: 0.221924, acc.: 92.19%] [G loss: 3.276969]\n",
      "3466 [D loss: 0.219998, acc.: 90.23%] [G loss: 3.052769]\n",
      "3467 [D loss: 0.253837, acc.: 90.62%] [G loss: 3.270890]\n",
      "3468 [D loss: 0.214348, acc.: 91.80%] [G loss: 3.038496]\n",
      "3469 [D loss: 0.199509, acc.: 91.41%] [G loss: 3.654706]\n",
      "3470 [D loss: 0.211280, acc.: 91.41%] [G loss: 3.370986]\n",
      "3471 [D loss: 0.212970, acc.: 91.02%] [G loss: 3.199953]\n",
      "3472 [D loss: 0.205151, acc.: 91.41%] [G loss: 2.994715]\n",
      "3473 [D loss: 0.204190, acc.: 91.41%] [G loss: 3.594696]\n",
      "3474 [D loss: 0.227532, acc.: 90.62%] [G loss: 3.181519]\n",
      "3475 [D loss: 0.186354, acc.: 91.80%] [G loss: 3.186692]\n",
      "3476 [D loss: 0.180762, acc.: 92.58%] [G loss: 3.562719]\n",
      "3477 [D loss: 0.219606, acc.: 90.23%] [G loss: 3.534635]\n",
      "3478 [D loss: 0.214188, acc.: 91.80%] [G loss: 3.073126]\n",
      "3479 [D loss: 0.173559, acc.: 92.58%] [G loss: 3.284083]\n",
      "3480 [D loss: 0.194882, acc.: 91.41%] [G loss: 3.209860]\n",
      "3481 [D loss: 0.214448, acc.: 92.19%] [G loss: 3.252663]\n",
      "3482 [D loss: 0.230268, acc.: 90.23%] [G loss: 3.246036]\n",
      "3483 [D loss: 0.211829, acc.: 90.23%] [G loss: 3.085364]\n",
      "3484 [D loss: 0.222512, acc.: 92.58%] [G loss: 3.048642]\n",
      "3485 [D loss: 0.200077, acc.: 91.02%] [G loss: 3.313485]\n",
      "3486 [D loss: 0.195930, acc.: 91.80%] [G loss: 3.076320]\n",
      "3487 [D loss: 0.204898, acc.: 92.97%] [G loss: 3.010356]\n",
      "3488 [D loss: 0.193274, acc.: 91.80%] [G loss: 3.275409]\n",
      "3489 [D loss: 0.202815, acc.: 92.19%] [G loss: 3.297534]\n",
      "3490 [D loss: 0.222362, acc.: 91.02%] [G loss: 2.883110]\n",
      "3491 [D loss: 0.220489, acc.: 90.62%] [G loss: 3.388790]\n",
      "3492 [D loss: 0.202434, acc.: 91.80%] [G loss: 3.027973]\n",
      "3493 [D loss: 0.204442, acc.: 91.02%] [G loss: 3.123470]\n",
      "3494 [D loss: 0.213130, acc.: 91.02%] [G loss: 3.098265]\n",
      "3495 [D loss: 0.230077, acc.: 91.02%] [G loss: 3.397598]\n",
      "3496 [D loss: 0.203387, acc.: 92.97%] [G loss: 3.248829]\n",
      "3497 [D loss: 0.219746, acc.: 90.62%] [G loss: 3.423416]\n",
      "3498 [D loss: 0.205337, acc.: 91.41%] [G loss: 3.328970]\n",
      "3499 [D loss: 0.206828, acc.: 90.23%] [G loss: 3.080381]\n",
      "3500 [D loss: 0.200332, acc.: 91.02%] [G loss: 3.359138]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "3501 [D loss: 0.200881, acc.: 91.80%] [G loss: 3.345138]\n",
      "3502 [D loss: 0.207642, acc.: 91.41%] [G loss: 3.522139]\n",
      "3503 [D loss: 0.211209, acc.: 91.02%] [G loss: 3.339550]\n",
      "3504 [D loss: 0.205045, acc.: 91.80%] [G loss: 3.176104]\n",
      "3505 [D loss: 0.205245, acc.: 91.02%] [G loss: 2.913301]\n",
      "3506 [D loss: 0.203442, acc.: 91.80%] [G loss: 3.436049]\n",
      "3507 [D loss: 0.225658, acc.: 90.23%] [G loss: 3.111787]\n",
      "3508 [D loss: 0.203638, acc.: 92.97%] [G loss: 3.224819]\n",
      "3509 [D loss: 0.217093, acc.: 89.84%] [G loss: 3.167728]\n",
      "3510 [D loss: 0.203290, acc.: 90.62%] [G loss: 3.131150]\n",
      "3511 [D loss: 0.209082, acc.: 91.02%] [G loss: 3.015864]\n",
      "3512 [D loss: 0.195366, acc.: 92.58%] [G loss: 3.295669]\n",
      "3513 [D loss: 0.235800, acc.: 90.62%] [G loss: 3.044626]\n",
      "3514 [D loss: 0.221227, acc.: 91.80%] [G loss: 3.153354]\n",
      "3515 [D loss: 0.209878, acc.: 92.97%] [G loss: 3.219437]\n",
      "3516 [D loss: 0.180315, acc.: 92.58%] [G loss: 3.122778]\n",
      "3517 [D loss: 0.201311, acc.: 92.19%] [G loss: 3.169085]\n",
      "3518 [D loss: 0.205073, acc.: 91.41%] [G loss: 2.956802]\n",
      "3519 [D loss: 0.210849, acc.: 90.62%] [G loss: 3.159738]\n",
      "3520 [D loss: 0.214037, acc.: 91.41%] [G loss: 3.156380]\n",
      "3521 [D loss: 0.193938, acc.: 92.97%] [G loss: 3.064875]\n",
      "3522 [D loss: 0.192192, acc.: 91.41%] [G loss: 2.869412]\n",
      "3523 [D loss: 0.211601, acc.: 91.02%] [G loss: 2.999356]\n",
      "3524 [D loss: 0.201754, acc.: 90.23%] [G loss: 3.408705]\n",
      "3525 [D loss: 0.222320, acc.: 90.62%] [G loss: 3.326424]\n",
      "3526 [D loss: 0.234158, acc.: 92.19%] [G loss: 3.050446]\n",
      "3527 [D loss: 0.214048, acc.: 91.80%] [G loss: 3.130898]\n",
      "3528 [D loss: 0.227374, acc.: 91.02%] [G loss: 3.291390]\n",
      "3529 [D loss: 0.210508, acc.: 91.02%] [G loss: 3.314030]\n",
      "3530 [D loss: 0.199012, acc.: 91.80%] [G loss: 3.211174]\n",
      "3531 [D loss: 0.183834, acc.: 93.36%] [G loss: 3.360188]\n",
      "3532 [D loss: 0.230243, acc.: 90.62%] [G loss: 3.496131]\n",
      "3533 [D loss: 0.195506, acc.: 91.41%] [G loss: 3.263905]\n",
      "3534 [D loss: 0.211483, acc.: 91.02%] [G loss: 3.170125]\n",
      "3535 [D loss: 0.176444, acc.: 94.14%] [G loss: 2.983826]\n",
      "3536 [D loss: 0.218284, acc.: 90.62%] [G loss: 3.249749]\n",
      "3537 [D loss: 0.201202, acc.: 93.36%] [G loss: 3.137668]\n",
      "3538 [D loss: 0.196096, acc.: 92.58%] [G loss: 3.051308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3539 [D loss: 0.214877, acc.: 90.62%] [G loss: 3.057652]\n",
      "3540 [D loss: 0.189030, acc.: 92.58%] [G loss: 2.999772]\n",
      "3541 [D loss: 0.202623, acc.: 92.19%] [G loss: 2.999969]\n",
      "3542 [D loss: 0.186225, acc.: 92.19%] [G loss: 2.745382]\n",
      "3543 [D loss: 0.200401, acc.: 91.80%] [G loss: 3.135795]\n",
      "3544 [D loss: 0.212012, acc.: 91.41%] [G loss: 3.391410]\n",
      "3545 [D loss: 0.200401, acc.: 91.80%] [G loss: 3.120350]\n",
      "3546 [D loss: 0.207306, acc.: 91.80%] [G loss: 3.006004]\n",
      "3547 [D loss: 0.186383, acc.: 92.19%] [G loss: 3.103083]\n",
      "3548 [D loss: 0.191831, acc.: 91.41%] [G loss: 3.039757]\n",
      "3549 [D loss: 0.197032, acc.: 91.41%] [G loss: 2.942042]\n",
      "3550 [D loss: 0.223007, acc.: 91.02%] [G loss: 3.481015]\n",
      "3551 [D loss: 0.223520, acc.: 89.45%] [G loss: 3.213147]\n",
      "3552 [D loss: 0.225904, acc.: 91.02%] [G loss: 3.464876]\n",
      "3553 [D loss: 0.212674, acc.: 91.41%] [G loss: 3.335679]\n",
      "3554 [D loss: 0.218335, acc.: 91.02%] [G loss: 3.544358]\n",
      "3555 [D loss: 0.222206, acc.: 90.62%] [G loss: 2.921831]\n",
      "3556 [D loss: 0.197488, acc.: 91.02%] [G loss: 3.376242]\n",
      "3557 [D loss: 0.203797, acc.: 91.80%] [G loss: 3.284812]\n",
      "3558 [D loss: 0.168907, acc.: 92.97%] [G loss: 3.220714]\n",
      "3559 [D loss: 0.197858, acc.: 90.62%] [G loss: 3.094468]\n",
      "3560 [D loss: 0.178442, acc.: 92.58%] [G loss: 2.940017]\n",
      "3561 [D loss: 0.178712, acc.: 93.36%] [G loss: 3.280331]\n",
      "3562 [D loss: 0.198863, acc.: 91.02%] [G loss: 3.721843]\n",
      "3563 [D loss: 0.218374, acc.: 91.80%] [G loss: 3.063878]\n",
      "3564 [D loss: 0.218987, acc.: 91.80%] [G loss: 3.111959]\n",
      "3565 [D loss: 0.231807, acc.: 90.23%] [G loss: 3.516022]\n",
      "3566 [D loss: 0.232942, acc.: 91.02%] [G loss: 3.553245]\n",
      "3567 [D loss: 0.213727, acc.: 91.80%] [G loss: 3.235213]\n",
      "3568 [D loss: 0.204419, acc.: 91.02%] [G loss: 3.372140]\n",
      "3569 [D loss: 0.197697, acc.: 92.19%] [G loss: 2.933468]\n",
      "3570 [D loss: 0.204857, acc.: 91.41%] [G loss: 3.053205]\n",
      "3571 [D loss: 0.206036, acc.: 91.80%] [G loss: 3.246167]\n",
      "3572 [D loss: 0.207804, acc.: 91.80%] [G loss: 3.109733]\n",
      "3573 [D loss: 0.206437, acc.: 92.97%] [G loss: 2.982726]\n",
      "3574 [D loss: 0.199015, acc.: 91.41%] [G loss: 3.559538]\n",
      "3575 [D loss: 0.215398, acc.: 90.23%] [G loss: 3.425788]\n",
      "3576 [D loss: 0.217356, acc.: 90.62%] [G loss: 3.010424]\n",
      "3577 [D loss: 0.207812, acc.: 91.80%] [G loss: 3.174189]\n",
      "3578 [D loss: 0.199551, acc.: 89.45%] [G loss: 3.477295]\n",
      "3579 [D loss: 0.233892, acc.: 89.84%] [G loss: 2.928405]\n",
      "3580 [D loss: 0.199092, acc.: 91.41%] [G loss: 2.979393]\n",
      "3581 [D loss: 0.219915, acc.: 91.41%] [G loss: 3.224088]\n",
      "3582 [D loss: 0.196028, acc.: 92.97%] [G loss: 3.108834]\n",
      "3583 [D loss: 0.215755, acc.: 91.41%] [G loss: 3.162818]\n",
      "3584 [D loss: 0.205209, acc.: 91.02%] [G loss: 3.468694]\n",
      "3585 [D loss: 0.189730, acc.: 91.80%] [G loss: 3.508471]\n",
      "3586 [D loss: 0.173071, acc.: 93.36%] [G loss: 2.939883]\n",
      "3587 [D loss: 0.191237, acc.: 91.80%] [G loss: 3.488633]\n",
      "3588 [D loss: 0.205622, acc.: 91.41%] [G loss: 3.037556]\n",
      "3589 [D loss: 0.207520, acc.: 90.62%] [G loss: 3.195623]\n",
      "3590 [D loss: 0.209475, acc.: 90.62%] [G loss: 2.958317]\n",
      "3591 [D loss: 0.203615, acc.: 91.02%] [G loss: 3.949036]\n",
      "3592 [D loss: 0.203320, acc.: 92.58%] [G loss: 2.971817]\n",
      "3593 [D loss: 0.199782, acc.: 91.80%] [G loss: 3.294132]\n",
      "3594 [D loss: 0.183910, acc.: 92.58%] [G loss: 3.327823]\n",
      "3595 [D loss: 0.244418, acc.: 90.62%] [G loss: 3.204121]\n",
      "3596 [D loss: 0.199088, acc.: 91.41%] [G loss: 3.272796]\n",
      "3597 [D loss: 0.211845, acc.: 92.97%] [G loss: 3.125551]\n",
      "3598 [D loss: 0.215955, acc.: 90.23%] [G loss: 3.291241]\n",
      "3599 [D loss: 0.197485, acc.: 91.41%] [G loss: 3.254595]\n",
      "3600 [D loss: 0.215729, acc.: 91.02%] [G loss: 3.219957]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "3601 [D loss: 0.218571, acc.: 91.02%] [G loss: 3.172153]\n",
      "3602 [D loss: 0.217517, acc.: 91.02%] [G loss: 3.376812]\n",
      "3603 [D loss: 0.228754, acc.: 91.41%] [G loss: 3.086158]\n",
      "3604 [D loss: 0.206386, acc.: 91.80%] [G loss: 2.962709]\n",
      "3605 [D loss: 0.222753, acc.: 91.41%] [G loss: 2.975251]\n",
      "3606 [D loss: 0.205531, acc.: 92.58%] [G loss: 3.347155]\n",
      "3607 [D loss: 0.222016, acc.: 91.41%] [G loss: 3.339177]\n",
      "3608 [D loss: 0.232945, acc.: 91.02%] [G loss: 3.162384]\n",
      "3609 [D loss: 0.212280, acc.: 90.62%] [G loss: 3.154740]\n",
      "3610 [D loss: 0.201911, acc.: 91.02%] [G loss: 3.539739]\n",
      "3611 [D loss: 0.204798, acc.: 91.02%] [G loss: 3.313468]\n",
      "3612 [D loss: 0.189272, acc.: 92.19%] [G loss: 3.505732]\n",
      "3613 [D loss: 0.201243, acc.: 92.19%] [G loss: 3.242741]\n",
      "3614 [D loss: 0.198268, acc.: 90.62%] [G loss: 2.987220]\n",
      "3615 [D loss: 0.196843, acc.: 91.02%] [G loss: 3.511169]\n",
      "3616 [D loss: 0.195218, acc.: 91.41%] [G loss: 3.546014]\n",
      "3617 [D loss: 0.209844, acc.: 89.45%] [G loss: 3.143680]\n",
      "3618 [D loss: 0.194979, acc.: 92.19%] [G loss: 3.089011]\n",
      "3619 [D loss: 0.212146, acc.: 92.58%] [G loss: 3.185556]\n",
      "3620 [D loss: 0.194293, acc.: 90.23%] [G loss: 3.114282]\n",
      "3621 [D loss: 0.176716, acc.: 91.80%] [G loss: 3.519203]\n",
      "3622 [D loss: 0.205119, acc.: 91.02%] [G loss: 3.525873]\n",
      "3623 [D loss: 0.177554, acc.: 92.97%] [G loss: 3.137469]\n",
      "3624 [D loss: 0.192554, acc.: 92.58%] [G loss: 3.247808]\n",
      "3625 [D loss: 0.217893, acc.: 91.41%] [G loss: 3.486810]\n",
      "3626 [D loss: 0.246549, acc.: 88.28%] [G loss: 3.555065]\n",
      "3627 [D loss: 0.191948, acc.: 91.80%] [G loss: 3.133852]\n",
      "3628 [D loss: 0.228859, acc.: 91.41%] [G loss: 2.956368]\n",
      "3629 [D loss: 0.193791, acc.: 92.58%] [G loss: 3.361421]\n",
      "3630 [D loss: 0.226695, acc.: 89.45%] [G loss: 3.359683]\n",
      "3631 [D loss: 0.191401, acc.: 92.19%] [G loss: 3.260409]\n",
      "3632 [D loss: 0.210954, acc.: 91.02%] [G loss: 3.323049]\n",
      "3633 [D loss: 0.193203, acc.: 91.80%] [G loss: 3.267064]\n",
      "3634 [D loss: 0.208158, acc.: 91.02%] [G loss: 3.181439]\n",
      "3635 [D loss: 0.188502, acc.: 92.19%] [G loss: 2.961015]\n",
      "3636 [D loss: 0.203973, acc.: 91.41%] [G loss: 3.409936]\n",
      "3637 [D loss: 0.202789, acc.: 91.80%] [G loss: 3.223540]\n",
      "3638 [D loss: 0.208068, acc.: 92.19%] [G loss: 3.441162]\n",
      "3639 [D loss: 0.196852, acc.: 91.41%] [G loss: 3.536386]\n",
      "3640 [D loss: 0.226575, acc.: 90.23%] [G loss: 3.136045]\n",
      "3641 [D loss: 0.184755, acc.: 92.58%] [G loss: 3.150124]\n",
      "3642 [D loss: 0.207783, acc.: 91.02%] [G loss: 3.321934]\n",
      "3643 [D loss: 0.225021, acc.: 91.02%] [G loss: 3.167846]\n",
      "3644 [D loss: 0.208398, acc.: 92.19%] [G loss: 3.106853]\n",
      "3645 [D loss: 0.198234, acc.: 91.02%] [G loss: 3.336578]\n",
      "3646 [D loss: 0.215473, acc.: 90.62%] [G loss: 3.121932]\n",
      "3647 [D loss: 0.204444, acc.: 89.84%] [G loss: 3.062037]\n",
      "3648 [D loss: 0.206252, acc.: 91.02%] [G loss: 3.243490]\n",
      "3649 [D loss: 0.193920, acc.: 90.23%] [G loss: 3.245373]\n",
      "3650 [D loss: 0.207973, acc.: 92.19%] [G loss: 3.271685]\n",
      "3651 [D loss: 0.215603, acc.: 91.02%] [G loss: 3.428813]\n",
      "3652 [D loss: 0.205184, acc.: 92.19%] [G loss: 3.209617]\n",
      "3653 [D loss: 0.192632, acc.: 92.58%] [G loss: 3.241516]\n",
      "3654 [D loss: 0.180446, acc.: 92.58%] [G loss: 3.343307]\n",
      "3655 [D loss: 0.206529, acc.: 90.23%] [G loss: 3.547685]\n",
      "3656 [D loss: 0.196683, acc.: 92.97%] [G loss: 3.015550]\n",
      "3657 [D loss: 0.178296, acc.: 92.19%] [G loss: 3.275347]\n",
      "3658 [D loss: 0.273974, acc.: 90.62%] [G loss: 3.245000]\n",
      "3659 [D loss: 0.171968, acc.: 91.80%] [G loss: 3.653497]\n",
      "3660 [D loss: 0.187974, acc.: 91.80%] [G loss: 3.289185]\n",
      "3661 [D loss: 0.167326, acc.: 91.80%] [G loss: 3.128494]\n",
      "3662 [D loss: 0.203393, acc.: 91.41%] [G loss: 3.499299]\n",
      "3663 [D loss: 0.246977, acc.: 88.28%] [G loss: 2.972529]\n",
      "3664 [D loss: 0.204839, acc.: 91.80%] [G loss: 3.372374]\n",
      "3665 [D loss: 0.212799, acc.: 91.02%] [G loss: 2.932246]\n",
      "3666 [D loss: 0.204468, acc.: 91.41%] [G loss: 3.085660]\n",
      "3667 [D loss: 0.223904, acc.: 91.02%] [G loss: 3.144267]\n",
      "3668 [D loss: 0.219190, acc.: 90.62%] [G loss: 3.178567]\n",
      "3669 [D loss: 0.232155, acc.: 90.62%] [G loss: 3.199978]\n",
      "3670 [D loss: 0.195623, acc.: 91.41%] [G loss: 3.376361]\n",
      "3671 [D loss: 0.227293, acc.: 90.23%] [G loss: 3.073904]\n",
      "3672 [D loss: 0.223192, acc.: 91.02%] [G loss: 3.149329]\n",
      "3673 [D loss: 0.223407, acc.: 90.23%] [G loss: 3.332270]\n",
      "3674 [D loss: 0.226633, acc.: 90.23%] [G loss: 3.106141]\n",
      "3675 [D loss: 0.227998, acc.: 90.23%] [G loss: 3.375659]\n",
      "3676 [D loss: 0.219595, acc.: 92.19%] [G loss: 3.081964]\n",
      "3677 [D loss: 0.230759, acc.: 91.02%] [G loss: 3.391162]\n",
      "3678 [D loss: 0.239604, acc.: 91.41%] [G loss: 3.084437]\n",
      "3679 [D loss: 0.220019, acc.: 90.62%] [G loss: 3.312937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3680 [D loss: 0.219936, acc.: 91.02%] [G loss: 3.006146]\n",
      "3681 [D loss: 0.190352, acc.: 92.58%] [G loss: 3.353102]\n",
      "3682 [D loss: 0.230852, acc.: 90.23%] [G loss: 3.252399]\n",
      "3683 [D loss: 0.191396, acc.: 91.80%] [G loss: 3.008756]\n",
      "3684 [D loss: 0.208926, acc.: 91.80%] [G loss: 3.258480]\n",
      "3685 [D loss: 0.201963, acc.: 92.19%] [G loss: 3.115954]\n",
      "3686 [D loss: 0.213545, acc.: 89.84%] [G loss: 3.328005]\n",
      "3687 [D loss: 0.193145, acc.: 91.80%] [G loss: 3.386117]\n",
      "3688 [D loss: 0.194222, acc.: 91.80%] [G loss: 2.788061]\n",
      "3689 [D loss: 0.205494, acc.: 91.02%] [G loss: 2.960099]\n",
      "3690 [D loss: 0.229621, acc.: 90.23%] [G loss: 3.100500]\n",
      "3691 [D loss: 0.236416, acc.: 90.62%] [G loss: 3.252327]\n",
      "3692 [D loss: 0.209840, acc.: 91.02%] [G loss: 3.225923]\n",
      "3693 [D loss: 0.208094, acc.: 92.19%] [G loss: 2.903431]\n",
      "3694 [D loss: 0.207125, acc.: 91.41%] [G loss: 2.931159]\n",
      "3695 [D loss: 0.203434, acc.: 91.41%] [G loss: 2.933165]\n",
      "3696 [D loss: 0.217998, acc.: 90.23%] [G loss: 3.022736]\n",
      "3697 [D loss: 0.244779, acc.: 89.45%] [G loss: 3.196045]\n",
      "3698 [D loss: 0.213206, acc.: 91.02%] [G loss: 3.646671]\n",
      "3699 [D loss: 0.215779, acc.: 90.23%] [G loss: 3.018401]\n",
      "3700 [D loss: 0.203568, acc.: 90.62%] [G loss: 3.444713]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "3701 [D loss: 0.229218, acc.: 90.62%] [G loss: 3.086333]\n",
      "3702 [D loss: 0.199170, acc.: 91.41%] [G loss: 3.152551]\n",
      "3703 [D loss: 0.219783, acc.: 91.80%] [G loss: 3.263831]\n",
      "3704 [D loss: 0.201664, acc.: 90.62%] [G loss: 2.950656]\n",
      "3705 [D loss: 0.202307, acc.: 92.19%] [G loss: 3.310354]\n",
      "3706 [D loss: 0.169965, acc.: 91.80%] [G loss: 3.101936]\n",
      "3707 [D loss: 0.206381, acc.: 90.62%] [G loss: 3.420737]\n",
      "3708 [D loss: 0.193554, acc.: 91.02%] [G loss: 3.310505]\n",
      "3709 [D loss: 0.193453, acc.: 91.41%] [G loss: 3.688031]\n",
      "3710 [D loss: 0.198863, acc.: 92.19%] [G loss: 2.951508]\n",
      "3711 [D loss: 0.221668, acc.: 91.41%] [G loss: 3.454552]\n",
      "3712 [D loss: 0.217903, acc.: 91.80%] [G loss: 3.219481]\n",
      "3713 [D loss: 0.225426, acc.: 91.41%] [G loss: 3.325449]\n",
      "3714 [D loss: 0.202306, acc.: 90.62%] [G loss: 3.552094]\n",
      "3715 [D loss: 0.216809, acc.: 90.23%] [G loss: 3.125089]\n",
      "3716 [D loss: 0.211207, acc.: 90.62%] [G loss: 3.223877]\n",
      "3717 [D loss: 0.194562, acc.: 91.02%] [G loss: 3.328414]\n",
      "3718 [D loss: 0.184528, acc.: 92.58%] [G loss: 3.108877]\n",
      "3719 [D loss: 0.201209, acc.: 91.80%] [G loss: 3.187996]\n",
      "3720 [D loss: 0.195974, acc.: 92.19%] [G loss: 3.352663]\n",
      "3721 [D loss: 0.201680, acc.: 90.62%] [G loss: 3.002091]\n",
      "3722 [D loss: 0.221590, acc.: 89.84%] [G loss: 3.003516]\n",
      "3723 [D loss: 0.200906, acc.: 90.62%] [G loss: 3.405257]\n",
      "3724 [D loss: 0.227837, acc.: 90.62%] [G loss: 2.934938]\n",
      "3725 [D loss: 0.193683, acc.: 91.80%] [G loss: 3.119595]\n",
      "3726 [D loss: 0.196147, acc.: 92.58%] [G loss: 3.101598]\n",
      "3727 [D loss: 0.208690, acc.: 91.02%] [G loss: 3.238972]\n",
      "3728 [D loss: 0.220256, acc.: 89.45%] [G loss: 2.830377]\n",
      "3729 [D loss: 0.215468, acc.: 90.23%] [G loss: 3.468063]\n",
      "3730 [D loss: 0.239939, acc.: 90.23%] [G loss: 3.028841]\n",
      "3731 [D loss: 0.222136, acc.: 91.02%] [G loss: 3.190229]\n",
      "3732 [D loss: 0.224225, acc.: 89.84%] [G loss: 3.238932]\n",
      "3733 [D loss: 0.198585, acc.: 91.02%] [G loss: 3.121492]\n",
      "3734 [D loss: 0.197520, acc.: 92.97%] [G loss: 3.059228]\n",
      "3735 [D loss: 0.216391, acc.: 89.45%] [G loss: 3.370925]\n",
      "3736 [D loss: 0.193050, acc.: 91.80%] [G loss: 3.226419]\n",
      "3737 [D loss: 0.206199, acc.: 91.02%] [G loss: 2.966444]\n",
      "3738 [D loss: 0.171853, acc.: 91.80%] [G loss: 3.477925]\n",
      "3739 [D loss: 0.181578, acc.: 93.75%] [G loss: 3.350359]\n",
      "3740 [D loss: 0.202644, acc.: 91.80%] [G loss: 3.346930]\n",
      "3741 [D loss: 0.183990, acc.: 91.80%] [G loss: 3.232097]\n",
      "3742 [D loss: 0.217555, acc.: 90.62%] [G loss: 3.135964]\n",
      "3743 [D loss: 0.176495, acc.: 92.19%] [G loss: 3.125576]\n",
      "3744 [D loss: 0.233189, acc.: 90.23%] [G loss: 3.363604]\n",
      "3745 [D loss: 0.218799, acc.: 90.23%] [G loss: 3.155143]\n",
      "3746 [D loss: 0.199503, acc.: 91.41%] [G loss: 3.241514]\n",
      "3747 [D loss: 0.214197, acc.: 90.62%] [G loss: 2.963910]\n",
      "3748 [D loss: 0.201310, acc.: 92.19%] [G loss: 3.137142]\n",
      "3749 [D loss: 0.200298, acc.: 92.19%] [G loss: 3.309229]\n",
      "3750 [D loss: 0.211275, acc.: 90.62%] [G loss: 3.294751]\n",
      "3751 [D loss: 0.211675, acc.: 91.80%] [G loss: 3.026654]\n",
      "3752 [D loss: 0.205030, acc.: 91.41%] [G loss: 3.122976]\n",
      "3753 [D loss: 0.194796, acc.: 91.80%] [G loss: 3.050176]\n",
      "3754 [D loss: 0.230774, acc.: 90.62%] [G loss: 3.216055]\n",
      "3755 [D loss: 0.193174, acc.: 91.41%] [G loss: 3.106634]\n",
      "3756 [D loss: 0.218060, acc.: 89.84%] [G loss: 3.150515]\n",
      "3757 [D loss: 0.195961, acc.: 90.62%] [G loss: 3.348765]\n",
      "3758 [D loss: 0.216814, acc.: 91.41%] [G loss: 3.388714]\n",
      "3759 [D loss: 0.235867, acc.: 90.62%] [G loss: 3.064966]\n",
      "3760 [D loss: 0.204917, acc.: 92.19%] [G loss: 2.889589]\n",
      "3761 [D loss: 0.187309, acc.: 91.80%] [G loss: 3.081520]\n",
      "3762 [D loss: 0.204935, acc.: 91.41%] [G loss: 3.179198]\n",
      "3763 [D loss: 0.190368, acc.: 91.80%] [G loss: 3.635713]\n",
      "3764 [D loss: 0.212907, acc.: 91.80%] [G loss: 3.200724]\n",
      "3765 [D loss: 0.212815, acc.: 91.80%] [G loss: 3.394722]\n",
      "3766 [D loss: 0.213526, acc.: 91.41%] [G loss: 2.933755]\n",
      "3767 [D loss: 0.210119, acc.: 91.41%] [G loss: 3.004386]\n",
      "3768 [D loss: 0.217087, acc.: 92.19%] [G loss: 3.397723]\n",
      "3769 [D loss: 0.224595, acc.: 91.02%] [G loss: 3.320649]\n",
      "3770 [D loss: 0.191228, acc.: 91.02%] [G loss: 3.146857]\n",
      "3771 [D loss: 0.171107, acc.: 92.19%] [G loss: 3.057357]\n",
      "3772 [D loss: 0.207431, acc.: 91.02%] [G loss: 3.150218]\n",
      "3773 [D loss: 0.201246, acc.: 91.41%] [G loss: 3.162823]\n",
      "3774 [D loss: 0.193081, acc.: 92.58%] [G loss: 3.458681]\n",
      "3775 [D loss: 0.218054, acc.: 92.58%] [G loss: 2.929446]\n",
      "3776 [D loss: 0.212394, acc.: 92.19%] [G loss: 3.026873]\n",
      "3777 [D loss: 0.220718, acc.: 91.02%] [G loss: 3.053450]\n",
      "3778 [D loss: 0.214225, acc.: 91.02%] [G loss: 3.325437]\n",
      "3779 [D loss: 0.198594, acc.: 91.02%] [G loss: 3.199321]\n",
      "3780 [D loss: 0.209568, acc.: 91.41%] [G loss: 3.430726]\n",
      "3781 [D loss: 0.206741, acc.: 91.80%] [G loss: 3.114269]\n",
      "3782 [D loss: 0.202919, acc.: 92.19%] [G loss: 2.987047]\n",
      "3783 [D loss: 0.205063, acc.: 91.80%] [G loss: 2.884724]\n",
      "3784 [D loss: 0.208197, acc.: 91.02%] [G loss: 3.146085]\n",
      "3785 [D loss: 0.208668, acc.: 91.02%] [G loss: 3.127583]\n",
      "3786 [D loss: 0.209655, acc.: 92.19%] [G loss: 3.058686]\n",
      "3787 [D loss: 0.187113, acc.: 91.80%] [G loss: 3.230004]\n",
      "3788 [D loss: 0.209960, acc.: 92.19%] [G loss: 2.817242]\n",
      "3789 [D loss: 0.219979, acc.: 90.23%] [G loss: 3.000993]\n",
      "3790 [D loss: 0.208756, acc.: 92.19%] [G loss: 3.106541]\n",
      "3791 [D loss: 0.208197, acc.: 91.41%] [G loss: 3.214740]\n",
      "3792 [D loss: 0.206149, acc.: 92.58%] [G loss: 2.879419]\n",
      "3793 [D loss: 0.200249, acc.: 92.19%] [G loss: 2.922544]\n",
      "3794 [D loss: 0.213209, acc.: 91.02%] [G loss: 2.831484]\n",
      "3795 [D loss: 0.217982, acc.: 90.62%] [G loss: 2.827211]\n",
      "3796 [D loss: 0.203416, acc.: 91.41%] [G loss: 3.438706]\n",
      "3797 [D loss: 0.206628, acc.: 92.19%] [G loss: 3.128735]\n",
      "3798 [D loss: 0.204745, acc.: 91.41%] [G loss: 3.099082]\n",
      "3799 [D loss: 0.204917, acc.: 92.58%] [G loss: 2.966354]\n",
      "3800 [D loss: 0.204795, acc.: 91.80%] [G loss: 2.912699]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "3801 [D loss: 0.193240, acc.: 92.19%] [G loss: 3.228992]\n",
      "3802 [D loss: 0.218238, acc.: 90.23%] [G loss: 3.255956]\n",
      "3803 [D loss: 0.219389, acc.: 91.02%] [G loss: 3.306942]\n",
      "3804 [D loss: 0.188902, acc.: 92.19%] [G loss: 3.356991]\n",
      "3805 [D loss: 0.183002, acc.: 91.41%] [G loss: 3.144001]\n",
      "3806 [D loss: 0.182652, acc.: 92.19%] [G loss: 3.441675]\n",
      "3807 [D loss: 0.201271, acc.: 91.80%] [G loss: 3.221907]\n",
      "3808 [D loss: 0.219715, acc.: 92.19%] [G loss: 3.114740]\n",
      "3809 [D loss: 0.189407, acc.: 92.58%] [G loss: 3.266072]\n",
      "3810 [D loss: 0.185874, acc.: 91.80%] [G loss: 3.301708]\n",
      "3811 [D loss: 0.196378, acc.: 91.80%] [G loss: 3.190099]\n",
      "3812 [D loss: 0.174419, acc.: 92.97%] [G loss: 3.399345]\n",
      "3813 [D loss: 0.188385, acc.: 91.80%] [G loss: 2.983648]\n",
      "3814 [D loss: 0.188282, acc.: 93.36%] [G loss: 3.357187]\n",
      "3815 [D loss: 0.198178, acc.: 92.19%] [G loss: 3.224296]\n",
      "3816 [D loss: 0.209225, acc.: 90.62%] [G loss: 3.493724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3817 [D loss: 0.215251, acc.: 91.02%] [G loss: 3.342755]\n",
      "3818 [D loss: 0.207803, acc.: 90.23%] [G loss: 3.219933]\n",
      "3819 [D loss: 0.196181, acc.: 92.97%] [G loss: 2.879823]\n",
      "3820 [D loss: 0.176953, acc.: 92.97%] [G loss: 3.324287]\n",
      "3821 [D loss: 0.205820, acc.: 91.02%] [G loss: 2.954540]\n",
      "3822 [D loss: 0.216048, acc.: 90.62%] [G loss: 3.416169]\n",
      "3823 [D loss: 0.210527, acc.: 91.80%] [G loss: 3.174439]\n",
      "3824 [D loss: 0.207949, acc.: 91.41%] [G loss: 3.038632]\n",
      "3825 [D loss: 0.203060, acc.: 92.19%] [G loss: 3.062369]\n",
      "3826 [D loss: 0.209140, acc.: 92.19%] [G loss: 3.117868]\n",
      "3827 [D loss: 0.224597, acc.: 90.23%] [G loss: 3.386531]\n",
      "3828 [D loss: 0.209258, acc.: 91.80%] [G loss: 3.216639]\n",
      "3829 [D loss: 0.199797, acc.: 91.02%] [G loss: 3.240300]\n",
      "3830 [D loss: 0.193531, acc.: 91.02%] [G loss: 2.853677]\n",
      "3831 [D loss: 0.199294, acc.: 91.02%] [G loss: 3.324312]\n",
      "3832 [D loss: 0.202061, acc.: 91.41%] [G loss: 3.078754]\n",
      "3833 [D loss: 0.203886, acc.: 92.19%] [G loss: 3.031008]\n",
      "3834 [D loss: 0.202289, acc.: 91.02%] [G loss: 3.497199]\n",
      "3835 [D loss: 0.185954, acc.: 91.80%] [G loss: 3.677834]\n",
      "3836 [D loss: 0.145922, acc.: 93.75%] [G loss: 3.491198]\n",
      "3837 [D loss: 0.194483, acc.: 93.36%] [G loss: 3.342706]\n",
      "3838 [D loss: 0.197077, acc.: 92.97%] [G loss: 3.095257]\n",
      "3839 [D loss: 0.180259, acc.: 92.97%] [G loss: 3.647382]\n",
      "3840 [D loss: 0.195090, acc.: 91.80%] [G loss: 3.180370]\n",
      "3841 [D loss: 0.180059, acc.: 92.58%] [G loss: 3.957441]\n",
      "3842 [D loss: 0.188250, acc.: 92.58%] [G loss: 3.370774]\n",
      "3843 [D loss: 0.184024, acc.: 92.19%] [G loss: 3.230716]\n",
      "3844 [D loss: 0.198958, acc.: 92.97%] [G loss: 3.297838]\n",
      "3845 [D loss: 0.210115, acc.: 91.41%] [G loss: 3.049856]\n",
      "3846 [D loss: 0.206185, acc.: 91.80%] [G loss: 3.158572]\n",
      "3847 [D loss: 0.233344, acc.: 88.67%] [G loss: 3.409354]\n",
      "3848 [D loss: 0.219657, acc.: 91.02%] [G loss: 3.317725]\n",
      "3849 [D loss: 0.218741, acc.: 89.84%] [G loss: 3.096291]\n",
      "3850 [D loss: 0.214327, acc.: 91.80%] [G loss: 3.347673]\n",
      "3851 [D loss: 0.183542, acc.: 91.41%] [G loss: 3.393609]\n",
      "3852 [D loss: 0.196064, acc.: 91.02%] [G loss: 3.462466]\n",
      "3853 [D loss: 0.185509, acc.: 92.58%] [G loss: 3.322500]\n",
      "3854 [D loss: 0.230513, acc.: 89.84%] [G loss: 2.985611]\n",
      "3855 [D loss: 0.198295, acc.: 91.41%] [G loss: 3.161806]\n",
      "3856 [D loss: 0.224921, acc.: 90.62%] [G loss: 3.199637]\n",
      "3857 [D loss: 0.235739, acc.: 89.45%] [G loss: 3.185826]\n",
      "3858 [D loss: 0.186020, acc.: 91.41%] [G loss: 3.260503]\n",
      "3859 [D loss: 0.217856, acc.: 90.23%] [G loss: 3.078094]\n",
      "3860 [D loss: 0.219690, acc.: 91.41%] [G loss: 2.813993]\n",
      "3861 [D loss: 0.222255, acc.: 90.23%] [G loss: 3.215648]\n",
      "3862 [D loss: 0.206225, acc.: 92.19%] [G loss: 3.915643]\n",
      "3863 [D loss: 0.217888, acc.: 91.80%] [G loss: 3.541827]\n",
      "3864 [D loss: 0.190278, acc.: 91.41%] [G loss: 3.304580]\n",
      "3865 [D loss: 0.197703, acc.: 92.97%] [G loss: 3.194998]\n",
      "3866 [D loss: 0.199623, acc.: 92.58%] [G loss: 2.864586]\n",
      "3867 [D loss: 0.204626, acc.: 90.62%] [G loss: 3.234693]\n",
      "3868 [D loss: 0.207149, acc.: 92.58%] [G loss: 3.289084]\n",
      "3869 [D loss: 0.219166, acc.: 91.80%] [G loss: 2.807866]\n",
      "3870 [D loss: 0.190946, acc.: 91.02%] [G loss: 3.414814]\n",
      "3871 [D loss: 0.190271, acc.: 91.02%] [G loss: 3.550314]\n",
      "3872 [D loss: 0.196154, acc.: 92.19%] [G loss: 2.958134]\n",
      "3873 [D loss: 0.202214, acc.: 91.41%] [G loss: 3.154027]\n",
      "3874 [D loss: 0.195118, acc.: 92.19%] [G loss: 3.115106]\n",
      "3875 [D loss: 0.173107, acc.: 92.97%] [G loss: 3.217031]\n",
      "3876 [D loss: 0.172092, acc.: 91.80%] [G loss: 3.495421]\n",
      "3877 [D loss: 0.174173, acc.: 93.75%] [G loss: 3.395749]\n",
      "3878 [D loss: 0.185226, acc.: 93.36%] [G loss: 3.392077]\n",
      "3879 [D loss: 0.208205, acc.: 91.02%] [G loss: 3.494220]\n",
      "3880 [D loss: 0.206659, acc.: 92.19%] [G loss: 2.955802]\n",
      "3881 [D loss: 0.208267, acc.: 91.02%] [G loss: 3.734320]\n",
      "3882 [D loss: 0.239896, acc.: 90.23%] [G loss: 3.268740]\n",
      "3883 [D loss: 0.200149, acc.: 92.19%] [G loss: 3.267430]\n",
      "3884 [D loss: 0.178192, acc.: 92.19%] [G loss: 2.941530]\n",
      "3885 [D loss: 0.197241, acc.: 91.41%] [G loss: 3.351064]\n",
      "3886 [D loss: 0.201184, acc.: 92.19%] [G loss: 3.349310]\n",
      "3887 [D loss: 0.195015, acc.: 90.23%] [G loss: 3.517832]\n",
      "3888 [D loss: 0.192438, acc.: 92.19%] [G loss: 3.089825]\n",
      "3889 [D loss: 0.195326, acc.: 92.58%] [G loss: 3.152820]\n",
      "3890 [D loss: 0.218658, acc.: 89.06%] [G loss: 3.343663]\n",
      "3891 [D loss: 0.218226, acc.: 91.80%] [G loss: 3.565577]\n",
      "3892 [D loss: 0.201104, acc.: 91.80%] [G loss: 3.293514]\n",
      "3893 [D loss: 0.206220, acc.: 93.36%] [G loss: 3.458923]\n",
      "3894 [D loss: 0.212622, acc.: 91.80%] [G loss: 3.403580]\n",
      "3895 [D loss: 0.221253, acc.: 90.23%] [G loss: 2.964633]\n",
      "3896 [D loss: 0.211466, acc.: 91.80%] [G loss: 3.242107]\n",
      "3897 [D loss: 0.207500, acc.: 90.62%] [G loss: 3.470018]\n",
      "3898 [D loss: 0.210448, acc.: 91.41%] [G loss: 3.123464]\n",
      "3899 [D loss: 0.193924, acc.: 92.58%] [G loss: 3.347448]\n",
      "3900 [D loss: 0.193211, acc.: 92.97%] [G loss: 3.243881]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "3901 [D loss: 0.171922, acc.: 92.97%] [G loss: 3.251542]\n",
      "3902 [D loss: 0.200833, acc.: 92.58%] [G loss: 3.744985]\n",
      "3903 [D loss: 0.205452, acc.: 91.80%] [G loss: 3.188917]\n",
      "3904 [D loss: 0.207753, acc.: 92.58%] [G loss: 3.288460]\n",
      "3905 [D loss: 0.208899, acc.: 91.02%] [G loss: 3.342882]\n",
      "3906 [D loss: 0.197705, acc.: 93.36%] [G loss: 3.559787]\n",
      "3907 [D loss: 0.198976, acc.: 91.80%] [G loss: 3.365247]\n",
      "3908 [D loss: 0.191993, acc.: 91.80%] [G loss: 3.285391]\n",
      "3909 [D loss: 0.179820, acc.: 92.19%] [G loss: 3.181755]\n",
      "3910 [D loss: 0.180001, acc.: 92.19%] [G loss: 3.157709]\n",
      "3911 [D loss: 0.221345, acc.: 90.62%] [G loss: 3.448936]\n",
      "3912 [D loss: 0.235557, acc.: 89.45%] [G loss: 3.082987]\n",
      "3913 [D loss: 0.228239, acc.: 91.02%] [G loss: 3.482777]\n",
      "3914 [D loss: 0.199015, acc.: 91.80%] [G loss: 3.252707]\n",
      "3915 [D loss: 0.220004, acc.: 91.80%] [G loss: 3.274421]\n",
      "3916 [D loss: 0.216245, acc.: 91.80%] [G loss: 3.107189]\n",
      "3917 [D loss: 0.225512, acc.: 89.45%] [G loss: 3.578844]\n",
      "3918 [D loss: 0.209035, acc.: 91.02%] [G loss: 3.422111]\n",
      "3919 [D loss: 0.197765, acc.: 91.41%] [G loss: 3.042410]\n",
      "3920 [D loss: 0.194755, acc.: 91.02%] [G loss: 3.608517]\n",
      "3921 [D loss: 0.179057, acc.: 92.97%] [G loss: 3.092036]\n",
      "3922 [D loss: 0.213841, acc.: 90.23%] [G loss: 3.374114]\n",
      "3923 [D loss: 0.193786, acc.: 90.23%] [G loss: 3.440492]\n",
      "3924 [D loss: 0.213912, acc.: 91.80%] [G loss: 2.946862]\n",
      "3925 [D loss: 0.197684, acc.: 91.41%] [G loss: 3.161161]\n",
      "3926 [D loss: 0.193052, acc.: 91.41%] [G loss: 3.399540]\n",
      "3927 [D loss: 0.191051, acc.: 91.41%] [G loss: 3.160626]\n",
      "3928 [D loss: 0.180891, acc.: 92.19%] [G loss: 3.390902]\n",
      "3929 [D loss: 0.216278, acc.: 90.23%] [G loss: 3.720653]\n",
      "3930 [D loss: 0.189420, acc.: 91.80%] [G loss: 3.046827]\n",
      "3931 [D loss: 0.221796, acc.: 90.62%] [G loss: 3.365096]\n",
      "3932 [D loss: 0.206797, acc.: 91.41%] [G loss: 3.868430]\n",
      "3933 [D loss: 0.191637, acc.: 92.58%] [G loss: 3.078175]\n",
      "3934 [D loss: 0.164671, acc.: 94.14%] [G loss: 3.329466]\n",
      "3935 [D loss: 0.196837, acc.: 92.58%] [G loss: 3.134820]\n",
      "3936 [D loss: 0.191616, acc.: 93.36%] [G loss: 3.202617]\n",
      "3937 [D loss: 0.224496, acc.: 89.45%] [G loss: 3.451452]\n",
      "3938 [D loss: 0.215305, acc.: 91.02%] [G loss: 3.291328]\n",
      "3939 [D loss: 0.176452, acc.: 92.58%] [G loss: 3.193103]\n",
      "3940 [D loss: 0.178168, acc.: 91.80%] [G loss: 3.137707]\n",
      "3941 [D loss: 0.191955, acc.: 92.19%] [G loss: 3.329881]\n",
      "3942 [D loss: 0.187273, acc.: 91.80%] [G loss: 3.276381]\n",
      "3943 [D loss: 0.207908, acc.: 91.41%] [G loss: 3.585584]\n",
      "3944 [D loss: 0.188593, acc.: 93.36%] [G loss: 3.152176]\n",
      "3945 [D loss: 0.213837, acc.: 90.23%] [G loss: 3.208500]\n",
      "3946 [D loss: 0.223298, acc.: 92.19%] [G loss: 3.294726]\n",
      "3947 [D loss: 0.188638, acc.: 92.58%] [G loss: 3.307257]\n",
      "3948 [D loss: 0.213459, acc.: 89.84%] [G loss: 3.384494]\n",
      "3949 [D loss: 0.224752, acc.: 91.02%] [G loss: 3.021889]\n",
      "3950 [D loss: 0.208092, acc.: 91.02%] [G loss: 3.204339]\n",
      "3951 [D loss: 0.209654, acc.: 91.80%] [G loss: 2.690607]\n",
      "3952 [D loss: 0.206924, acc.: 91.80%] [G loss: 2.980509]\n",
      "3953 [D loss: 0.205205, acc.: 91.02%] [G loss: 3.066167]\n",
      "3954 [D loss: 0.187138, acc.: 92.19%] [G loss: 3.307798]\n",
      "3955 [D loss: 0.220518, acc.: 91.02%] [G loss: 3.062661]\n",
      "3956 [D loss: 0.193499, acc.: 93.36%] [G loss: 2.935609]\n",
      "3957 [D loss: 0.211746, acc.: 91.41%] [G loss: 3.618854]\n",
      "3958 [D loss: 0.189681, acc.: 91.80%] [G loss: 3.360894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3959 [D loss: 0.210593, acc.: 90.62%] [G loss: 3.563529]\n",
      "3960 [D loss: 0.188039, acc.: 91.80%] [G loss: 3.407872]\n",
      "3961 [D loss: 0.206520, acc.: 89.45%] [G loss: 2.996272]\n",
      "3962 [D loss: 0.194524, acc.: 91.41%] [G loss: 3.237861]\n",
      "3963 [D loss: 0.196341, acc.: 91.41%] [G loss: 3.061287]\n",
      "3964 [D loss: 0.193302, acc.: 92.19%] [G loss: 3.432836]\n",
      "3965 [D loss: 0.167472, acc.: 91.80%] [G loss: 3.407264]\n",
      "3966 [D loss: 0.183356, acc.: 92.19%] [G loss: 3.530977]\n",
      "3967 [D loss: 0.244058, acc.: 88.67%] [G loss: 3.207156]\n",
      "3968 [D loss: 0.204733, acc.: 91.41%] [G loss: 2.973504]\n",
      "3969 [D loss: 0.189579, acc.: 92.19%] [G loss: 2.942287]\n",
      "3970 [D loss: 0.191566, acc.: 92.58%] [G loss: 3.315535]\n",
      "3971 [D loss: 0.229242, acc.: 90.62%] [G loss: 3.085482]\n",
      "3972 [D loss: 0.193160, acc.: 91.41%] [G loss: 3.503210]\n",
      "3973 [D loss: 0.196864, acc.: 91.80%] [G loss: 3.327869]\n",
      "3974 [D loss: 0.217337, acc.: 92.19%] [G loss: 3.217806]\n",
      "3975 [D loss: 0.201136, acc.: 91.41%] [G loss: 3.470322]\n",
      "3976 [D loss: 0.205866, acc.: 90.62%] [G loss: 3.116861]\n",
      "3977 [D loss: 0.194662, acc.: 91.80%] [G loss: 3.363430]\n",
      "3978 [D loss: 0.225288, acc.: 91.02%] [G loss: 3.034359]\n",
      "3979 [D loss: 0.199624, acc.: 91.41%] [G loss: 3.069739]\n",
      "3980 [D loss: 0.220381, acc.: 90.62%] [G loss: 3.451401]\n",
      "3981 [D loss: 0.197421, acc.: 91.41%] [G loss: 3.266242]\n",
      "3982 [D loss: 0.196755, acc.: 91.80%] [G loss: 3.286148]\n",
      "3983 [D loss: 0.220349, acc.: 91.41%] [G loss: 2.997307]\n",
      "3984 [D loss: 0.212650, acc.: 90.62%] [G loss: 3.166769]\n",
      "3985 [D loss: 0.198277, acc.: 91.80%] [G loss: 3.471800]\n",
      "3986 [D loss: 0.189686, acc.: 92.58%] [G loss: 3.389603]\n",
      "3987 [D loss: 0.175367, acc.: 92.19%] [G loss: 3.421410]\n",
      "3988 [D loss: 0.169362, acc.: 93.36%] [G loss: 3.591792]\n",
      "3989 [D loss: 0.205205, acc.: 92.19%] [G loss: 3.139445]\n",
      "3990 [D loss: 0.192597, acc.: 92.58%] [G loss: 3.149640]\n",
      "3991 [D loss: 0.220756, acc.: 90.62%] [G loss: 3.071225]\n",
      "3992 [D loss: 0.221354, acc.: 91.41%] [G loss: 3.079355]\n",
      "3993 [D loss: 0.227009, acc.: 91.80%] [G loss: 3.152679]\n",
      "3994 [D loss: 0.216436, acc.: 91.02%] [G loss: 3.089124]\n",
      "3995 [D loss: 0.209895, acc.: 91.02%] [G loss: 2.984182]\n",
      "3996 [D loss: 0.191672, acc.: 91.80%] [G loss: 3.134748]\n",
      "3997 [D loss: 0.221021, acc.: 91.80%] [G loss: 3.246585]\n",
      "3998 [D loss: 0.197171, acc.: 92.19%] [G loss: 3.159252]\n",
      "3999 [D loss: 0.190364, acc.: 91.41%] [G loss: 3.212039]\n",
      "4000 [D loss: 0.227690, acc.: 89.84%] [G loss: 3.673932]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "4001 [D loss: 0.211164, acc.: 91.02%] [G loss: 3.165696]\n",
      "4002 [D loss: 0.188353, acc.: 92.19%] [G loss: 3.325263]\n",
      "4003 [D loss: 0.203202, acc.: 92.19%] [G loss: 3.266422]\n",
      "4004 [D loss: 0.175671, acc.: 91.02%] [G loss: 3.507040]\n",
      "4005 [D loss: 0.177094, acc.: 91.80%] [G loss: 3.334214]\n",
      "4006 [D loss: 0.175895, acc.: 91.02%] [G loss: 3.603034]\n",
      "4007 [D loss: 0.191009, acc.: 91.41%] [G loss: 3.750214]\n",
      "4008 [D loss: 0.193646, acc.: 92.58%] [G loss: 3.448500]\n",
      "4009 [D loss: 0.224448, acc.: 89.45%] [G loss: 3.273878]\n",
      "4010 [D loss: 0.197539, acc.: 91.80%] [G loss: 3.531009]\n",
      "4011 [D loss: 0.174562, acc.: 91.80%] [G loss: 3.260402]\n",
      "4012 [D loss: 0.175296, acc.: 92.97%] [G loss: 3.466527]\n",
      "4013 [D loss: 0.178557, acc.: 91.41%] [G loss: 3.656334]\n",
      "4014 [D loss: 0.203107, acc.: 91.02%] [G loss: 3.392038]\n",
      "4015 [D loss: 0.215309, acc.: 91.80%] [G loss: 3.286528]\n",
      "4016 [D loss: 0.200126, acc.: 90.62%] [G loss: 3.715374]\n",
      "4017 [D loss: 0.192203, acc.: 92.19%] [G loss: 3.112230]\n",
      "4018 [D loss: 0.206186, acc.: 91.41%] [G loss: 3.018553]\n",
      "4019 [D loss: 0.216707, acc.: 91.41%] [G loss: 3.361582]\n",
      "4020 [D loss: 0.215739, acc.: 92.19%] [G loss: 3.157100]\n",
      "4021 [D loss: 0.229293, acc.: 90.62%] [G loss: 3.417591]\n",
      "4022 [D loss: 0.227873, acc.: 90.23%] [G loss: 3.175488]\n",
      "4023 [D loss: 0.216357, acc.: 91.41%] [G loss: 3.044257]\n",
      "4024 [D loss: 0.227687, acc.: 90.23%] [G loss: 3.258920]\n",
      "4025 [D loss: 0.189457, acc.: 92.19%] [G loss: 3.169556]\n",
      "4026 [D loss: 0.210277, acc.: 90.62%] [G loss: 3.180415]\n",
      "4027 [D loss: 0.194513, acc.: 91.80%] [G loss: 3.577100]\n",
      "4028 [D loss: 0.213051, acc.: 90.62%] [G loss: 3.195866]\n",
      "4029 [D loss: 0.209984, acc.: 89.84%] [G loss: 2.937922]\n",
      "4030 [D loss: 0.186342, acc.: 91.41%] [G loss: 3.245539]\n",
      "4031 [D loss: 0.222034, acc.: 90.23%] [G loss: 3.000149]\n",
      "4032 [D loss: 0.229345, acc.: 90.23%] [G loss: 3.135686]\n",
      "4033 [D loss: 0.216458, acc.: 91.80%] [G loss: 2.842993]\n",
      "4034 [D loss: 0.211216, acc.: 89.84%] [G loss: 3.170579]\n",
      "4035 [D loss: 0.195487, acc.: 91.02%] [G loss: 3.217732]\n",
      "4036 [D loss: 0.194834, acc.: 91.80%] [G loss: 2.967370]\n",
      "4037 [D loss: 0.189238, acc.: 91.41%] [G loss: 3.803747]\n",
      "4038 [D loss: 0.182303, acc.: 91.02%] [G loss: 3.222064]\n",
      "4039 [D loss: 0.193486, acc.: 92.58%] [G loss: 3.381718]\n",
      "4040 [D loss: 0.200830, acc.: 91.80%] [G loss: 3.171454]\n",
      "4041 [D loss: 0.195465, acc.: 91.41%] [G loss: 3.322478]\n",
      "4042 [D loss: 0.212959, acc.: 90.23%] [G loss: 3.113887]\n",
      "4043 [D loss: 0.221526, acc.: 90.62%] [G loss: 3.521472]\n",
      "4044 [D loss: 0.180819, acc.: 91.80%] [G loss: 3.382438]\n",
      "4045 [D loss: 0.177921, acc.: 92.58%] [G loss: 3.240348]\n",
      "4046 [D loss: 0.183279, acc.: 91.80%] [G loss: 3.234781]\n",
      "4047 [D loss: 0.224380, acc.: 91.41%] [G loss: 3.182885]\n",
      "4048 [D loss: 0.203198, acc.: 91.41%] [G loss: 3.070386]\n",
      "4049 [D loss: 0.191433, acc.: 91.80%] [G loss: 3.137397]\n",
      "4050 [D loss: 0.187713, acc.: 91.41%] [G loss: 3.544549]\n",
      "4051 [D loss: 0.174599, acc.: 91.80%] [G loss: 3.327940]\n",
      "4052 [D loss: 0.184018, acc.: 92.58%] [G loss: 3.385298]\n",
      "4053 [D loss: 0.199885, acc.: 92.19%] [G loss: 3.487046]\n",
      "4054 [D loss: 0.192472, acc.: 91.80%] [G loss: 3.265960]\n",
      "4055 [D loss: 0.189443, acc.: 92.58%] [G loss: 3.329812]\n",
      "4056 [D loss: 0.208680, acc.: 91.80%] [G loss: 3.307124]\n",
      "4057 [D loss: 0.175706, acc.: 92.19%] [G loss: 3.345996]\n",
      "4058 [D loss: 0.179901, acc.: 92.58%] [G loss: 3.474839]\n",
      "4059 [D loss: 0.195621, acc.: 92.97%] [G loss: 3.188383]\n",
      "4060 [D loss: 0.190360, acc.: 92.97%] [G loss: 3.322369]\n",
      "4061 [D loss: 0.230908, acc.: 90.23%] [G loss: 2.889658]\n",
      "4062 [D loss: 0.212136, acc.: 92.19%] [G loss: 3.043333]\n",
      "4063 [D loss: 0.231553, acc.: 91.41%] [G loss: 3.053225]\n",
      "4064 [D loss: 0.207176, acc.: 91.80%] [G loss: 3.568827]\n",
      "4065 [D loss: 0.192885, acc.: 91.41%] [G loss: 3.656781]\n",
      "4066 [D loss: 0.177839, acc.: 92.97%] [G loss: 3.331492]\n",
      "4067 [D loss: 0.203155, acc.: 92.19%] [G loss: 3.728656]\n",
      "4068 [D loss: 0.177613, acc.: 92.19%] [G loss: 3.398454]\n",
      "4069 [D loss: 0.212168, acc.: 91.02%] [G loss: 3.196327]\n",
      "4070 [D loss: 0.172006, acc.: 93.36%] [G loss: 3.310062]\n",
      "4071 [D loss: 0.202276, acc.: 92.19%] [G loss: 3.353336]\n",
      "4072 [D loss: 0.205068, acc.: 91.41%] [G loss: 3.146895]\n",
      "4073 [D loss: 0.226904, acc.: 88.28%] [G loss: 3.452916]\n",
      "4074 [D loss: 0.234093, acc.: 90.62%] [G loss: 3.021510]\n",
      "4075 [D loss: 0.212032, acc.: 91.80%] [G loss: 3.195599]\n",
      "4076 [D loss: 0.202141, acc.: 91.80%] [G loss: 3.315082]\n",
      "4077 [D loss: 0.186410, acc.: 91.41%] [G loss: 3.658676]\n",
      "4078 [D loss: 0.216344, acc.: 90.62%] [G loss: 3.041620]\n",
      "4079 [D loss: 0.166184, acc.: 92.58%] [G loss: 3.864947]\n",
      "4080 [D loss: 0.204781, acc.: 91.02%] [G loss: 3.231479]\n",
      "4081 [D loss: 0.190599, acc.: 91.41%] [G loss: 3.280020]\n",
      "4082 [D loss: 0.220569, acc.: 89.84%] [G loss: 3.586740]\n",
      "4083 [D loss: 0.218307, acc.: 91.02%] [G loss: 3.473011]\n",
      "4084 [D loss: 0.225110, acc.: 89.45%] [G loss: 3.333325]\n",
      "4085 [D loss: 0.182355, acc.: 91.02%] [G loss: 3.459628]\n",
      "4086 [D loss: 0.230553, acc.: 89.45%] [G loss: 3.193435]\n",
      "4087 [D loss: 0.202918, acc.: 91.41%] [G loss: 3.546216]\n",
      "4088 [D loss: 0.231555, acc.: 90.62%] [G loss: 3.309751]\n",
      "4089 [D loss: 0.212991, acc.: 91.02%] [G loss: 3.311494]\n",
      "4090 [D loss: 0.238002, acc.: 89.84%] [G loss: 3.134167]\n",
      "4091 [D loss: 0.217714, acc.: 91.02%] [G loss: 3.220709]\n",
      "4092 [D loss: 0.229542, acc.: 89.84%] [G loss: 3.413876]\n",
      "4093 [D loss: 0.213663, acc.: 91.02%] [G loss: 3.310157]\n",
      "4094 [D loss: 0.220206, acc.: 91.80%] [G loss: 2.939302]\n",
      "4095 [D loss: 0.203086, acc.: 91.41%] [G loss: 3.060686]\n",
      "4096 [D loss: 0.189307, acc.: 91.41%] [G loss: 3.062145]\n",
      "4097 [D loss: 0.210639, acc.: 91.41%] [G loss: 3.261997]\n",
      "4098 [D loss: 0.217248, acc.: 89.45%] [G loss: 3.176478]\n",
      "4099 [D loss: 0.198098, acc.: 91.80%] [G loss: 3.450054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4100 [D loss: 0.207106, acc.: 90.23%] [G loss: 3.233778]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "4101 [D loss: 0.163930, acc.: 93.36%] [G loss: 2.976538]\n",
      "4102 [D loss: 0.193222, acc.: 91.02%] [G loss: 3.167498]\n",
      "4103 [D loss: 0.207270, acc.: 90.62%] [G loss: 3.323003]\n",
      "4104 [D loss: 0.225057, acc.: 91.02%] [G loss: 2.844201]\n",
      "4105 [D loss: 0.231929, acc.: 89.84%] [G loss: 3.053746]\n",
      "4106 [D loss: 0.227295, acc.: 91.02%] [G loss: 3.307792]\n",
      "4107 [D loss: 0.237637, acc.: 90.62%] [G loss: 3.185606]\n",
      "4108 [D loss: 0.210793, acc.: 91.80%] [G loss: 3.574331]\n",
      "4109 [D loss: 0.183435, acc.: 92.19%] [G loss: 3.311329]\n",
      "4110 [D loss: 0.218536, acc.: 90.62%] [G loss: 3.187293]\n",
      "4111 [D loss: 0.196435, acc.: 91.41%] [G loss: 3.380156]\n",
      "4112 [D loss: 0.219531, acc.: 91.02%] [G loss: 3.063465]\n",
      "4113 [D loss: 0.222322, acc.: 91.02%] [G loss: 3.215257]\n",
      "4114 [D loss: 0.214427, acc.: 90.23%] [G loss: 3.444523]\n",
      "4115 [D loss: 0.159753, acc.: 92.58%] [G loss: 3.574274]\n",
      "4116 [D loss: 0.158329, acc.: 91.80%] [G loss: 3.401666]\n",
      "4117 [D loss: 0.221041, acc.: 91.80%] [G loss: 3.195606]\n",
      "4118 [D loss: 0.188001, acc.: 91.41%] [G loss: 3.619665]\n",
      "4119 [D loss: 0.237358, acc.: 89.84%] [G loss: 3.476169]\n",
      "4120 [D loss: 0.210136, acc.: 90.62%] [G loss: 3.216131]\n",
      "4121 [D loss: 0.193011, acc.: 92.19%] [G loss: 3.703337]\n",
      "4122 [D loss: 0.183890, acc.: 92.97%] [G loss: 3.342529]\n",
      "4123 [D loss: 0.157941, acc.: 93.36%] [G loss: 3.515522]\n",
      "4124 [D loss: 0.189539, acc.: 92.97%] [G loss: 3.171030]\n",
      "4125 [D loss: 0.194493, acc.: 91.41%] [G loss: 3.250229]\n",
      "4126 [D loss: 0.201328, acc.: 91.41%] [G loss: 3.339026]\n",
      "4127 [D loss: 0.223109, acc.: 91.02%] [G loss: 2.965389]\n",
      "4128 [D loss: 0.186650, acc.: 91.80%] [G loss: 3.286791]\n",
      "4129 [D loss: 0.197100, acc.: 92.19%] [G loss: 3.247166]\n",
      "4130 [D loss: 0.191939, acc.: 91.41%] [G loss: 3.222865]\n",
      "4131 [D loss: 0.176930, acc.: 91.80%] [G loss: 3.548793]\n",
      "4132 [D loss: 0.197126, acc.: 91.41%] [G loss: 3.558053]\n",
      "4133 [D loss: 0.185831, acc.: 92.19%] [G loss: 3.362991]\n",
      "4134 [D loss: 0.212968, acc.: 91.80%] [G loss: 3.629802]\n",
      "4135 [D loss: 0.214091, acc.: 91.80%] [G loss: 3.298237]\n",
      "4136 [D loss: 0.185338, acc.: 92.58%] [G loss: 3.070419]\n",
      "4137 [D loss: 0.194142, acc.: 91.80%] [G loss: 3.310141]\n",
      "4138 [D loss: 0.187405, acc.: 91.02%] [G loss: 3.285635]\n",
      "4139 [D loss: 0.207004, acc.: 91.41%] [G loss: 3.272501]\n",
      "4140 [D loss: 0.201567, acc.: 91.02%] [G loss: 3.431215]\n",
      "4141 [D loss: 0.201281, acc.: 91.80%] [G loss: 3.546406]\n",
      "4142 [D loss: 0.203109, acc.: 91.41%] [G loss: 3.447616]\n",
      "4143 [D loss: 0.213579, acc.: 91.41%] [G loss: 3.135616]\n",
      "4144 [D loss: 0.203924, acc.: 90.23%] [G loss: 3.442430]\n",
      "4145 [D loss: 0.180677, acc.: 91.80%] [G loss: 3.794273]\n",
      "4146 [D loss: 0.201265, acc.: 91.41%] [G loss: 3.328409]\n",
      "4147 [D loss: 0.210452, acc.: 91.80%] [G loss: 3.231343]\n",
      "4148 [D loss: 0.213991, acc.: 91.41%] [G loss: 3.414911]\n",
      "4149 [D loss: 0.190892, acc.: 91.80%] [G loss: 3.298395]\n",
      "4150 [D loss: 0.169385, acc.: 92.19%] [G loss: 3.539968]\n",
      "4151 [D loss: 0.211874, acc.: 91.41%] [G loss: 3.544011]\n",
      "4152 [D loss: 0.219407, acc.: 92.19%] [G loss: 3.130115]\n",
      "4153 [D loss: 0.195699, acc.: 92.58%] [G loss: 3.013075]\n",
      "4154 [D loss: 0.181464, acc.: 93.36%] [G loss: 2.745724]\n",
      "4155 [D loss: 0.176594, acc.: 92.97%] [G loss: 3.082215]\n",
      "4156 [D loss: 0.210272, acc.: 90.23%] [G loss: 3.235640]\n",
      "4157 [D loss: 0.202904, acc.: 90.62%] [G loss: 3.218638]\n",
      "4158 [D loss: 0.210427, acc.: 91.41%] [G loss: 3.310043]\n",
      "4159 [D loss: 0.203227, acc.: 92.19%] [G loss: 2.994868]\n",
      "4160 [D loss: 0.220733, acc.: 89.84%] [G loss: 3.212697]\n",
      "4161 [D loss: 0.191672, acc.: 91.80%] [G loss: 3.560153]\n",
      "4162 [D loss: 0.165262, acc.: 92.58%] [G loss: 3.621159]\n",
      "4163 [D loss: 0.174534, acc.: 92.97%] [G loss: 3.534474]\n",
      "4164 [D loss: 0.203110, acc.: 91.80%] [G loss: 3.420684]\n",
      "4165 [D loss: 0.211406, acc.: 91.02%] [G loss: 3.380545]\n",
      "4166 [D loss: 0.220592, acc.: 91.02%] [G loss: 3.591498]\n",
      "4167 [D loss: 0.195024, acc.: 92.19%] [G loss: 3.227819]\n",
      "4168 [D loss: 0.202512, acc.: 91.02%] [G loss: 3.455395]\n",
      "4169 [D loss: 0.224413, acc.: 91.80%] [G loss: 3.078578]\n",
      "4170 [D loss: 0.200400, acc.: 90.62%] [G loss: 3.419241]\n",
      "4171 [D loss: 0.221593, acc.: 91.02%] [G loss: 3.412502]\n",
      "4172 [D loss: 0.192622, acc.: 92.19%] [G loss: 3.483538]\n",
      "4173 [D loss: 0.194920, acc.: 91.41%] [G loss: 3.280624]\n",
      "4174 [D loss: 0.233269, acc.: 90.62%] [G loss: 3.390517]\n",
      "4175 [D loss: 0.196162, acc.: 91.02%] [G loss: 3.311386]\n",
      "4176 [D loss: 0.216726, acc.: 89.84%] [G loss: 3.162942]\n",
      "4177 [D loss: 0.182427, acc.: 92.19%] [G loss: 2.920412]\n",
      "4178 [D loss: 0.195266, acc.: 92.97%] [G loss: 3.403314]\n",
      "4179 [D loss: 0.187686, acc.: 92.97%] [G loss: 3.398102]\n",
      "4180 [D loss: 0.187526, acc.: 91.41%] [G loss: 3.343675]\n",
      "4181 [D loss: 0.153462, acc.: 92.97%] [G loss: 3.399080]\n",
      "4182 [D loss: 0.188031, acc.: 93.36%] [G loss: 2.959100]\n",
      "4183 [D loss: 0.209873, acc.: 91.41%] [G loss: 3.224967]\n",
      "4184 [D loss: 0.179321, acc.: 93.36%] [G loss: 3.380038]\n",
      "4185 [D loss: 0.213580, acc.: 91.41%] [G loss: 3.303509]\n",
      "4186 [D loss: 0.202002, acc.: 92.97%] [G loss: 3.157338]\n",
      "4187 [D loss: 0.205820, acc.: 91.02%] [G loss: 3.081179]\n",
      "4188 [D loss: 0.194449, acc.: 91.80%] [G loss: 3.029392]\n",
      "4189 [D loss: 0.185416, acc.: 92.58%] [G loss: 3.283775]\n",
      "4190 [D loss: 0.183843, acc.: 91.80%] [G loss: 3.172168]\n",
      "4191 [D loss: 0.206921, acc.: 92.58%] [G loss: 3.050423]\n",
      "4192 [D loss: 0.195509, acc.: 90.23%] [G loss: 3.260550]\n",
      "4193 [D loss: 0.220047, acc.: 89.45%] [G loss: 3.296229]\n",
      "4194 [D loss: 0.200403, acc.: 92.19%] [G loss: 3.303463]\n",
      "4195 [D loss: 0.228650, acc.: 91.41%] [G loss: 3.158134]\n",
      "4196 [D loss: 0.203624, acc.: 91.80%] [G loss: 3.391655]\n",
      "4197 [D loss: 0.210696, acc.: 91.02%] [G loss: 3.263943]\n",
      "4198 [D loss: 0.180470, acc.: 92.19%] [G loss: 3.333691]\n",
      "4199 [D loss: 0.217791, acc.: 92.19%] [G loss: 3.186119]\n",
      "4200 [D loss: 0.202374, acc.: 91.80%] [G loss: 3.151557]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "4201 [D loss: 0.227382, acc.: 89.84%] [G loss: 3.360812]\n",
      "4202 [D loss: 0.204941, acc.: 91.41%] [G loss: 3.433834]\n",
      "4203 [D loss: 0.210887, acc.: 89.84%] [G loss: 3.222021]\n",
      "4204 [D loss: 0.183481, acc.: 91.80%] [G loss: 3.430759]\n",
      "4205 [D loss: 0.232632, acc.: 90.62%] [G loss: 3.240425]\n",
      "4206 [D loss: 0.197840, acc.: 91.41%] [G loss: 3.186334]\n",
      "4207 [D loss: 0.237629, acc.: 90.62%] [G loss: 2.888151]\n",
      "4208 [D loss: 0.224868, acc.: 91.02%] [G loss: 3.143533]\n",
      "4209 [D loss: 0.214788, acc.: 91.02%] [G loss: 3.418760]\n",
      "4210 [D loss: 0.205974, acc.: 90.62%] [G loss: 3.034126]\n",
      "4211 [D loss: 0.211909, acc.: 91.41%] [G loss: 2.975436]\n",
      "4212 [D loss: 0.233465, acc.: 90.62%] [G loss: 3.089513]\n",
      "4213 [D loss: 0.247196, acc.: 89.45%] [G loss: 3.031520]\n",
      "4214 [D loss: 0.232558, acc.: 92.19%] [G loss: 2.839869]\n",
      "4215 [D loss: 0.197547, acc.: 91.02%] [G loss: 3.139206]\n",
      "4216 [D loss: 0.216988, acc.: 90.23%] [G loss: 3.297601]\n",
      "4217 [D loss: 0.207948, acc.: 91.02%] [G loss: 3.394688]\n",
      "4218 [D loss: 0.201150, acc.: 91.41%] [G loss: 3.117867]\n",
      "4219 [D loss: 0.194763, acc.: 91.80%] [G loss: 3.044593]\n",
      "4220 [D loss: 0.227853, acc.: 91.80%] [G loss: 3.175243]\n",
      "4221 [D loss: 0.204544, acc.: 92.19%] [G loss: 3.024026]\n",
      "4222 [D loss: 0.227782, acc.: 91.41%] [G loss: 2.933503]\n",
      "4223 [D loss: 0.208179, acc.: 91.80%] [G loss: 3.182717]\n",
      "4224 [D loss: 0.209810, acc.: 92.19%] [G loss: 2.832669]\n",
      "4225 [D loss: 0.193573, acc.: 92.58%] [G loss: 3.096849]\n",
      "4226 [D loss: 0.180553, acc.: 92.19%] [G loss: 3.503008]\n",
      "4227 [D loss: 0.194069, acc.: 92.19%] [G loss: 3.542279]\n",
      "4228 [D loss: 0.187880, acc.: 91.80%] [G loss: 3.107203]\n",
      "4229 [D loss: 0.202398, acc.: 91.02%] [G loss: 3.120219]\n",
      "4230 [D loss: 0.219341, acc.: 91.41%] [G loss: 3.223208]\n",
      "4231 [D loss: 0.233486, acc.: 90.62%] [G loss: 3.204735]\n",
      "4232 [D loss: 0.207130, acc.: 91.02%] [G loss: 3.376328]\n",
      "4233 [D loss: 0.198251, acc.: 91.80%] [G loss: 3.212934]\n",
      "4234 [D loss: 0.172440, acc.: 92.97%] [G loss: 3.139400]\n",
      "4235 [D loss: 0.206366, acc.: 91.41%] [G loss: 2.963670]\n",
      "4236 [D loss: 0.229410, acc.: 90.62%] [G loss: 3.232885]\n",
      "4237 [D loss: 0.239832, acc.: 90.23%] [G loss: 3.144760]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4238 [D loss: 0.227663, acc.: 91.41%] [G loss: 2.850169]\n",
      "4239 [D loss: 0.235195, acc.: 90.62%] [G loss: 2.880065]\n",
      "4240 [D loss: 0.219633, acc.: 90.62%] [G loss: 3.815798]\n",
      "4241 [D loss: 0.217155, acc.: 91.02%] [G loss: 3.273314]\n",
      "4242 [D loss: 0.185738, acc.: 91.80%] [G loss: 3.370999]\n",
      "4243 [D loss: 0.209892, acc.: 91.80%] [G loss: 2.853312]\n",
      "4244 [D loss: 0.209629, acc.: 92.58%] [G loss: 3.083107]\n",
      "4245 [D loss: 0.231747, acc.: 91.02%] [G loss: 3.363788]\n",
      "4246 [D loss: 0.205214, acc.: 91.80%] [G loss: 3.224707]\n",
      "4247 [D loss: 0.211093, acc.: 91.41%] [G loss: 3.029130]\n",
      "4248 [D loss: 0.206822, acc.: 91.02%] [G loss: 3.153688]\n",
      "4249 [D loss: 0.186569, acc.: 92.58%] [G loss: 3.351530]\n",
      "4250 [D loss: 0.183390, acc.: 91.80%] [G loss: 3.254759]\n",
      "4251 [D loss: 0.163169, acc.: 92.58%] [G loss: 3.277818]\n",
      "4252 [D loss: 0.172433, acc.: 92.19%] [G loss: 3.409756]\n",
      "4253 [D loss: 0.209335, acc.: 91.02%] [G loss: 3.137736]\n",
      "4254 [D loss: 0.207781, acc.: 92.97%] [G loss: 3.621042]\n",
      "4255 [D loss: 0.214434, acc.: 90.62%] [G loss: 3.196578]\n",
      "4256 [D loss: 0.193016, acc.: 91.02%] [G loss: 3.196498]\n",
      "4257 [D loss: 0.192748, acc.: 91.80%] [G loss: 3.241938]\n",
      "4258 [D loss: 0.204403, acc.: 92.58%] [G loss: 3.241992]\n",
      "4259 [D loss: 0.192834, acc.: 92.97%] [G loss: 3.178171]\n",
      "4260 [D loss: 0.212557, acc.: 92.19%] [G loss: 2.954151]\n",
      "4261 [D loss: 0.184462, acc.: 92.97%] [G loss: 3.371835]\n",
      "4262 [D loss: 0.207229, acc.: 91.02%] [G loss: 3.112875]\n",
      "4263 [D loss: 0.196195, acc.: 92.19%] [G loss: 3.149621]\n",
      "4264 [D loss: 0.226520, acc.: 90.62%] [G loss: 3.103947]\n",
      "4265 [D loss: 0.227582, acc.: 91.80%] [G loss: 3.246047]\n",
      "4266 [D loss: 0.234683, acc.: 91.41%] [G loss: 3.126704]\n",
      "4267 [D loss: 0.228104, acc.: 90.62%] [G loss: 3.217849]\n",
      "4268 [D loss: 0.205448, acc.: 91.41%] [G loss: 3.097849]\n",
      "4269 [D loss: 0.172462, acc.: 93.36%] [G loss: 3.196473]\n",
      "4270 [D loss: 0.224890, acc.: 92.19%] [G loss: 2.910534]\n",
      "4271 [D loss: 0.220502, acc.: 91.80%] [G loss: 2.963448]\n",
      "4272 [D loss: 0.213378, acc.: 90.23%] [G loss: 3.625508]\n",
      "4273 [D loss: 0.222359, acc.: 91.02%] [G loss: 2.927505]\n",
      "4274 [D loss: 0.167849, acc.: 92.19%] [G loss: 3.365607]\n",
      "4275 [D loss: 0.191561, acc.: 91.41%] [G loss: 2.925380]\n",
      "4276 [D loss: 0.199613, acc.: 91.41%] [G loss: 3.516005]\n",
      "4277 [D loss: 0.215592, acc.: 90.23%] [G loss: 3.825616]\n",
      "4278 [D loss: 0.200177, acc.: 91.80%] [G loss: 3.213397]\n",
      "4279 [D loss: 0.180830, acc.: 92.19%] [G loss: 2.982636]\n",
      "4280 [D loss: 0.194431, acc.: 91.80%] [G loss: 3.138254]\n",
      "4281 [D loss: 0.209817, acc.: 92.97%] [G loss: 2.934085]\n",
      "4282 [D loss: 0.191409, acc.: 91.80%] [G loss: 3.347132]\n",
      "4283 [D loss: 0.189939, acc.: 92.19%] [G loss: 3.084594]\n",
      "4284 [D loss: 0.218867, acc.: 90.62%] [G loss: 3.260476]\n",
      "4285 [D loss: 0.170214, acc.: 92.97%] [G loss: 3.198164]\n",
      "4286 [D loss: 0.202673, acc.: 92.19%] [G loss: 2.996527]\n",
      "4287 [D loss: 0.234963, acc.: 89.45%] [G loss: 3.733051]\n",
      "4288 [D loss: 0.180909, acc.: 91.41%] [G loss: 3.482943]\n",
      "4289 [D loss: 0.202934, acc.: 91.80%] [G loss: 3.192181]\n",
      "4290 [D loss: 0.168097, acc.: 92.19%] [G loss: 3.721151]\n",
      "4291 [D loss: 0.211046, acc.: 92.19%] [G loss: 2.977373]\n",
      "4292 [D loss: 0.203019, acc.: 91.80%] [G loss: 3.204737]\n",
      "4293 [D loss: 0.234153, acc.: 90.23%] [G loss: 3.133511]\n",
      "4294 [D loss: 0.180000, acc.: 91.80%] [G loss: 3.212060]\n",
      "4295 [D loss: 0.198184, acc.: 92.19%] [G loss: 2.914558]\n",
      "4296 [D loss: 0.199951, acc.: 92.19%] [G loss: 3.352031]\n",
      "4297 [D loss: 0.187077, acc.: 92.19%] [G loss: 3.398704]\n",
      "4298 [D loss: 0.205038, acc.: 91.80%] [G loss: 3.418646]\n",
      "4299 [D loss: 0.203847, acc.: 91.02%] [G loss: 3.803721]\n",
      "4300 [D loss: 0.207669, acc.: 91.80%] [G loss: 2.858438]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "4301 [D loss: 0.205393, acc.: 91.02%] [G loss: 3.400567]\n",
      "4302 [D loss: 0.201422, acc.: 91.02%] [G loss: 3.454540]\n",
      "4303 [D loss: 0.218275, acc.: 92.19%] [G loss: 2.801242]\n",
      "4304 [D loss: 0.189995, acc.: 92.97%] [G loss: 2.926296]\n",
      "4305 [D loss: 0.229577, acc.: 91.80%] [G loss: 3.008095]\n",
      "4306 [D loss: 0.230906, acc.: 93.36%] [G loss: 3.147980]\n",
      "4307 [D loss: 0.221854, acc.: 91.02%] [G loss: 3.419905]\n",
      "4308 [D loss: 0.215357, acc.: 92.19%] [G loss: 3.296472]\n",
      "4309 [D loss: 0.230407, acc.: 90.23%] [G loss: 3.391572]\n",
      "4310 [D loss: 0.217098, acc.: 91.02%] [G loss: 3.269596]\n",
      "4311 [D loss: 0.243221, acc.: 87.89%] [G loss: 3.236220]\n",
      "4312 [D loss: 0.234084, acc.: 91.02%] [G loss: 3.119306]\n",
      "4313 [D loss: 0.202554, acc.: 91.80%] [G loss: 3.293756]\n",
      "4314 [D loss: 0.206336, acc.: 91.41%] [G loss: 2.734680]\n",
      "4315 [D loss: 0.172127, acc.: 92.19%] [G loss: 3.346894]\n",
      "4316 [D loss: 0.182474, acc.: 92.97%] [G loss: 3.147969]\n",
      "4317 [D loss: 0.151609, acc.: 92.97%] [G loss: 3.338978]\n",
      "4318 [D loss: 0.198874, acc.: 91.80%] [G loss: 2.961383]\n",
      "4319 [D loss: 0.193513, acc.: 92.19%] [G loss: 2.793575]\n",
      "4320 [D loss: 0.199030, acc.: 90.62%] [G loss: 2.934561]\n",
      "4321 [D loss: 0.206116, acc.: 91.41%] [G loss: 3.372490]\n",
      "4322 [D loss: 0.184028, acc.: 91.02%] [G loss: 3.655924]\n",
      "4323 [D loss: 0.201373, acc.: 91.80%] [G loss: 3.393302]\n",
      "4324 [D loss: 0.208520, acc.: 89.84%] [G loss: 2.925323]\n",
      "4325 [D loss: 0.232488, acc.: 87.89%] [G loss: 3.136927]\n",
      "4326 [D loss: 0.219369, acc.: 91.41%] [G loss: 3.183438]\n",
      "4327 [D loss: 0.219329, acc.: 92.58%] [G loss: 3.251117]\n",
      "4328 [D loss: 0.201731, acc.: 92.19%] [G loss: 3.148775]\n",
      "4329 [D loss: 0.232479, acc.: 91.02%] [G loss: 3.380861]\n",
      "4330 [D loss: 0.206094, acc.: 92.19%] [G loss: 2.958231]\n",
      "4331 [D loss: 0.224579, acc.: 90.62%] [G loss: 3.101551]\n",
      "4332 [D loss: 0.187703, acc.: 92.19%] [G loss: 3.184152]\n",
      "4333 [D loss: 0.202115, acc.: 90.23%] [G loss: 3.521007]\n",
      "4334 [D loss: 0.181649, acc.: 92.58%] [G loss: 3.225806]\n",
      "4335 [D loss: 0.188699, acc.: 92.19%] [G loss: 3.356819]\n",
      "4336 [D loss: 0.152677, acc.: 93.36%] [G loss: 3.737898]\n",
      "4337 [D loss: 0.179328, acc.: 93.75%] [G loss: 3.287966]\n",
      "4338 [D loss: 0.186482, acc.: 92.19%] [G loss: 3.355910]\n",
      "4339 [D loss: 0.186280, acc.: 92.19%] [G loss: 3.127038]\n",
      "4340 [D loss: 0.220662, acc.: 89.45%] [G loss: 3.089720]\n",
      "4341 [D loss: 0.173297, acc.: 91.80%] [G loss: 3.758452]\n",
      "4342 [D loss: 0.194326, acc.: 92.19%] [G loss: 3.107201]\n",
      "4343 [D loss: 0.190926, acc.: 91.80%] [G loss: 3.241654]\n",
      "4344 [D loss: 0.204777, acc.: 91.80%] [G loss: 3.563298]\n",
      "4345 [D loss: 0.207434, acc.: 92.97%] [G loss: 3.466094]\n",
      "4346 [D loss: 0.221137, acc.: 90.62%] [G loss: 3.326143]\n",
      "4347 [D loss: 0.212237, acc.: 91.41%] [G loss: 3.020135]\n",
      "4348 [D loss: 0.198781, acc.: 93.75%] [G loss: 3.158929]\n",
      "4349 [D loss: 0.192330, acc.: 91.80%] [G loss: 3.720095]\n",
      "4350 [D loss: 0.206214, acc.: 92.58%] [G loss: 2.858632]\n",
      "4351 [D loss: 0.197195, acc.: 92.19%] [G loss: 3.282067]\n",
      "4352 [D loss: 0.198397, acc.: 93.75%] [G loss: 3.110519]\n",
      "4353 [D loss: 0.181950, acc.: 93.36%] [G loss: 3.377426]\n",
      "4354 [D loss: 0.181400, acc.: 92.97%] [G loss: 3.645319]\n",
      "4355 [D loss: 0.218310, acc.: 92.19%] [G loss: 3.369355]\n",
      "4356 [D loss: 0.202999, acc.: 92.58%] [G loss: 3.692289]\n",
      "4357 [D loss: 0.180442, acc.: 92.19%] [G loss: 3.324935]\n",
      "4358 [D loss: 0.158363, acc.: 91.80%] [G loss: 3.420752]\n",
      "4359 [D loss: 0.216367, acc.: 91.80%] [G loss: 3.174405]\n",
      "4360 [D loss: 0.189051, acc.: 92.97%] [G loss: 3.106106]\n",
      "4361 [D loss: 0.190253, acc.: 91.80%] [G loss: 3.305644]\n",
      "4362 [D loss: 0.213324, acc.: 91.02%] [G loss: 3.037280]\n",
      "4363 [D loss: 0.186940, acc.: 92.19%] [G loss: 3.472012]\n",
      "4364 [D loss: 0.199952, acc.: 91.02%] [G loss: 3.411757]\n",
      "4365 [D loss: 0.175545, acc.: 91.41%] [G loss: 3.473833]\n",
      "4366 [D loss: 0.222929, acc.: 91.41%] [G loss: 3.355276]\n",
      "4367 [D loss: 0.238489, acc.: 90.62%] [G loss: 3.277068]\n",
      "4368 [D loss: 0.225048, acc.: 91.80%] [G loss: 3.380600]\n",
      "4369 [D loss: 0.222670, acc.: 91.80%] [G loss: 3.069660]\n",
      "4370 [D loss: 0.206099, acc.: 91.02%] [G loss: 3.496416]\n",
      "4371 [D loss: 0.191552, acc.: 92.19%] [G loss: 3.342592]\n",
      "4372 [D loss: 0.198730, acc.: 92.19%] [G loss: 2.972434]\n",
      "4373 [D loss: 0.202761, acc.: 90.62%] [G loss: 3.372567]\n",
      "4374 [D loss: 0.209984, acc.: 91.80%] [G loss: 3.304201]\n",
      "4375 [D loss: 0.200836, acc.: 91.41%] [G loss: 2.908984]\n",
      "4376 [D loss: 0.195247, acc.: 91.80%] [G loss: 3.454861]\n",
      "4377 [D loss: 0.202457, acc.: 92.19%] [G loss: 3.498776]\n",
      "4378 [D loss: 0.187824, acc.: 92.58%] [G loss: 3.433723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4379 [D loss: 0.184396, acc.: 92.19%] [G loss: 3.460671]\n",
      "4380 [D loss: 0.167597, acc.: 92.58%] [G loss: 3.585206]\n",
      "4381 [D loss: 0.165489, acc.: 92.97%] [G loss: 3.081629]\n",
      "4382 [D loss: 0.183901, acc.: 92.58%] [G loss: 2.746973]\n",
      "4383 [D loss: 0.169439, acc.: 92.97%] [G loss: 3.564846]\n",
      "4384 [D loss: 0.192904, acc.: 92.19%] [G loss: 3.213401]\n",
      "4385 [D loss: 0.169826, acc.: 92.58%] [G loss: 3.549271]\n",
      "4386 [D loss: 0.147130, acc.: 92.58%] [G loss: 3.921195]\n",
      "4387 [D loss: 0.180340, acc.: 92.58%] [G loss: 3.162116]\n",
      "4388 [D loss: 0.193919, acc.: 92.58%] [G loss: 3.130574]\n",
      "4389 [D loss: 0.184295, acc.: 91.80%] [G loss: 3.728048]\n",
      "4390 [D loss: 0.208981, acc.: 92.58%] [G loss: 3.027075]\n",
      "4391 [D loss: 0.224093, acc.: 90.23%] [G loss: 3.421118]\n",
      "4392 [D loss: 0.233154, acc.: 91.02%] [G loss: 3.083958]\n",
      "4393 [D loss: 0.220284, acc.: 91.80%] [G loss: 3.234635]\n",
      "4394 [D loss: 0.234213, acc.: 90.62%] [G loss: 2.828939]\n",
      "4395 [D loss: 0.187301, acc.: 92.58%] [G loss: 2.952398]\n",
      "4396 [D loss: 0.189683, acc.: 92.19%] [G loss: 3.352863]\n",
      "4397 [D loss: 0.223173, acc.: 91.02%] [G loss: 3.065275]\n",
      "4398 [D loss: 0.245119, acc.: 90.62%] [G loss: 3.094986]\n",
      "4399 [D loss: 0.222274, acc.: 91.41%] [G loss: 3.119220]\n",
      "4400 [D loss: 0.211510, acc.: 92.19%] [G loss: 2.980957]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "4401 [D loss: 0.184134, acc.: 92.97%] [G loss: 3.476608]\n",
      "4402 [D loss: 0.180079, acc.: 91.80%] [G loss: 3.396573]\n",
      "4403 [D loss: 0.204528, acc.: 91.80%] [G loss: 3.211536]\n",
      "4404 [D loss: 0.205947, acc.: 91.80%] [G loss: 2.887139]\n",
      "4405 [D loss: 0.217625, acc.: 91.02%] [G loss: 2.981224]\n",
      "4406 [D loss: 0.212242, acc.: 91.02%] [G loss: 3.321082]\n",
      "4407 [D loss: 0.183949, acc.: 92.97%] [G loss: 3.780427]\n",
      "4408 [D loss: 0.223781, acc.: 89.84%] [G loss: 2.704897]\n",
      "4409 [D loss: 0.198576, acc.: 91.02%] [G loss: 3.569004]\n",
      "4410 [D loss: 0.185367, acc.: 91.41%] [G loss: 3.666936]\n",
      "4411 [D loss: 0.184828, acc.: 93.36%] [G loss: 3.230787]\n",
      "4412 [D loss: 0.221189, acc.: 90.23%] [G loss: 3.407399]\n",
      "4413 [D loss: 0.181140, acc.: 92.97%] [G loss: 3.269482]\n",
      "4414 [D loss: 0.215526, acc.: 91.02%] [G loss: 3.555539]\n",
      "4415 [D loss: 0.204147, acc.: 92.58%] [G loss: 3.044609]\n",
      "4416 [D loss: 0.221925, acc.: 91.02%] [G loss: 3.117740]\n",
      "4417 [D loss: 0.185762, acc.: 92.97%] [G loss: 3.042531]\n",
      "4418 [D loss: 0.203471, acc.: 91.80%] [G loss: 3.473224]\n",
      "4419 [D loss: 0.189027, acc.: 92.19%] [G loss: 3.661912]\n",
      "4420 [D loss: 0.232561, acc.: 91.02%] [G loss: 2.967397]\n",
      "4421 [D loss: 0.160932, acc.: 92.97%] [G loss: 3.328106]\n",
      "4422 [D loss: 0.218648, acc.: 92.97%] [G loss: 3.150150]\n",
      "4423 [D loss: 0.194838, acc.: 91.80%] [G loss: 3.325308]\n",
      "4424 [D loss: 0.195973, acc.: 90.62%] [G loss: 3.196747]\n",
      "4425 [D loss: 0.202083, acc.: 91.80%] [G loss: 3.463277]\n",
      "4426 [D loss: 0.212834, acc.: 91.80%] [G loss: 3.141195]\n",
      "4427 [D loss: 0.187169, acc.: 92.19%] [G loss: 3.208788]\n",
      "4428 [D loss: 0.187925, acc.: 92.97%] [G loss: 3.264694]\n",
      "4429 [D loss: 0.209062, acc.: 91.41%] [G loss: 3.037590]\n",
      "4430 [D loss: 0.187128, acc.: 91.80%] [G loss: 3.482943]\n",
      "4431 [D loss: 0.203540, acc.: 92.58%] [G loss: 3.116863]\n",
      "4432 [D loss: 0.203358, acc.: 91.02%] [G loss: 3.117614]\n",
      "4433 [D loss: 0.194197, acc.: 91.80%] [G loss: 3.135778]\n",
      "4434 [D loss: 0.219090, acc.: 90.62%] [G loss: 3.083109]\n",
      "4435 [D loss: 0.200402, acc.: 90.62%] [G loss: 3.746781]\n",
      "4436 [D loss: 0.184023, acc.: 92.58%] [G loss: 3.293782]\n",
      "4437 [D loss: 0.200263, acc.: 91.02%] [G loss: 3.239240]\n",
      "4438 [D loss: 0.215575, acc.: 89.06%] [G loss: 3.201814]\n",
      "4439 [D loss: 0.207413, acc.: 91.80%] [G loss: 3.110716]\n",
      "4440 [D loss: 0.238721, acc.: 91.02%] [G loss: 3.439372]\n",
      "4441 [D loss: 0.218239, acc.: 90.62%] [G loss: 3.422555]\n",
      "4442 [D loss: 0.207978, acc.: 91.02%] [G loss: 3.384543]\n",
      "4443 [D loss: 0.218901, acc.: 91.02%] [G loss: 3.132207]\n",
      "4444 [D loss: 0.197059, acc.: 93.36%] [G loss: 2.745344]\n",
      "4445 [D loss: 0.200566, acc.: 91.80%] [G loss: 3.144386]\n",
      "4446 [D loss: 0.175366, acc.: 92.19%] [G loss: 3.804300]\n",
      "4447 [D loss: 0.192345, acc.: 91.02%] [G loss: 3.112373]\n",
      "4448 [D loss: 0.203935, acc.: 91.41%] [G loss: 3.276736]\n",
      "4449 [D loss: 0.211903, acc.: 89.84%] [G loss: 3.131339]\n",
      "4450 [D loss: 0.217418, acc.: 91.41%] [G loss: 3.040569]\n",
      "4451 [D loss: 0.191592, acc.: 91.80%] [G loss: 3.565504]\n",
      "4452 [D loss: 0.190962, acc.: 91.80%] [G loss: 3.185112]\n",
      "4453 [D loss: 0.231139, acc.: 90.62%] [G loss: 2.858480]\n",
      "4454 [D loss: 0.184154, acc.: 92.58%] [G loss: 3.421672]\n",
      "4455 [D loss: 0.182771, acc.: 92.19%] [G loss: 3.595538]\n",
      "4456 [D loss: 0.202395, acc.: 91.02%] [G loss: 3.825884]\n",
      "4457 [D loss: 0.216031, acc.: 91.02%] [G loss: 3.398416]\n",
      "4458 [D loss: 0.174306, acc.: 92.58%] [G loss: 3.262832]\n",
      "4459 [D loss: 0.206659, acc.: 90.23%] [G loss: 2.977451]\n",
      "4460 [D loss: 0.189480, acc.: 91.02%] [G loss: 3.088566]\n",
      "4461 [D loss: 0.202126, acc.: 92.58%] [G loss: 3.110383]\n",
      "4462 [D loss: 0.219666, acc.: 91.02%] [G loss: 2.929281]\n",
      "4463 [D loss: 0.226959, acc.: 91.02%] [G loss: 3.340864]\n",
      "4464 [D loss: 0.204521, acc.: 91.80%] [G loss: 3.563289]\n",
      "4465 [D loss: 0.175442, acc.: 92.58%] [G loss: 3.311707]\n",
      "4466 [D loss: 0.215435, acc.: 89.45%] [G loss: 3.111644]\n",
      "4467 [D loss: 0.184015, acc.: 92.19%] [G loss: 2.981208]\n",
      "4468 [D loss: 0.213148, acc.: 90.62%] [G loss: 2.976442]\n",
      "4469 [D loss: 0.174781, acc.: 92.19%] [G loss: 3.276011]\n",
      "4470 [D loss: 0.196621, acc.: 92.97%] [G loss: 3.078950]\n",
      "4471 [D loss: 0.191608, acc.: 91.80%] [G loss: 3.225386]\n",
      "4472 [D loss: 0.168991, acc.: 92.97%] [G loss: 3.127279]\n",
      "4473 [D loss: 0.199126, acc.: 92.19%] [G loss: 2.966556]\n",
      "4474 [D loss: 0.208938, acc.: 92.19%] [G loss: 3.257306]\n",
      "4475 [D loss: 0.228526, acc.: 90.62%] [G loss: 3.302312]\n",
      "4476 [D loss: 0.215296, acc.: 90.23%] [G loss: 3.075815]\n",
      "4477 [D loss: 0.228012, acc.: 91.02%] [G loss: 3.287279]\n",
      "4478 [D loss: 0.178402, acc.: 92.19%] [G loss: 3.529950]\n",
      "4479 [D loss: 0.173742, acc.: 93.36%] [G loss: 3.220654]\n",
      "4480 [D loss: 0.209448, acc.: 91.41%] [G loss: 3.133541]\n",
      "4481 [D loss: 0.219147, acc.: 91.41%] [G loss: 3.307890]\n",
      "4482 [D loss: 0.208594, acc.: 91.41%] [G loss: 3.282102]\n",
      "4483 [D loss: 0.227910, acc.: 91.41%] [G loss: 2.919319]\n",
      "4484 [D loss: 0.218058, acc.: 91.02%] [G loss: 3.100724]\n",
      "4485 [D loss: 0.220349, acc.: 90.62%] [G loss: 3.215267]\n",
      "4486 [D loss: 0.250744, acc.: 89.84%] [G loss: 3.366026]\n",
      "4487 [D loss: 0.192917, acc.: 91.80%] [G loss: 3.380775]\n",
      "4488 [D loss: 0.210467, acc.: 90.62%] [G loss: 3.133706]\n",
      "4489 [D loss: 0.224086, acc.: 91.02%] [G loss: 2.928587]\n",
      "4490 [D loss: 0.211659, acc.: 91.80%] [G loss: 2.783664]\n",
      "4491 [D loss: 0.241910, acc.: 90.23%] [G loss: 2.849858]\n",
      "4492 [D loss: 0.208836, acc.: 91.41%] [G loss: 2.967768]\n",
      "4493 [D loss: 0.228193, acc.: 89.84%] [G loss: 2.949403]\n",
      "4494 [D loss: 0.217552, acc.: 91.02%] [G loss: 2.627592]\n",
      "4495 [D loss: 0.199290, acc.: 92.19%] [G loss: 2.929271]\n",
      "4496 [D loss: 0.214424, acc.: 91.02%] [G loss: 3.236152]\n",
      "4497 [D loss: 0.188610, acc.: 92.19%] [G loss: 3.394479]\n",
      "4498 [D loss: 0.193238, acc.: 91.80%] [G loss: 3.417943]\n",
      "4499 [D loss: 0.207934, acc.: 90.62%] [G loss: 4.086961]\n",
      "4500 [D loss: 0.201699, acc.: 92.58%] [G loss: 3.035725]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "4501 [D loss: 0.197030, acc.: 92.19%] [G loss: 3.566385]\n",
      "4502 [D loss: 0.193124, acc.: 92.19%] [G loss: 3.275315]\n",
      "4503 [D loss: 0.165629, acc.: 91.41%] [G loss: 3.919490]\n",
      "4504 [D loss: 0.169898, acc.: 93.75%] [G loss: 3.306342]\n",
      "4505 [D loss: 0.176344, acc.: 92.97%] [G loss: 3.214007]\n",
      "4506 [D loss: 0.160317, acc.: 92.97%] [G loss: 3.370909]\n",
      "4507 [D loss: 0.217703, acc.: 91.80%] [G loss: 3.082652]\n",
      "4508 [D loss: 0.179903, acc.: 93.36%] [G loss: 3.500639]\n",
      "4509 [D loss: 0.194349, acc.: 91.41%] [G loss: 3.171740]\n",
      "4510 [D loss: 0.176828, acc.: 91.80%] [G loss: 3.587810]\n",
      "4511 [D loss: 0.159980, acc.: 93.75%] [G loss: 3.065046]\n",
      "4512 [D loss: 0.192785, acc.: 91.02%] [G loss: 3.416178]\n",
      "4513 [D loss: 0.184381, acc.: 91.02%] [G loss: 3.916502]\n",
      "4514 [D loss: 0.176647, acc.: 92.58%] [G loss: 3.193826]\n",
      "4515 [D loss: 0.167641, acc.: 92.97%] [G loss: 3.059494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4516 [D loss: 0.169546, acc.: 93.75%] [G loss: 3.346698]\n",
      "4517 [D loss: 0.183766, acc.: 92.58%] [G loss: 3.140805]\n",
      "4518 [D loss: 0.210928, acc.: 91.41%] [G loss: 3.332438]\n",
      "4519 [D loss: 0.220143, acc.: 91.41%] [G loss: 3.085893]\n",
      "4520 [D loss: 0.197013, acc.: 92.58%] [G loss: 3.337287]\n",
      "4521 [D loss: 0.196833, acc.: 92.19%] [G loss: 3.243140]\n",
      "4522 [D loss: 0.185669, acc.: 91.80%] [G loss: 3.152228]\n",
      "4523 [D loss: 0.196980, acc.: 92.19%] [G loss: 3.016956]\n",
      "4524 [D loss: 0.199131, acc.: 93.36%] [G loss: 3.508607]\n",
      "4525 [D loss: 0.212729, acc.: 91.41%] [G loss: 3.297731]\n",
      "4526 [D loss: 0.211933, acc.: 91.80%] [G loss: 3.337323]\n",
      "4527 [D loss: 0.218075, acc.: 92.19%] [G loss: 3.071066]\n",
      "4528 [D loss: 0.222388, acc.: 89.06%] [G loss: 3.023981]\n",
      "4529 [D loss: 0.187043, acc.: 91.41%] [G loss: 3.534284]\n",
      "4530 [D loss: 0.194963, acc.: 91.80%] [G loss: 3.589848]\n",
      "4531 [D loss: 0.207728, acc.: 91.41%] [G loss: 3.244015]\n",
      "4532 [D loss: 0.197987, acc.: 93.36%] [G loss: 3.205430]\n",
      "4533 [D loss: 0.206543, acc.: 91.41%] [G loss: 3.114094]\n",
      "4534 [D loss: 0.208200, acc.: 92.58%] [G loss: 3.051343]\n",
      "4535 [D loss: 0.243129, acc.: 89.84%] [G loss: 3.659755]\n",
      "4536 [D loss: 0.231556, acc.: 91.41%] [G loss: 3.224058]\n",
      "4537 [D loss: 0.182387, acc.: 91.80%] [G loss: 3.142942]\n",
      "4538 [D loss: 0.169857, acc.: 92.58%] [G loss: 3.496369]\n",
      "4539 [D loss: 0.177988, acc.: 92.19%] [G loss: 3.317472]\n",
      "4540 [D loss: 0.159904, acc.: 93.36%] [G loss: 3.242201]\n",
      "4541 [D loss: 0.202519, acc.: 89.84%] [G loss: 4.081216]\n",
      "4542 [D loss: 0.229933, acc.: 91.02%] [G loss: 3.291909]\n",
      "4543 [D loss: 0.185977, acc.: 91.41%] [G loss: 3.233863]\n",
      "4544 [D loss: 0.158600, acc.: 93.75%] [G loss: 3.304809]\n",
      "4545 [D loss: 0.181164, acc.: 93.36%] [G loss: 3.162457]\n",
      "4546 [D loss: 0.204602, acc.: 90.62%] [G loss: 3.581084]\n",
      "4547 [D loss: 0.197483, acc.: 92.58%] [G loss: 3.639570]\n",
      "4548 [D loss: 0.161973, acc.: 91.41%] [G loss: 3.641280]\n",
      "4549 [D loss: 0.163345, acc.: 92.97%] [G loss: 3.700691]\n",
      "4550 [D loss: 0.171892, acc.: 93.36%] [G loss: 3.309497]\n",
      "4551 [D loss: 0.196447, acc.: 89.84%] [G loss: 3.476728]\n",
      "4552 [D loss: 0.165721, acc.: 91.80%] [G loss: 3.536180]\n",
      "4553 [D loss: 0.188719, acc.: 91.41%] [G loss: 3.536015]\n",
      "4554 [D loss: 0.183728, acc.: 91.80%] [G loss: 3.626096]\n",
      "4555 [D loss: 0.194304, acc.: 92.19%] [G loss: 3.276990]\n",
      "4556 [D loss: 0.193425, acc.: 92.58%] [G loss: 2.944153]\n",
      "4557 [D loss: 0.198241, acc.: 91.80%] [G loss: 3.750838]\n",
      "4558 [D loss: 0.170847, acc.: 92.19%] [G loss: 3.694563]\n",
      "4559 [D loss: 0.163113, acc.: 91.41%] [G loss: 3.778181]\n",
      "4560 [D loss: 0.205193, acc.: 91.80%] [G loss: 3.311046]\n",
      "4561 [D loss: 0.189189, acc.: 91.80%] [G loss: 3.558012]\n",
      "4562 [D loss: 0.189940, acc.: 92.97%] [G loss: 3.424701]\n",
      "4563 [D loss: 0.186340, acc.: 91.80%] [G loss: 3.408020]\n",
      "4564 [D loss: 0.188752, acc.: 91.02%] [G loss: 3.854638]\n",
      "4565 [D loss: 0.191841, acc.: 91.02%] [G loss: 3.217844]\n",
      "4566 [D loss: 0.198749, acc.: 91.80%] [G loss: 3.292554]\n",
      "4567 [D loss: 0.185301, acc.: 92.19%] [G loss: 3.557410]\n",
      "4568 [D loss: 0.180961, acc.: 92.58%] [G loss: 3.612283]\n",
      "4569 [D loss: 0.215892, acc.: 92.19%] [G loss: 3.312345]\n",
      "4570 [D loss: 0.205652, acc.: 92.19%] [G loss: 3.560908]\n",
      "4571 [D loss: 0.173635, acc.: 92.97%] [G loss: 3.378489]\n",
      "4572 [D loss: 0.184309, acc.: 92.19%] [G loss: 3.335431]\n",
      "4573 [D loss: 0.174965, acc.: 91.41%] [G loss: 3.982165]\n",
      "4574 [D loss: 0.176504, acc.: 92.97%] [G loss: 3.121040]\n",
      "4575 [D loss: 0.209480, acc.: 91.02%] [G loss: 3.418501]\n",
      "4576 [D loss: 0.188854, acc.: 92.58%] [G loss: 3.918303]\n",
      "4577 [D loss: 0.189214, acc.: 92.19%] [G loss: 3.332814]\n",
      "4578 [D loss: 0.182851, acc.: 92.19%] [G loss: 3.213732]\n",
      "4579 [D loss: 0.174911, acc.: 93.36%] [G loss: 3.687912]\n",
      "4580 [D loss: 0.211189, acc.: 91.80%] [G loss: 3.028474]\n",
      "4581 [D loss: 0.173591, acc.: 93.36%] [G loss: 3.337916]\n",
      "4582 [D loss: 0.226929, acc.: 90.23%] [G loss: 3.300693]\n",
      "4583 [D loss: 0.208593, acc.: 91.02%] [G loss: 3.195818]\n",
      "4584 [D loss: 0.207558, acc.: 90.23%] [G loss: 3.136192]\n",
      "4585 [D loss: 0.208141, acc.: 91.80%] [G loss: 3.267252]\n",
      "4586 [D loss: 0.190267, acc.: 91.41%] [G loss: 3.640628]\n",
      "4587 [D loss: 0.202847, acc.: 92.19%] [G loss: 3.190561]\n",
      "4588 [D loss: 0.210975, acc.: 91.41%] [G loss: 3.256096]\n",
      "4589 [D loss: 0.198676, acc.: 90.23%] [G loss: 3.472119]\n",
      "4590 [D loss: 0.187251, acc.: 92.97%] [G loss: 3.373772]\n",
      "4591 [D loss: 0.217263, acc.: 90.62%] [G loss: 3.429296]\n",
      "4592 [D loss: 0.197312, acc.: 91.80%] [G loss: 3.971160]\n",
      "4593 [D loss: 0.186643, acc.: 91.80%] [G loss: 3.158219]\n",
      "4594 [D loss: 0.195104, acc.: 92.58%] [G loss: 3.030493]\n",
      "4595 [D loss: 0.192073, acc.: 91.41%] [G loss: 3.812972]\n",
      "4596 [D loss: 0.183455, acc.: 91.80%] [G loss: 3.199046]\n",
      "4597 [D loss: 0.190527, acc.: 92.58%] [G loss: 3.481444]\n",
      "4598 [D loss: 0.212049, acc.: 89.84%] [G loss: 3.452199]\n",
      "4599 [D loss: 0.196719, acc.: 92.19%] [G loss: 3.658896]\n",
      "4600 [D loss: 0.238137, acc.: 90.23%] [G loss: 3.143497]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "4601 [D loss: 0.209056, acc.: 91.02%] [G loss: 3.302664]\n",
      "4602 [D loss: 0.207500, acc.: 92.97%] [G loss: 3.102090]\n",
      "4603 [D loss: 0.201800, acc.: 91.80%] [G loss: 3.715175]\n",
      "4604 [D loss: 0.220362, acc.: 91.02%] [G loss: 3.305482]\n",
      "4605 [D loss: 0.216366, acc.: 91.80%] [G loss: 3.273941]\n",
      "4606 [D loss: 0.185699, acc.: 91.02%] [G loss: 3.259278]\n",
      "4607 [D loss: 0.179989, acc.: 92.97%] [G loss: 3.344714]\n",
      "4608 [D loss: 0.203414, acc.: 92.19%] [G loss: 3.414642]\n",
      "4609 [D loss: 0.203342, acc.: 91.02%] [G loss: 3.360243]\n",
      "4610 [D loss: 0.219176, acc.: 88.67%] [G loss: 3.048079]\n",
      "4611 [D loss: 0.226228, acc.: 90.62%] [G loss: 3.191963]\n",
      "4612 [D loss: 0.214210, acc.: 90.23%] [G loss: 3.246963]\n",
      "4613 [D loss: 0.210340, acc.: 92.97%] [G loss: 2.785878]\n",
      "4614 [D loss: 0.189806, acc.: 90.62%] [G loss: 3.652882]\n",
      "4615 [D loss: 0.194364, acc.: 90.62%] [G loss: 3.653666]\n",
      "4616 [D loss: 0.190441, acc.: 91.41%] [G loss: 3.239221]\n",
      "4617 [D loss: 0.180939, acc.: 91.02%] [G loss: 3.837496]\n",
      "4618 [D loss: 0.243089, acc.: 87.50%] [G loss: 2.819220]\n",
      "4619 [D loss: 0.197353, acc.: 91.02%] [G loss: 3.462638]\n",
      "4620 [D loss: 0.206645, acc.: 92.19%] [G loss: 3.237864]\n",
      "4621 [D loss: 0.219708, acc.: 91.02%] [G loss: 3.297630]\n",
      "4622 [D loss: 0.230069, acc.: 91.02%] [G loss: 3.147762]\n",
      "4623 [D loss: 0.203876, acc.: 90.62%] [G loss: 3.486997]\n",
      "4624 [D loss: 0.206627, acc.: 91.80%] [G loss: 3.179641]\n",
      "4625 [D loss: 0.196267, acc.: 92.19%] [G loss: 3.194783]\n",
      "4626 [D loss: 0.198715, acc.: 89.84%] [G loss: 3.739454]\n",
      "4627 [D loss: 0.207677, acc.: 90.23%] [G loss: 3.147796]\n",
      "4628 [D loss: 0.217825, acc.: 90.23%] [G loss: 3.426980]\n",
      "4629 [D loss: 0.185132, acc.: 91.41%] [G loss: 3.795109]\n",
      "4630 [D loss: 0.186642, acc.: 93.36%] [G loss: 3.242892]\n",
      "4631 [D loss: 0.223801, acc.: 91.80%] [G loss: 3.174687]\n",
      "4632 [D loss: 0.186622, acc.: 92.19%] [G loss: 3.483616]\n",
      "4633 [D loss: 0.204067, acc.: 92.97%] [G loss: 2.942771]\n",
      "4634 [D loss: 0.215298, acc.: 91.41%] [G loss: 3.339936]\n",
      "4635 [D loss: 0.194898, acc.: 90.62%] [G loss: 4.028980]\n",
      "4636 [D loss: 0.204598, acc.: 90.62%] [G loss: 3.170319]\n",
      "4637 [D loss: 0.201365, acc.: 91.80%] [G loss: 3.235715]\n",
      "4638 [D loss: 0.186862, acc.: 91.02%] [G loss: 3.890177]\n",
      "4639 [D loss: 0.148527, acc.: 93.75%] [G loss: 3.393666]\n",
      "4640 [D loss: 0.199908, acc.: 92.19%] [G loss: 3.345137]\n",
      "4641 [D loss: 0.180565, acc.: 91.80%] [G loss: 3.874261]\n",
      "4642 [D loss: 0.191442, acc.: 91.80%] [G loss: 3.176582]\n",
      "4643 [D loss: 0.178533, acc.: 92.19%] [G loss: 3.637680]\n",
      "4644 [D loss: 0.225846, acc.: 89.84%] [G loss: 3.682967]\n",
      "4645 [D loss: 0.217812, acc.: 89.84%] [G loss: 3.385934]\n",
      "4646 [D loss: 0.193280, acc.: 91.80%] [G loss: 3.297101]\n",
      "4647 [D loss: 0.198584, acc.: 91.41%] [G loss: 3.874421]\n",
      "4648 [D loss: 0.199647, acc.: 92.97%] [G loss: 3.324322]\n",
      "4649 [D loss: 0.169266, acc.: 93.75%] [G loss: 3.291267]\n",
      "4650 [D loss: 0.206136, acc.: 91.80%] [G loss: 3.615616]\n",
      "4651 [D loss: 0.222046, acc.: 90.62%] [G loss: 3.290880]\n",
      "4652 [D loss: 0.207391, acc.: 91.02%] [G loss: 3.233285]\n",
      "4653 [D loss: 0.223205, acc.: 90.62%] [G loss: 3.259930]\n",
      "4654 [D loss: 0.202849, acc.: 92.58%] [G loss: 3.377129]\n",
      "4655 [D loss: 0.255594, acc.: 90.23%] [G loss: 3.051053]\n",
      "4656 [D loss: 0.214531, acc.: 91.02%] [G loss: 3.641010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4657 [D loss: 0.204416, acc.: 92.19%] [G loss: 3.134442]\n",
      "4658 [D loss: 0.207943, acc.: 90.62%] [G loss: 3.393978]\n",
      "4659 [D loss: 0.218385, acc.: 90.23%] [G loss: 2.861731]\n",
      "4660 [D loss: 0.190765, acc.: 92.19%] [G loss: 3.540374]\n",
      "4661 [D loss: 0.180564, acc.: 91.80%] [G loss: 3.885613]\n",
      "4662 [D loss: 0.158652, acc.: 93.75%] [G loss: 3.311375]\n",
      "4663 [D loss: 0.171487, acc.: 93.36%] [G loss: 3.147031]\n",
      "4664 [D loss: 0.197568, acc.: 91.41%] [G loss: 3.161523]\n",
      "4665 [D loss: 0.211958, acc.: 91.80%] [G loss: 3.651923]\n",
      "4666 [D loss: 0.175195, acc.: 92.58%] [G loss: 4.203863]\n",
      "4667 [D loss: 0.182033, acc.: 92.19%] [G loss: 3.289543]\n",
      "4668 [D loss: 0.160229, acc.: 92.58%] [G loss: 3.415336]\n",
      "4669 [D loss: 0.182146, acc.: 92.97%] [G loss: 3.473017]\n",
      "4670 [D loss: 0.158928, acc.: 93.75%] [G loss: 3.113698]\n",
      "4671 [D loss: 0.216767, acc.: 91.80%] [G loss: 3.311017]\n",
      "4672 [D loss: 0.192764, acc.: 92.97%] [G loss: 3.204520]\n",
      "4673 [D loss: 0.205125, acc.: 92.19%] [G loss: 3.296970]\n",
      "4674 [D loss: 0.181813, acc.: 92.97%] [G loss: 3.522858]\n",
      "4675 [D loss: 0.197058, acc.: 92.97%] [G loss: 3.083603]\n",
      "4676 [D loss: 0.221360, acc.: 90.62%] [G loss: 3.232144]\n",
      "4677 [D loss: 0.192987, acc.: 91.41%] [G loss: 3.357343]\n",
      "4678 [D loss: 0.200211, acc.: 92.97%] [G loss: 3.326013]\n",
      "4679 [D loss: 0.178997, acc.: 92.19%] [G loss: 3.953082]\n",
      "4680 [D loss: 0.171695, acc.: 91.80%] [G loss: 3.413352]\n",
      "4681 [D loss: 0.195195, acc.: 92.19%] [G loss: 3.390895]\n",
      "4682 [D loss: 0.195615, acc.: 91.02%] [G loss: 3.593098]\n",
      "4683 [D loss: 0.210138, acc.: 91.80%] [G loss: 3.130953]\n",
      "4684 [D loss: 0.214612, acc.: 92.19%] [G loss: 3.293731]\n",
      "4685 [D loss: 0.223632, acc.: 91.41%] [G loss: 3.245503]\n",
      "4686 [D loss: 0.206065, acc.: 91.41%] [G loss: 3.237386]\n",
      "4687 [D loss: 0.203877, acc.: 91.80%] [G loss: 3.252417]\n",
      "4688 [D loss: 0.218100, acc.: 91.80%] [G loss: 3.101050]\n",
      "4689 [D loss: 0.187445, acc.: 92.58%] [G loss: 3.165132]\n",
      "4690 [D loss: 0.209917, acc.: 91.41%] [G loss: 3.278616]\n",
      "4691 [D loss: 0.202271, acc.: 91.41%] [G loss: 3.689756]\n",
      "4692 [D loss: 0.204114, acc.: 90.23%] [G loss: 3.123515]\n",
      "4693 [D loss: 0.231034, acc.: 91.02%] [G loss: 3.009055]\n",
      "4694 [D loss: 0.189829, acc.: 92.97%] [G loss: 3.129622]\n",
      "4695 [D loss: 0.200165, acc.: 92.58%] [G loss: 3.529825]\n",
      "4696 [D loss: 0.190335, acc.: 90.62%] [G loss: 3.548067]\n",
      "4697 [D loss: 0.186888, acc.: 92.97%] [G loss: 3.131168]\n",
      "4698 [D loss: 0.173580, acc.: 92.97%] [G loss: 3.769433]\n",
      "4699 [D loss: 0.190080, acc.: 91.02%] [G loss: 3.680318]\n",
      "4700 [D loss: 0.177507, acc.: 93.36%] [G loss: 3.164623]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "4701 [D loss: 0.179737, acc.: 92.19%] [G loss: 3.752697]\n",
      "4702 [D loss: 0.203164, acc.: 90.62%] [G loss: 3.380672]\n",
      "4703 [D loss: 0.201330, acc.: 92.97%] [G loss: 3.241741]\n",
      "4704 [D loss: 0.202383, acc.: 89.45%] [G loss: 3.595029]\n",
      "4705 [D loss: 0.227837, acc.: 91.02%] [G loss: 3.367010]\n",
      "4706 [D loss: 0.179417, acc.: 92.58%] [G loss: 2.903587]\n",
      "4707 [D loss: 0.184666, acc.: 91.80%] [G loss: 3.623552]\n",
      "4708 [D loss: 0.170497, acc.: 92.97%] [G loss: 3.731075]\n",
      "4709 [D loss: 0.203306, acc.: 91.02%] [G loss: 3.358344]\n",
      "4710 [D loss: 0.187278, acc.: 92.19%] [G loss: 4.424014]\n",
      "4711 [D loss: 0.216246, acc.: 91.80%] [G loss: 3.049294]\n",
      "4712 [D loss: 0.187803, acc.: 90.23%] [G loss: 3.385736]\n",
      "4713 [D loss: 0.170105, acc.: 91.02%] [G loss: 4.022706]\n",
      "4714 [D loss: 0.192080, acc.: 92.58%] [G loss: 3.189148]\n",
      "4715 [D loss: 0.187515, acc.: 92.19%] [G loss: 3.661128]\n",
      "4716 [D loss: 0.195700, acc.: 91.80%] [G loss: 3.503745]\n",
      "4717 [D loss: 0.174067, acc.: 92.58%] [G loss: 3.487938]\n",
      "4718 [D loss: 0.194929, acc.: 92.58%] [G loss: 3.202830]\n",
      "4719 [D loss: 0.183198, acc.: 91.80%] [G loss: 3.437978]\n",
      "4720 [D loss: 0.215466, acc.: 91.41%] [G loss: 3.113177]\n",
      "4721 [D loss: 0.200540, acc.: 91.80%] [G loss: 3.089469]\n",
      "4722 [D loss: 0.213678, acc.: 91.41%] [G loss: 3.268788]\n",
      "4723 [D loss: 0.166392, acc.: 92.19%] [G loss: 3.683589]\n",
      "4724 [D loss: 0.201696, acc.: 91.41%] [G loss: 3.543966]\n",
      "4725 [D loss: 0.189025, acc.: 92.97%] [G loss: 3.650460]\n",
      "4726 [D loss: 0.180005, acc.: 91.41%] [G loss: 3.626901]\n",
      "4727 [D loss: 0.218385, acc.: 90.62%] [G loss: 3.685079]\n",
      "4728 [D loss: 0.227952, acc.: 90.23%] [G loss: 3.588269]\n",
      "4729 [D loss: 0.201556, acc.: 92.19%] [G loss: 3.352346]\n",
      "4730 [D loss: 0.192569, acc.: 92.58%] [G loss: 3.107234]\n",
      "4731 [D loss: 0.194170, acc.: 91.80%] [G loss: 4.033337]\n",
      "4732 [D loss: 0.184225, acc.: 91.80%] [G loss: 3.394253]\n",
      "4733 [D loss: 0.203210, acc.: 89.84%] [G loss: 3.318002]\n",
      "4734 [D loss: 0.212998, acc.: 90.23%] [G loss: 3.728726]\n",
      "4735 [D loss: 0.217373, acc.: 91.41%] [G loss: 3.509143]\n",
      "4736 [D loss: 0.190954, acc.: 91.02%] [G loss: 3.217166]\n",
      "4737 [D loss: 0.181754, acc.: 91.41%] [G loss: 3.862646]\n",
      "4738 [D loss: 0.208248, acc.: 92.19%] [G loss: 3.231139]\n",
      "4739 [D loss: 0.199581, acc.: 90.23%] [G loss: 3.681269]\n",
      "4740 [D loss: 0.171003, acc.: 92.58%] [G loss: 3.774242]\n",
      "4741 [D loss: 0.201321, acc.: 92.19%] [G loss: 3.278587]\n",
      "4742 [D loss: 0.190114, acc.: 93.36%] [G loss: 3.540673]\n",
      "4743 [D loss: 0.194915, acc.: 91.41%] [G loss: 3.550517]\n",
      "4744 [D loss: 0.199497, acc.: 91.41%] [G loss: 3.066745]\n",
      "4745 [D loss: 0.180032, acc.: 93.36%] [G loss: 3.065888]\n",
      "4746 [D loss: 0.221095, acc.: 90.23%] [G loss: 3.204116]\n",
      "4747 [D loss: 0.218789, acc.: 90.23%] [G loss: 3.386424]\n",
      "4748 [D loss: 0.198152, acc.: 92.58%] [G loss: 3.430899]\n",
      "4749 [D loss: 0.177669, acc.: 90.23%] [G loss: 3.675340]\n",
      "4750 [D loss: 0.158895, acc.: 92.97%] [G loss: 3.558155]\n",
      "4751 [D loss: 0.162586, acc.: 93.36%] [G loss: 3.622872]\n",
      "4752 [D loss: 0.155990, acc.: 92.97%] [G loss: 3.771095]\n",
      "4753 [D loss: 0.158778, acc.: 94.14%] [G loss: 3.032775]\n",
      "4754 [D loss: 0.171310, acc.: 92.97%] [G loss: 3.488675]\n",
      "4755 [D loss: 0.184680, acc.: 92.58%] [G loss: 3.714190]\n",
      "4756 [D loss: 0.184331, acc.: 91.80%] [G loss: 3.646226]\n",
      "4757 [D loss: 0.160254, acc.: 92.19%] [G loss: 3.750558]\n",
      "4758 [D loss: 0.197982, acc.: 92.19%] [G loss: 3.047267]\n",
      "4759 [D loss: 0.196578, acc.: 91.41%] [G loss: 2.864602]\n",
      "4760 [D loss: 0.217491, acc.: 91.02%] [G loss: 3.364113]\n",
      "4761 [D loss: 0.191270, acc.: 92.58%] [G loss: 3.952238]\n",
      "4762 [D loss: 0.156830, acc.: 92.97%] [G loss: 3.749699]\n",
      "4763 [D loss: 0.161889, acc.: 93.36%] [G loss: 3.438607]\n",
      "4764 [D loss: 0.179350, acc.: 91.02%] [G loss: 3.592325]\n",
      "4765 [D loss: 0.209479, acc.: 91.02%] [G loss: 3.226901]\n",
      "4766 [D loss: 0.191586, acc.: 90.62%] [G loss: 3.301260]\n",
      "4767 [D loss: 0.230157, acc.: 91.02%] [G loss: 3.627752]\n",
      "4768 [D loss: 0.193622, acc.: 91.41%] [G loss: 3.877610]\n",
      "4769 [D loss: 0.192185, acc.: 90.23%] [G loss: 4.095540]\n",
      "4770 [D loss: 0.215642, acc.: 90.23%] [G loss: 3.443337]\n",
      "4771 [D loss: 0.211707, acc.: 90.62%] [G loss: 2.974095]\n",
      "4772 [D loss: 0.198381, acc.: 91.80%] [G loss: 3.457051]\n",
      "4773 [D loss: 0.197377, acc.: 90.62%] [G loss: 3.906441]\n",
      "4774 [D loss: 0.194460, acc.: 92.58%] [G loss: 3.168882]\n",
      "4775 [D loss: 0.196610, acc.: 92.58%] [G loss: 3.404891]\n",
      "4776 [D loss: 0.175032, acc.: 90.62%] [G loss: 3.729064]\n",
      "4777 [D loss: 0.174580, acc.: 92.58%] [G loss: 3.327261]\n",
      "4778 [D loss: 0.186700, acc.: 91.02%] [G loss: 3.071060]\n",
      "4779 [D loss: 0.219189, acc.: 89.45%] [G loss: 2.970201]\n",
      "4780 [D loss: 0.215901, acc.: 90.62%] [G loss: 3.609378]\n",
      "4781 [D loss: 0.187371, acc.: 91.41%] [G loss: 3.842574]\n",
      "4782 [D loss: 0.247367, acc.: 89.45%] [G loss: 2.983148]\n",
      "4783 [D loss: 0.235792, acc.: 90.62%] [G loss: 3.248563]\n",
      "4784 [D loss: 0.220246, acc.: 91.41%] [G loss: 3.032740]\n",
      "4785 [D loss: 0.196338, acc.: 91.80%] [G loss: 2.874661]\n",
      "4786 [D loss: 0.212741, acc.: 91.80%] [G loss: 2.953332]\n",
      "4787 [D loss: 0.185137, acc.: 92.19%] [G loss: 3.175290]\n",
      "4788 [D loss: 0.211689, acc.: 91.02%] [G loss: 3.519582]\n",
      "4789 [D loss: 0.179982, acc.: 91.80%] [G loss: 3.560539]\n",
      "4790 [D loss: 0.179558, acc.: 91.80%] [G loss: 3.489675]\n",
      "4791 [D loss: 0.182208, acc.: 91.02%] [G loss: 3.765518]\n",
      "4792 [D loss: 0.184331, acc.: 91.02%] [G loss: 3.680096]\n",
      "4793 [D loss: 0.176821, acc.: 92.19%] [G loss: 3.369848]\n",
      "4794 [D loss: 0.200711, acc.: 91.41%] [G loss: 3.433554]\n",
      "4795 [D loss: 0.167721, acc.: 93.36%] [G loss: 3.666372]\n",
      "4796 [D loss: 0.173347, acc.: 93.36%] [G loss: 3.185445]\n",
      "4797 [D loss: 0.161088, acc.: 93.75%] [G loss: 4.397146]\n",
      "4798 [D loss: 0.183232, acc.: 92.19%] [G loss: 3.480298]\n",
      "4799 [D loss: 0.227368, acc.: 91.02%] [G loss: 3.076519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800 [D loss: 0.179256, acc.: 91.80%] [G loss: 3.749772]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "4801 [D loss: 0.158764, acc.: 92.97%] [G loss: 3.995684]\n",
      "4802 [D loss: 0.152402, acc.: 93.36%] [G loss: 3.409022]\n",
      "4803 [D loss: 0.148057, acc.: 92.97%] [G loss: 3.850687]\n",
      "4804 [D loss: 0.187873, acc.: 91.80%] [G loss: 3.257323]\n",
      "4805 [D loss: 0.172295, acc.: 92.19%] [G loss: 3.271387]\n",
      "4806 [D loss: 0.165915, acc.: 92.58%] [G loss: 4.426965]\n",
      "4807 [D loss: 0.145493, acc.: 93.75%] [G loss: 3.205461]\n",
      "4808 [D loss: 0.172365, acc.: 92.58%] [G loss: 3.490598]\n",
      "4809 [D loss: 0.190996, acc.: 90.62%] [G loss: 3.807270]\n",
      "4810 [D loss: 0.213930, acc.: 91.41%] [G loss: 3.416676]\n",
      "4811 [D loss: 0.190925, acc.: 92.58%] [G loss: 3.673972]\n",
      "4812 [D loss: 0.192311, acc.: 91.80%] [G loss: 3.495692]\n",
      "4813 [D loss: 0.216735, acc.: 90.23%] [G loss: 3.229908]\n",
      "4814 [D loss: 0.203274, acc.: 91.41%] [G loss: 3.530993]\n",
      "4815 [D loss: 0.211571, acc.: 89.84%] [G loss: 3.651292]\n",
      "4816 [D loss: 0.191517, acc.: 92.19%] [G loss: 3.133591]\n",
      "4817 [D loss: 0.188692, acc.: 92.97%] [G loss: 3.193341]\n",
      "4818 [D loss: 0.196022, acc.: 92.19%] [G loss: 3.365771]\n",
      "4819 [D loss: 0.198463, acc.: 89.84%] [G loss: 3.722426]\n",
      "4820 [D loss: 0.189840, acc.: 92.19%] [G loss: 3.302403]\n",
      "4821 [D loss: 0.168134, acc.: 92.58%] [G loss: 4.100388]\n",
      "4822 [D loss: 0.199998, acc.: 91.80%] [G loss: 3.059896]\n",
      "4823 [D loss: 0.181155, acc.: 92.58%] [G loss: 3.323316]\n",
      "4824 [D loss: 0.190985, acc.: 91.80%] [G loss: 3.974949]\n",
      "4825 [D loss: 0.185386, acc.: 93.36%] [G loss: 2.982776]\n",
      "4826 [D loss: 0.190601, acc.: 92.19%] [G loss: 3.105132]\n",
      "4827 [D loss: 0.194654, acc.: 92.19%] [G loss: 3.440899]\n",
      "4828 [D loss: 0.164434, acc.: 93.75%] [G loss: 3.650151]\n",
      "4829 [D loss: 0.173947, acc.: 92.58%] [G loss: 3.574934]\n",
      "4830 [D loss: 0.197800, acc.: 92.19%] [G loss: 3.193995]\n",
      "4831 [D loss: 0.208771, acc.: 91.80%] [G loss: 3.209051]\n",
      "4832 [D loss: 0.223310, acc.: 91.02%] [G loss: 3.150721]\n",
      "4833 [D loss: 0.207419, acc.: 92.58%] [G loss: 3.419437]\n",
      "4834 [D loss: 0.216800, acc.: 91.02%] [G loss: 3.550939]\n",
      "4835 [D loss: 0.196469, acc.: 92.19%] [G loss: 3.422337]\n",
      "4836 [D loss: 0.162402, acc.: 93.36%] [G loss: 3.388662]\n",
      "4837 [D loss: 0.166051, acc.: 93.36%] [G loss: 3.596049]\n",
      "4838 [D loss: 0.178595, acc.: 91.80%] [G loss: 3.342790]\n",
      "4839 [D loss: 0.220916, acc.: 91.02%] [G loss: 3.382049]\n",
      "4840 [D loss: 0.237679, acc.: 90.23%] [G loss: 3.265166]\n",
      "4841 [D loss: 0.224601, acc.: 91.02%] [G loss: 3.392517]\n",
      "4842 [D loss: 0.221730, acc.: 92.19%] [G loss: 3.038156]\n",
      "4843 [D loss: 0.208335, acc.: 92.19%] [G loss: 2.887836]\n",
      "4844 [D loss: 0.212319, acc.: 92.19%] [G loss: 2.950489]\n",
      "4845 [D loss: 0.222193, acc.: 92.19%] [G loss: 3.144277]\n",
      "4846 [D loss: 0.215266, acc.: 91.41%] [G loss: 3.198689]\n",
      "4847 [D loss: 0.191552, acc.: 92.19%] [G loss: 3.257883]\n",
      "4848 [D loss: 0.201216, acc.: 91.41%] [G loss: 3.558899]\n",
      "4849 [D loss: 0.171848, acc.: 92.97%] [G loss: 3.451404]\n",
      "4850 [D loss: 0.210095, acc.: 92.58%] [G loss: 3.164038]\n",
      "4851 [D loss: 0.191728, acc.: 91.02%] [G loss: 3.600557]\n",
      "4852 [D loss: 0.197301, acc.: 91.41%] [G loss: 3.417197]\n",
      "4853 [D loss: 0.203290, acc.: 90.62%] [G loss: 3.033156]\n",
      "4854 [D loss: 0.209608, acc.: 92.19%] [G loss: 3.062937]\n",
      "4855 [D loss: 0.225075, acc.: 92.19%] [G loss: 3.449302]\n",
      "4856 [D loss: 0.195135, acc.: 91.41%] [G loss: 3.272392]\n",
      "4857 [D loss: 0.222034, acc.: 91.80%] [G loss: 3.178206]\n",
      "4858 [D loss: 0.196551, acc.: 92.97%] [G loss: 3.401143]\n",
      "4859 [D loss: 0.165830, acc.: 92.97%] [G loss: 3.950558]\n",
      "4860 [D loss: 0.195903, acc.: 92.19%] [G loss: 3.371876]\n",
      "4861 [D loss: 0.191595, acc.: 92.58%] [G loss: 3.304774]\n",
      "4862 [D loss: 0.189381, acc.: 92.19%] [G loss: 3.208893]\n",
      "4863 [D loss: 0.188816, acc.: 91.80%] [G loss: 3.307450]\n",
      "4864 [D loss: 0.202223, acc.: 92.19%] [G loss: 3.024446]\n",
      "4865 [D loss: 0.163021, acc.: 92.19%] [G loss: 4.033374]\n",
      "4866 [D loss: 0.199641, acc.: 90.62%] [G loss: 3.275085]\n",
      "4867 [D loss: 0.194147, acc.: 92.58%] [G loss: 3.239584]\n",
      "4868 [D loss: 0.192607, acc.: 92.97%] [G loss: 4.086800]\n",
      "4869 [D loss: 0.172522, acc.: 91.41%] [G loss: 3.670868]\n",
      "4870 [D loss: 0.168088, acc.: 92.58%] [G loss: 3.231036]\n",
      "4871 [D loss: 0.193811, acc.: 93.36%] [G loss: 3.429647]\n",
      "4872 [D loss: 0.134611, acc.: 94.14%] [G loss: 3.827502]\n",
      "4873 [D loss: 0.219428, acc.: 89.06%] [G loss: 3.264741]\n",
      "4874 [D loss: 0.180581, acc.: 92.58%] [G loss: 3.910071]\n",
      "4875 [D loss: 0.199323, acc.: 91.02%] [G loss: 3.680007]\n",
      "4876 [D loss: 0.154089, acc.: 94.14%] [G loss: 3.704181]\n",
      "4877 [D loss: 0.179090, acc.: 93.36%] [G loss: 3.677283]\n",
      "4878 [D loss: 0.194854, acc.: 92.19%] [G loss: 3.005797]\n",
      "4879 [D loss: 0.203671, acc.: 92.19%] [G loss: 3.501877]\n",
      "4880 [D loss: 0.180137, acc.: 92.97%] [G loss: 3.733122]\n",
      "4881 [D loss: 0.173537, acc.: 92.58%] [G loss: 3.237279]\n",
      "4882 [D loss: 0.208314, acc.: 90.23%] [G loss: 3.063186]\n",
      "4883 [D loss: 0.184040, acc.: 92.19%] [G loss: 3.851958]\n",
      "4884 [D loss: 0.164004, acc.: 93.75%] [G loss: 3.840901]\n",
      "4885 [D loss: 0.162813, acc.: 92.97%] [G loss: 3.195824]\n",
      "4886 [D loss: 0.194120, acc.: 91.80%] [G loss: 3.762453]\n",
      "4887 [D loss: 0.161407, acc.: 92.58%] [G loss: 3.323675]\n",
      "4888 [D loss: 0.154095, acc.: 92.58%] [G loss: 3.512517]\n",
      "4889 [D loss: 0.176527, acc.: 91.41%] [G loss: 3.719082]\n",
      "4890 [D loss: 0.204914, acc.: 92.58%] [G loss: 3.323555]\n",
      "4891 [D loss: 0.216977, acc.: 92.19%] [G loss: 2.787959]\n",
      "4892 [D loss: 0.210155, acc.: 92.97%] [G loss: 3.237000]\n",
      "4893 [D loss: 0.195340, acc.: 93.36%] [G loss: 3.553638]\n",
      "4894 [D loss: 0.203791, acc.: 93.36%] [G loss: 3.418357]\n",
      "4895 [D loss: 0.216248, acc.: 90.62%] [G loss: 3.354378]\n",
      "4896 [D loss: 0.194020, acc.: 91.80%] [G loss: 3.286415]\n",
      "4897 [D loss: 0.205382, acc.: 91.80%] [G loss: 3.057152]\n",
      "4898 [D loss: 0.214321, acc.: 90.62%] [G loss: 3.269825]\n",
      "4899 [D loss: 0.183340, acc.: 92.58%] [G loss: 3.093328]\n",
      "4900 [D loss: 0.193583, acc.: 90.62%] [G loss: 3.405758]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n",
      "4901 [D loss: 0.169075, acc.: 92.19%] [G loss: 3.569180]\n",
      "4902 [D loss: 0.186342, acc.: 91.80%] [G loss: 3.306370]\n",
      "4903 [D loss: 0.137506, acc.: 92.97%] [G loss: 4.428677]\n",
      "4904 [D loss: 0.183147, acc.: 91.80%] [G loss: 3.333650]\n",
      "4905 [D loss: 0.133292, acc.: 93.75%] [G loss: 3.787629]\n",
      "4906 [D loss: 0.184958, acc.: 91.41%] [G loss: 3.613333]\n",
      "4907 [D loss: 0.219013, acc.: 92.97%] [G loss: 3.125289]\n",
      "4908 [D loss: 0.205858, acc.: 93.36%] [G loss: 3.572516]\n",
      "4909 [D loss: 0.185738, acc.: 92.97%] [G loss: 3.917474]\n",
      "4910 [D loss: 0.183194, acc.: 92.19%] [G loss: 3.394760]\n",
      "4911 [D loss: 0.184492, acc.: 91.80%] [G loss: 3.570725]\n",
      "4912 [D loss: 0.165335, acc.: 92.97%] [G loss: 3.691981]\n",
      "4913 [D loss: 0.193522, acc.: 93.75%] [G loss: 3.388202]\n",
      "4914 [D loss: 0.184460, acc.: 93.75%] [G loss: 3.409416]\n",
      "4915 [D loss: 0.205222, acc.: 91.41%] [G loss: 3.179595]\n",
      "4916 [D loss: 0.211254, acc.: 91.80%] [G loss: 3.289670]\n",
      "4917 [D loss: 0.182782, acc.: 92.19%] [G loss: 3.636302]\n",
      "4918 [D loss: 0.181875, acc.: 92.58%] [G loss: 3.518206]\n",
      "4919 [D loss: 0.153153, acc.: 92.97%] [G loss: 3.580327]\n",
      "4920 [D loss: 0.152495, acc.: 91.80%] [G loss: 3.696955]\n",
      "4921 [D loss: 0.188093, acc.: 93.36%] [G loss: 3.402707]\n",
      "4922 [D loss: 0.171840, acc.: 93.75%] [G loss: 3.788571]\n",
      "4923 [D loss: 0.178591, acc.: 92.58%] [G loss: 3.708431]\n",
      "4924 [D loss: 0.154836, acc.: 94.14%] [G loss: 3.324533]\n",
      "4925 [D loss: 0.171837, acc.: 92.19%] [G loss: 3.999377]\n",
      "4926 [D loss: 0.202340, acc.: 91.80%] [G loss: 3.027485]\n",
      "4927 [D loss: 0.200478, acc.: 91.80%] [G loss: 3.717173]\n",
      "4928 [D loss: 0.175235, acc.: 92.58%] [G loss: 3.816494]\n",
      "4929 [D loss: 0.174474, acc.: 91.80%] [G loss: 3.531173]\n",
      "4930 [D loss: 0.174538, acc.: 93.75%] [G loss: 3.546849]\n",
      "4931 [D loss: 0.208450, acc.: 92.58%] [G loss: 3.367394]\n",
      "4932 [D loss: 0.191995, acc.: 92.97%] [G loss: 3.161637]\n",
      "4933 [D loss: 0.193691, acc.: 92.58%] [G loss: 3.546813]\n",
      "4934 [D loss: 0.198686, acc.: 91.41%] [G loss: 3.857792]\n",
      "4935 [D loss: 0.198132, acc.: 91.80%] [G loss: 3.460115]\n",
      "4936 [D loss: 0.199266, acc.: 91.80%] [G loss: 3.209497]\n",
      "4937 [D loss: 0.169710, acc.: 92.58%] [G loss: 4.166322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4938 [D loss: 0.232701, acc.: 91.02%] [G loss: 3.037869]\n",
      "4939 [D loss: 0.198277, acc.: 91.80%] [G loss: 3.121519]\n",
      "4940 [D loss: 0.193941, acc.: 90.62%] [G loss: 3.421009]\n",
      "4941 [D loss: 0.177137, acc.: 92.97%] [G loss: 3.691541]\n",
      "4942 [D loss: 0.193000, acc.: 92.58%] [G loss: 3.236797]\n",
      "4943 [D loss: 0.204184, acc.: 90.23%] [G loss: 3.350921]\n",
      "4944 [D loss: 0.182708, acc.: 91.02%] [G loss: 3.872222]\n",
      "4945 [D loss: 0.187839, acc.: 92.19%] [G loss: 3.698518]\n",
      "4946 [D loss: 0.184384, acc.: 92.58%] [G loss: 3.200753]\n",
      "4947 [D loss: 0.202221, acc.: 90.62%] [G loss: 3.414512]\n",
      "4948 [D loss: 0.209575, acc.: 92.19%] [G loss: 3.264449]\n",
      "4949 [D loss: 0.174162, acc.: 93.36%] [G loss: 4.208138]\n",
      "4950 [D loss: 0.220193, acc.: 91.02%] [G loss: 3.435091]\n",
      "4951 [D loss: 0.164542, acc.: 93.36%] [G loss: 3.987699]\n",
      "4952 [D loss: 0.173061, acc.: 92.97%] [G loss: 3.367356]\n",
      "4953 [D loss: 0.189814, acc.: 92.58%] [G loss: 3.317192]\n",
      "4954 [D loss: 0.211748, acc.: 91.80%] [G loss: 3.927263]\n",
      "4955 [D loss: 0.206383, acc.: 92.19%] [G loss: 3.677501]\n",
      "4956 [D loss: 0.207573, acc.: 92.58%] [G loss: 3.446834]\n",
      "4957 [D loss: 0.165741, acc.: 92.97%] [G loss: 4.215686]\n",
      "4958 [D loss: 0.184662, acc.: 92.58%] [G loss: 2.827164]\n",
      "4959 [D loss: 0.154207, acc.: 93.36%] [G loss: 3.846305]\n",
      "4960 [D loss: 0.179019, acc.: 93.36%] [G loss: 3.545789]\n",
      "4961 [D loss: 0.192239, acc.: 91.80%] [G loss: 2.973048]\n",
      "4962 [D loss: 0.197679, acc.: 91.02%] [G loss: 3.382679]\n",
      "4963 [D loss: 0.185448, acc.: 92.19%] [G loss: 3.771690]\n",
      "4964 [D loss: 0.215840, acc.: 92.19%] [G loss: 3.482291]\n",
      "4965 [D loss: 0.149143, acc.: 93.75%] [G loss: 4.218605]\n",
      "4966 [D loss: 0.195222, acc.: 92.97%] [G loss: 3.124447]\n",
      "4967 [D loss: 0.163040, acc.: 93.75%] [G loss: 3.405844]\n",
      "4968 [D loss: 0.172629, acc.: 93.75%] [G loss: 3.565208]\n",
      "4969 [D loss: 0.203569, acc.: 92.58%] [G loss: 3.407748]\n",
      "4970 [D loss: 0.205116, acc.: 91.80%] [G loss: 3.284564]\n",
      "4971 [D loss: 0.207956, acc.: 91.41%] [G loss: 3.198673]\n",
      "4972 [D loss: 0.219150, acc.: 90.62%] [G loss: 3.690224]\n",
      "4973 [D loss: 0.177783, acc.: 91.80%] [G loss: 3.532657]\n",
      "4974 [D loss: 0.194239, acc.: 91.02%] [G loss: 3.353006]\n",
      "4975 [D loss: 0.202532, acc.: 91.80%] [G loss: 3.498039]\n",
      "4976 [D loss: 0.179780, acc.: 92.97%] [G loss: 3.328377]\n",
      "4977 [D loss: 0.221667, acc.: 91.02%] [G loss: 3.278724]\n",
      "4978 [D loss: 0.196056, acc.: 89.84%] [G loss: 4.220301]\n",
      "4979 [D loss: 0.193053, acc.: 93.36%] [G loss: 3.331796]\n",
      "4980 [D loss: 0.161028, acc.: 93.36%] [G loss: 3.890373]\n",
      "4981 [D loss: 0.168504, acc.: 92.58%] [G loss: 3.910151]\n",
      "4982 [D loss: 0.180303, acc.: 94.14%] [G loss: 3.076874]\n",
      "4983 [D loss: 0.145804, acc.: 94.53%] [G loss: 3.874465]\n",
      "4984 [D loss: 0.206051, acc.: 92.58%] [G loss: 3.372015]\n",
      "4985 [D loss: 0.154639, acc.: 92.58%] [G loss: 4.486018]\n",
      "4986 [D loss: 0.205774, acc.: 90.62%] [G loss: 3.271644]\n",
      "4987 [D loss: 0.209280, acc.: 92.19%] [G loss: 3.389750]\n",
      "4988 [D loss: 0.196213, acc.: 92.19%] [G loss: 4.064606]\n",
      "4989 [D loss: 0.209180, acc.: 91.02%] [G loss: 3.069641]\n",
      "4990 [D loss: 0.188956, acc.: 91.80%] [G loss: 3.614946]\n",
      "4991 [D loss: 0.178417, acc.: 92.19%] [G loss: 3.311095]\n",
      "4992 [D loss: 0.138406, acc.: 92.58%] [G loss: 4.494380]\n",
      "4993 [D loss: 0.169691, acc.: 92.58%] [G loss: 3.471071]\n",
      "4994 [D loss: 0.184519, acc.: 92.97%] [G loss: 3.430836]\n",
      "4995 [D loss: 0.165552, acc.: 92.58%] [G loss: 3.602243]\n",
      "4996 [D loss: 0.191790, acc.: 91.02%] [G loss: 3.113256]\n",
      "4997 [D loss: 0.200751, acc.: 92.19%] [G loss: 3.669389]\n",
      "4998 [D loss: 0.196407, acc.: 92.19%] [G loss: 3.399025]\n",
      "4999 [D loss: 0.176357, acc.: 92.58%] [G loss: 3.341828]\n",
      "5000 [D loss: 0.182665, acc.: 91.02%] [G loss: 3.596382]\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_1:0\", shape=(128, 32), dtype=float32) for input (128, 32), but it was re-called on a Tensor with incompatible shape (432, 32).\n",
      "generated_data\n"
     ]
    }
   ],
   "source": [
    "#Training the GAN model chosen: Vanilla GAN, CGAN, DCGAN, etc.\n",
    "synthesizer = model(gan_args)\n",
    "synthesizer.train(train_sample, train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fabiana/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models/gan/saved/generator_fraud/assets\n"
     ]
    }
   ],
   "source": [
    "#You can easily save the trained generator and loaded it aftwerwards\n",
    "synthesizer.save('models/gan/saved', 'generator_fraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(128, 32)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, 128)                4224      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (128, 256)                33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (128, 512)                131584    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (128, 31)                 15903     \n",
      "=================================================================\n",
      "Total params: 184,735\n",
      "Trainable params: 184,735\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "synthesizer.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "synthesizer.discriminator.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setup parameters visualization parameters\n",
    "seed = 17\n",
    "test_size = 492 # number of fraud cases\n",
    "noise_dim = 32\n",
    "\n",
    "np.random.seed(seed)\n",
    "z = np.random.normal(size=(test_size, noise_dim))\n",
    "real = synthesizer.get_data_batch(train=train_sample, batch_size=test_size, seed=seed)\n",
    "real_samples = pd.DataFrame(real, columns=data_cols+label_cols)\n",
    "labels = fraud_w_classes['Class']\n",
    "\n",
    "model_names = ['GAN']\n",
    "colors = ['deepskyblue','blue']\n",
    "markers = ['o','^']\n",
    "class_labels = ['Class 1','Class 2']\n",
    "\n",
    "col1, col2 = 'V17', 'V10'\n",
    "\n",
    "base_dir = 'cache/'\n",
    "\n",
    "#Actual fraud data visualization\n",
    "model_steps = [ 0, 200, 500, 1000, 5000]\n",
    "rows = len(model_steps)\n",
    "columns = 5\n",
    "\n",
    "axarr = [[]]*len(model_steps)\n",
    "\n",
    "fig = plt.figure(figsize=(14,rows*3))\n",
    "\n",
    "for model_step_ix, model_step in enumerate(model_steps):        \n",
    "    axarr[model_step_ix] = plt.subplot(rows, columns, model_step_ix*columns + 1)\n",
    "    \n",
    "    for group, color, marker, label in zip(real_samples.groupby('Class_1'), colors, markers, class_labels ):\n",
    "        plt.scatter( group[1][[col1]], group[1][[col2]], \n",
    "                         label=label, marker=marker, edgecolors=color, facecolors='none' )\n",
    "    \n",
    "    plt.title('Actual Fraud Data')\n",
    "    plt.ylabel(col2) # Only add y label to left plot\n",
    "    plt.xlabel(col1)\n",
    "    xlims, ylims = axarr[model_step_ix].get_xlim(), axarr[model_step_ix].get_ylim()    \n",
    "    \n",
    "    if model_step_ix == 0: \n",
    "        legend = plt.legend()\n",
    "        legend.get_frame().set_facecolor('white')\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(128, 31)]               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (128, 512)                16384     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (128, 512)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (128, 256)                131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (128, 256)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (128, 128)                32896     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (128, 1)                  129       \n",
      "=================================================================\n",
      "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "Total params: 361,474\n",
      "Trainable params: 180,737\n",
      "Non-trainable params: 180,737\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    " if model_step_ix == 0: \n",
    "        legend = plt.legend()\n",
    "        legend.get_frame().set_facecolor('white')\n",
    "    \n",
    "    for i, model_name in enumerate( model_names[:] ):\n",
    "        \n",
    "        [ model_name, with_class, type0, generator_model ] = models[model_name]\n",
    "        \n",
    "        generator_model.load_weights( base_dir + model_name + '_generator_model_weights_step_'+str(model_step)+'.h5')\n",
    "\n",
    "        ax = plt.subplot(rows, columns, model_step_ix*columns + 1 + (i+1) )\n",
    "        \n",
    "        if with_class:\n",
    "            g_z = generator_model.predict([z, labels])\n",
    "            gen_samples = pd.DataFrame(g_z, columns=data_cols+label_cols)\n",
    "            for group, color, marker, label in zip( gen_samples.groupby('Class_1'), colors, markers, class_labels ):\n",
    "                plt.scatter( group[1][[col1]], group[1][[col2]], \n",
    "                                 label=label, marker=marker, edgecolors=color, facecolors='none' )\n",
    "        else:\n",
    "            g_z = generator_model.predict(z)\n",
    "            gen_samples = pd.DataFrame(g_z, columns=data_cols)\n",
    "            gen_samples.to_csv('Generated_sample.csv')\n",
    "            plt.scatter( gen_samples[[col1]], gen_samples[[col2]], \n",
    "                             label=class_labels[0], marker=markers[0], edgecolors=colors[0], facecolors='none' )\n",
    "        plt.title(model_name)   \n",
    "        plt.xlabel(data_cols[0])\n",
    "        ax.set_xlim(xlims), ax.set_ylim(ylims)\n",
    "\n",
    "\n",
    "plt.suptitle('Comparison of GAN outputs', size=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0.075,0,1,0.95])\n",
    "\n",
    "# Adding text labels for traning steps\n",
    "vpositions = np.array([ i._position.bounds[1] for i in axarr ])\n",
    "vpositions += ((vpositions[0] - vpositions[1]) * 0.35 )\n",
    "for model_step_ix, model_step in enumerate( model_steps ):\n",
    "    fig.text( 0.05, vpositions[model_step_ix], 'training\\nstep\\n'+str(model_step), ha='center', va='center', size=12)\n",
    "\n",
    "plt.savefig('Comparison_of_GAN_outputs.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x1080 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup parameters visualization parameters\n",
    "seed = 17\n",
    "test_size = 492 # number of fraud cases\n",
    "noise_dim = 32\n",
    "\n",
    "np.random.seed(seed)\n",
    "z = np.random.normal(size=(test_size, noise_dim))\n",
    "real = synthesizer.get_data_batch(train=train_sample, batch_size=test_size, seed=seed)\n",
    "real_samples = pd.DataFrame(real, columns=data_cols+label_cols)\n",
    "labels = fraud_w_classes['Class']\n",
    "\n",
    "model_names = ['GAN']\n",
    "colors = ['deepskyblue','blue']\n",
    "markers = ['o','^']\n",
    "class_labels = ['Class 1','Class 2']\n",
    "\n",
    "col1, col2 = 'V17', 'V10'\n",
    "\n",
    "base_dir = 'cache/'\n",
    "\n",
    "#Actual fraud data visualization\n",
    "model_steps = [ 0, 200, 500, 1000, 5000]\n",
    "rows = len(model_steps)\n",
    "columns = 5\n",
    "\n",
    "axarr = [[]]*len(model_steps)\n",
    "\n",
    "fig = plt.figure(figsize=(14,rows*3))\n",
    "\n",
    "for model_step_ix, model_step in enumerate(model_steps):        \n",
    "    axarr[model_step_ix] = plt.subplot(rows, columns, model_step_ix*columns + 1)\n",
    "    \n",
    "    for group, color, marker, label in zip(real_samples.groupby('Class_1'), colors, markers, class_labels ):\n",
    "        plt.scatter( group[1][[col1]], group[1][[col2]], \n",
    "                         label=label, marker=marker, edgecolors=color, facecolors='none' )\n",
    "    \n",
    "    plt.title('Actual Fraud Data')\n",
    "    plt.ylabel(col2) # Only add y label to left plot\n",
    "    plt.xlabel(col1)\n",
    "    xlims, ylims = axarr[model_step_ix].get_xlim(), axarr[model_step_ix].get_ylim()    \n",
    "    \n",
    "    if model_step_ix == 0: \n",
    "        legend = plt.legend()\n",
    "        legend.get_frame().set_facecolor('white')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " if model_step_ix == 0: \n",
    "        legend = plt.legend()\n",
    "        legend.get_frame().set_facecolor('white')\n",
    "    \n",
    "    for i, model_name in enumerate( model_names[:] ):\n",
    "        \n",
    "        [ model_name, with_class, type0, generator_model ] = models[model_name]\n",
    "        \n",
    "        generator_model.load_weights( base_dir + model_name + '_generator_model_weights_step_'+str(model_step)+'.h5')\n",
    "\n",
    "        ax = plt.subplot(rows, columns, model_step_ix*columns + 1 + (i+1) )\n",
    "        \n",
    "        if with_class:\n",
    "            g_z = generator_model.predict([z, labels])\n",
    "            gen_samples = pd.DataFrame(g_z, columns=data_cols+label_cols)\n",
    "            for group, color, marker, label in zip( gen_samples.groupby('Class_1'), colors, markers, class_labels ):\n",
    "                plt.scatter( group[1][[col1]], group[1][[col2]], \n",
    "                                 label=label, marker=marker, edgecolors=color, facecolors='none' )\n",
    "        else:\n",
    "            g_z = generator_model.predict(z)\n",
    "            gen_samples = pd.DataFrame(g_z, columns=data_cols)\n",
    "            gen_samples.to_csv('Generated_sample.csv')\n",
    "            plt.scatter( gen_samples[[col1]], gen_samples[[col2]], \n",
    "                             label=class_labels[0], marker=markers[0], edgecolors=colors[0], facecolors='none' )\n",
    "        plt.title(model_name)   \n",
    "        plt.xlabel(data_cols[0])\n",
    "        ax.set_xlim(xlims), ax.set_ylim(ylims)\n",
    "\n",
    "\n",
    "plt.suptitle('Comparison of GAN outputs', size=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0.075,0,1,0.95])\n",
    "\n",
    "# Adding text labels for traning steps\n",
    "vpositions = np.array([ i._position.bounds[1] for i in axarr ])\n",
    "vpositions += ((vpositions[0] - vpositions[1]) * 0.35 )\n",
    "for model_step_ix, model_step in enumerate( model_steps ):\n",
    "    fig.text( 0.05, vpositions[model_step_ix], 'training\\nstep\\n'+str(model_step), ha='center', va='center', size=12)\n",
    "\n",
    "plt.savefig('Comparison_of_GAN_outputs.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}