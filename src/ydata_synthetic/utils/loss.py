"""
    Auxiliary loss functions to be used by the available synhtesizers
    The loss functions were developed using TF2
"""
import tensorflow as tf

def get_gan_losses_fn():
    """
    Get Vanilla GAN losses
    Returns discriminator and generator losses
    -------
    """
    bce = tf.losses.BinaryCrossentropy(from_logits=True)

    def d_loss_fn(r_logit, f_logit):
        r_loss = bce(tf.ones_like(r_logit), r_logit)
        f_loss = bce(tf.zeros_like(f_logit), f_logit)
        return r_loss, f_loss

    def g_loss_fn(f_logit):
        f_loss = bce(tf.ones_like(f_logit), f_logit)
        return f_loss

    return d_loss_fn, g_loss_fn

def get_lsgan_losses_fn():
    """
    Calculate LSGAN losses
    Returns discriminator loss, generator loss
    -------
    """
    mse = tf.losses.MeanSquaredError()

    def d_loss_fn(r_logit, f_logit):
        r_loss = mse(tf.ones_like(r_logit), r_logit)
        f_loss = mse(tf.zeros_like(f_logit), f_logit)
        return r_loss, f_loss

    def g_loss_fn(f_logit):
        f_loss = mse(tf.ones_like(f_logit), f_logit)
        return f_loss

    return d_loss_fn, g_loss_fn


def get_wgan_losses_fn(attributes=False):
    """
    Calculates Wasserstein loss for GANs
    Returns discriminator loss, generator loss and aux_loss if attributes=True
    -------
    """

    def d_loss_fn(r_logit, f_logit):
        r_loss = - tf.reduce_mean(r_logit)
        f_loss = tf.reduce_mean(f_logit)
        return r_loss, f_loss

    def g_loss_fn(f_logit):
        f_loss = - tf.reduce_mean(f_logit)
        return f_loss

    def d_aux_loss_fn(r_logit, f_logit):
        r_loss = - tf.reduce_mean(r_logit)
        f_loss = tf.reduce_mean(f_logit)
        return r_loss, f_loss

    loss_functions = [g_loss_fn, d_loss_fn]
    if attributes:
        loss_functions.append(d_aux_loss_fn)

    return loss_functions


def get_adversarial_losses_fn(mode, attributes=False):
    """
    Parameters
    ----------
    mode calculated loss
    Returns GAN gradients
    -------
    """
    result = None
    if mode == 'lsgan':
        result = get_lsgan_losses_fn()
    elif mode == 'wgan':
        result = get_wgan_losses_fn(attributes)
    else:
        result = get_gan_losses_fn()
    return result

def gradient_penalty(func, real, fake, mode):
    """
    Parameters
    ----------
    f A function
    real A tensor with the real data
    fake A tensor with the fake data generated by the Generator
    mode type of loss applied
    Returns the gradient penalty
    -------
    """
    def _gradient_penalty(func, real, fake=None):
        def _interpolate(a, b=None):
            if b is None:   # interpolation in DRAGAN
                beta = tf.random.uniform(shape=tf.shape(a), minval=0., maxval=1.)
                b = a + 0.5 * tf.math.reduce_std(a) * beta
            alpha = tf.random.uniform(shape=[tf.shape(a)[0], 1],
                                      minval=0., maxval=1.,
                                      dtype=tf.float32)

            a = tf.cast(a, tf.float32)
            inter = a + alpha * (b - a)
            inter.set_shape(a.shape)
            return inter

        x = _interpolate(real, fake)
        with tf.GradientTape() as t:
            t.watch(x)
            pred = func(x)
        grad = t.gradient(pred, x)
        norm = tf.norm(tf.reshape(grad, [tf.shape(grad)[0], -1]), axis=1)
        gp = tf.reduce_mean((norm - 1.)**2)

        return gp

    def _gradient_penalty_TS(func, real, fake=None):
        def _interpolate(a, b=None):
            if b is None:   # interpolation in DRAGAN
                beta = tf.random.uniform(shape=tf.shape(a), minval=0., maxval=1., dtype=tf.float32)
                b = a + 0.5 * tf.math.reduce_std(a) * beta

            alpha1 = tf.random.uniform(shape=[tf.shape(a[0])[0], 1],
                                       minval=0., maxval=1.,
                                       dtype=tf.float32)
            alpha2 = tf.expand_dims(alpha1, 2)

            a0 = tf.cast(a[0], tf.float32)
            a1 = tf.cast(a[1], tf.float32)
            interpolate_feat = a0 + alpha2 * (b[0] - a0)
            interpolate_attr = a1 + alpha1 * (b[1] - a1)

            interpolate_feat.set_shape(a[0].shape)
            interpolate_attr.set_shape(a[1].shape)

            inter = [interpolate_feat, interpolate_attr]
            return inter

        x = _interpolate(real, fake)
        with tf.GradientTape() as t:
            t.watch(x)
            pred = func(x)
        grad = t.gradient(pred, x)

        slopes1 = tf.reduce_sum(tf.square(grad[0]), [1, 2])

        slopes2 = tf.reduce_sum(tf.square(grad[1]), [1])
        slopes = tf.sqrt(slopes1 + slopes2 + 1e-8)

        gp = tf.reduce_mean((slopes - 1.)**2)
        return gp

    if mode == 'wgan-TS':
        gp = _gradient_penalty_TS(func, real, fake)
    elif mode == 'dragan':
        gp = _gradient_penalty(func, real)
    elif mode == 'wgan-gp':
        gp = _gradient_penalty(func, real, fake)
    else:
        gp = tf.constant(0, dtype=real.dtype)

    return gp
